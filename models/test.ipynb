{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00fef5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from attentions import MultiHeadAttentionRoPE\n",
    "from flax.nnx import MultiHeadAttention\n",
    "\n",
    "batch_size, seq_len, in_features = 2, 4, 8\n",
    "key1, key2, key3 = jax.random.split(jax.random.key(1), 3)\n",
    "q = jax.random.normal(key1, (batch_size, seq_len, in_features))\n",
    "k = jax.random.normal(key2, (batch_size, seq_len, in_features))\n",
    "v = jax.random.normal(key3, (batch_size, seq_len, in_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2452b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================MHA ROPE=============================\n",
      "Output shape: (2, 4, 8)\n",
      "Output sample: [ 0.06869456 -0.03120548 -0.16521749 -0.2619766   0.08072615]\n",
      "================================MHA===============================\n",
      "Output shape: (2, 4, 8)\n",
      "Output sample: [ 0.06869456 -0.03120548 -0.16521749 -0.2619766   0.08072615]\n"
     ]
    }
   ],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "mha_rope = MultiHeadAttentionRoPE(\n",
    "    num_heads=4,\n",
    "    in_features=in_features,\n",
    "    qkv_features=8,\n",
    "    out_features=8,\n",
    "    rngs=rngs,\n",
    "    use_rope=False,\n",
    "    decode=False,\n",
    ")\n",
    "\n",
    "rngs = nnx.Rngs(0)\n",
    "mha = MultiHeadAttention(\n",
    "    num_heads=4,\n",
    "    in_features=in_features,\n",
    "    qkv_features=8,\n",
    "    out_features=8,\n",
    "    rngs=rngs,\n",
    "    decode=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"=============================MHA ROPE=============================\")\n",
    "out = mha_rope(q, k, v)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Output sample:\", out[0, 0, :5])\n",
    "\n",
    "print(\"================================MHA===============================\")\n",
    "out = mha(q, k, v)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Output sample:\", out[0, 0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "139b60d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same shape: True\n",
      "Allclose? False\n",
      "Same shape: True\n",
      "Allclose? False\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(\n",
    "    num_heads=4,\n",
    "    in_features=in_features,\n",
    "    qkv_features=8,\n",
    "    out_features=8,\n",
    "    rngs=nnx.Rngs(1),\n",
    "    decode=False,\n",
    ")\n",
    "mha_plain = MultiHeadAttentionRoPE(\n",
    "    num_heads=4,\n",
    "    in_features=in_features,\n",
    "    qkv_features=8,\n",
    "    out_features=8,\n",
    "    rngs=nnx.Rngs(1),\n",
    "    use_rope=False,\n",
    "    decode=False,\n",
    ")\n",
    "\n",
    "out_plain = mha(q, k, v)\n",
    "\n",
    "print(\"Same shape:\", out.shape == out_plain.shape)\n",
    "print(\"Allclose?\", jnp.allclose(out, out_plain, atol=1e-5))\n",
    "\n",
    "out_plain = mha_plain(q, k, v)\n",
    "\n",
    "print(\"Same shape:\", out.shape == out_plain.shape)\n",
    "print(\"Allclose?\", jnp.allclose(out, out_plain, atol=1e-5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8dda6dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full output shape: (2, 4, 8)\n",
      "Step output shape: (2, 4, 8)\n",
      "Allclose: False\n",
      "\n",
      "First token diff:\n",
      "[ 0.1624817  1.6478726  0.7238274 -1.3535289  0.8455794]\n",
      "[ 0.16221178  1.6474111   0.72350943 -1.3529046   0.84493995]\n"
     ]
    }
   ],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "batch_size, seq_len, in_features = 2, 4, 8\n",
    "num_heads = 4\n",
    "\n",
    "q = jax.random.normal(jax.random.key(0), (batch_size, seq_len, in_features))\n",
    "\n",
    "mha_rope = MultiHeadAttentionRoPE(\n",
    "    num_heads=num_heads,\n",
    "    in_features=in_features,\n",
    "    qkv_features=8,\n",
    "    out_features=8,\n",
    "    rngs=nnx.Rngs(1),\n",
    "    decode=False,\n",
    "    dropout_rate=0.0,\n",
    "    use_rope=True,\n",
    ")\n",
    "\n",
    "tri = jnp.tril(jnp.ones((seq_len, seq_len), dtype=bool))\n",
    "causal_mask = jnp.broadcast_to(tri, (batch_size, num_heads, seq_len, seq_len))\n",
    "\n",
    "out_full = mha_rope(q, mask=causal_mask, decode=False)\n",
    "\n",
    "mha_decode = MultiHeadAttentionRoPE(\n",
    "    num_heads=num_heads,\n",
    "    in_features=in_features,\n",
    "    qkv_features=8,\n",
    "    out_features=8,\n",
    "    rngs=nnx.Rngs(1),\n",
    "    decode=True,\n",
    "    dropout_rate=0.0,\n",
    "    use_rope=True,\n",
    ")\n",
    "\n",
    "mha_decode.init_cache((batch_size, 1, in_features))\n",
    "\n",
    "outs = []\n",
    "for t in range(seq_len):\n",
    "    out_t = mha_decode(q[:, t:t+1, :])\n",
    "    outs.append(out_t)\n",
    "\n",
    "out_step = jnp.concatenate(outs, axis=1)\n",
    "\n",
    "print(\"Full output shape:\", out_full.shape)\n",
    "print(\"Step output shape:\", out_step.shape)\n",
    "print(\"Allclose:\", jnp.allclose(out_full, out_step, atol=1e-4))\n",
    "\n",
    "print(\"\\nFirst token diff:\")\n",
    "print(out_full[0, 0, :5])\n",
    "print(out_step[0, 0, :5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
