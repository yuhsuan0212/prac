{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a1f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import optax, orbax\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "import numpy as np\n",
    "import tiktoken, time, wandb\n",
    "\n",
    "from models.attentions import MultiHeadAttentionRoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0188c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternative data and model parallel\n",
    "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((1, 1)), ('batch', 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ec3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708224fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
    "if GPT2_variant == \"GPT2-medium\":\n",
    "    num_transformer_blocks = 24\n",
    "    seqlen = 1024\n",
    "    embed_dim = 1024\n",
    "    num_heads = 16\n",
    "    feed_forward_dim = 4 * embed_dim\n",
    "    batch_size = 32  \n",
    "else: ## Assume GPT2 otherwise\n",
    "    num_transformer_blocks = 12\n",
    "    seqlen = 1024\n",
    "    embed_dim = 768\n",
    "    num_heads = 12\n",
    "    feed_forward_dim = 4 * embed_dim\n",
    "    batch_size = 32\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "max_steps = 600000*12//batch_size\n",
    "init_learning_rate = 5e-4\n",
    "weight_decay = 1e-1\n",
    "top_k = 10\n",
    "dtype = jnp.bfloat16\n",
    "param_dtype = jnp.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f185c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(seq_len):\n",
    "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "\n",
    "\n",
    "dtype = jnp.bfloat16\n",
    "param_dtype = jnp.float32\n",
    "\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        dropout_rate: float,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.layer_norm1 = nnx.LayerNorm(\n",
    "            epsilon=1e-6,\n",
    "            num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), None),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.mha = nnx.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            in_features=embed_dim,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), None),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
    "        self.layer_norm2 = nnx.LayerNorm(\n",
    "            epsilon=1e-6,\n",
    "            num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), None),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=ff_dim,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), None),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=ff_dim,\n",
    "            out_features=embed_dim,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), None),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        input_shape = inputs.shape\n",
    "        bs, seq_len, emb_sz = input_shape\n",
    "\n",
    "        attention_output = self.mha(\n",
    "            inputs_q=self.layer_norm1(inputs),\n",
    "            mask=causal_attention_mask(seq_len),\n",
    "            decode=False,\n",
    "        )\n",
    "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
    "\n",
    "        # MLP\n",
    "        mlp_output = self.linear1(self.layer_norm2(x))\n",
    "        mlp_output = nnx.gelu(mlp_output)\n",
    "        mlp_output = self.linear2(mlp_output)\n",
    "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
    "\n",
    "        return x + mlp_output\n",
    "\n",
    "\n",
    "class TransformerBlockRoPE(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        dropout_rate: float,\n",
    "        rope_base: int,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.rms_norm1 = nnx.RMSNorm(\n",
    "            epsilon=1e-6,\n",
    "            num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.mha = MultiHeadAttentionRoPE(\n",
    "            num_heads=num_heads,\n",
    "            in_features=embed_dim,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), None),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rope_base=rope_base,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
    "        self.rms_norm2 = nnx.RMSNorm(\n",
    "            epsilon=1e-6,\n",
    "            num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=ff_dim,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), None),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=ff_dim,\n",
    "            out_features=embed_dim,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), None),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        input_shape = inputs.shape\n",
    "        bs, seq_len, emb_sz = input_shape\n",
    "\n",
    "        attention_output = self.mha(\n",
    "            inputs_q=self.rms_norm1(inputs),\n",
    "            mask=causal_attention_mask(seq_len),\n",
    "            decode=False,\n",
    "        )\n",
    "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
    "\n",
    "        # MLP\n",
    "        mlp_output = self.linear1(self.rms_norm2(x))\n",
    "        mlp_output = nnx.gelu(mlp_output)\n",
    "        mlp_output = self.linear2(mlp_output)\n",
    "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
    "\n",
    "        return x + mlp_output\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(nnx.Module):\n",
    "\n",
    "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
    "        self.token_emb = nnx.Embed(\n",
    "            num_embeddings=vocab_size,\n",
    "            features=embed_dim,\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.pos_emb = nnx.Embed(\n",
    "            num_embeddings=seqlen,\n",
    "            features=embed_dim,\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
    "        position_embedding = self.pos_emb(positions)\n",
    "        token_embedding = self.token_emb(x)\n",
    "        return self.token_emb, token_embedding + position_embedding\n",
    "\n",
    "\n",
    "class TokenEmbedding(nnx.Module):\n",
    "\n",
    "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
    "        self.token_emb = nnx.Embed(\n",
    "            num_embeddings=vocab_size,\n",
    "            features=embed_dim,\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        token_embedding = self.token_emb(x)\n",
    "        return self.token_emb, token_embedding\n",
    "\n",
    "class GPT2(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seqlen: int,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        rate: float,\n",
    "        feed_forward_dim: int,\n",
    "        num_transformer_blocks: int,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "            seqlen, vocab_size, embed_dim, rngs=rngs\n",
    "        )\n",
    "        self.dropout = nnx.Dropout(rate=rate)\n",
    "\n",
    "        self.transformer_blocks = nnx.List(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs\n",
    "                )\n",
    "                for _ in range(num_transformer_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nnx.LayerNorm(\n",
    "            epsilon=1e-6,\n",
    "            num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), None),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        token_embedding, x = self.embedding_layer(inputs)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        x = self.layer_norm(x)\n",
    "        # Weights tying\n",
    "        outputs = token_embedding.attend(x)\n",
    "        return outputs\n",
    "\n",
    "    @nnx.jit\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "        logits = nnx.softmax(logits)\n",
    "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
    "\n",
    "    @nnx.jit\n",
    "    def generate_step(self, padded_tokens, sample_index):\n",
    "        logits = self(padded_tokens)\n",
    "        next_token = self.sample_from(logits[0][sample_index])\n",
    "        return next_token\n",
    "\n",
    "    def generate_text(self, max_tokens, start_tokens):\n",
    "        generated = []\n",
    "        print(tokenizer.decode(start_tokens), flush=True, end=\"\")\n",
    "        for i in range(max_tokens):\n",
    "            sample_index = len(start_tokens) + len(generated) - 1\n",
    "            # TODO: use attention masking for better efficiency\n",
    "            padded_tokens = jnp.array(\n",
    "                (\n",
    "                    start_tokens\n",
    "                    + generated\n",
    "                    + [0] * (seqlen - len(start_tokens) - len(generated))\n",
    "                )\n",
    "            )[None, :]\n",
    "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
    "            if (\n",
    "                next_token\n",
    "                == tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[\n",
    "                    0\n",
    "                ]\n",
    "            ):\n",
    "                break\n",
    "            generated.append(next_token)\n",
    "            # decode and print next_token\n",
    "            print(tokenizer.decode([next_token]), flush=True, end=\"\")\n",
    "        return tokenizer.decode(start_tokens + generated)\n",
    "\n",
    "class GPT2RoPE(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seqlen: int,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        rate: float,\n",
    "        feed_forward_dim: int,\n",
    "        num_transformer_blocks: int,\n",
    "        rope_base: int,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.embedding_layer = TokenEmbedding(\n",
    "            seqlen, vocab_size, embed_dim, rngs=rngs\n",
    "        )\n",
    "        self.dropout = nnx.Dropout(rate=rate)\n",
    "\n",
    "        self.transformer_blocks = nnx.List(\n",
    "            [\n",
    "                TransformerBlockRoPE(\n",
    "                    embed_dim, num_heads, feed_forward_dim, dropout_rate, rope_base=rope_base, rngs=rngs\n",
    "                )\n",
    "                for _ in range(num_transformer_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.rms_norm = nnx.RMSNorm(\n",
    "            epsilon=1e-6,\n",
    "            num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), None),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        token_embedding, x = self.embedding_layer(inputs)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        x = self.rms_norm(x)\n",
    "        # Weights tying\n",
    "        outputs = token_embedding.attend(x)\n",
    "        return outputs\n",
    "\n",
    "    @nnx.jit\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "        logits = nnx.softmax(logits)\n",
    "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
    "\n",
    "    @nnx.jit\n",
    "    def generate_step(self, padded_tokens, sample_index):\n",
    "        logits = self(padded_tokens)\n",
    "        next_token = self.sample_from(logits[0][sample_index])\n",
    "        return next_token\n",
    "\n",
    "    def generate_text(self, max_tokens, start_tokens):\n",
    "        generated = []\n",
    "        print(tokenizer.decode(start_tokens), flush=True, end=\"\")\n",
    "        for i in range(max_tokens):\n",
    "            sample_index = len(start_tokens) + len(generated) - 1\n",
    "            # TODO: use attention masking for better efficiency\n",
    "            padded_tokens = jnp.array(\n",
    "                (\n",
    "                    start_tokens\n",
    "                    + generated\n",
    "                    + [0] * (seqlen - len(start_tokens) - len(generated))\n",
    "                )\n",
    "            )[None, :]\n",
    "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
    "            if (\n",
    "                next_token\n",
    "                == tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[\n",
    "                    0\n",
    "                ]\n",
    "            ):\n",
    "                break\n",
    "            generated.append(next_token)\n",
    "            # decode and print next_token\n",
    "            print(tokenizer.decode([next_token]), flush=True, end=\"\")\n",
    "        return tokenizer.decode(start_tokens + generated)\n",
    "\n",
    "def create_model(rngs):\n",
    "    return GPT2(\n",
    "        seqlen,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        dropout_rate,\n",
    "        feed_forward_dim,\n",
    "        num_transformer_blocks,\n",
    "        rngs=rngs,\n",
    "    )\n",
    "\n",
    "def create_model_RoPE(rngs):\n",
    "    return GPT2RoPE(\n",
    "        seqlen,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        dropout_rate,\n",
    "        feed_forward_dim,\n",
    "        num_transformer_blocks,\n",
    "        rope_base=3000,\n",
    "        rngs=rngs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project='GPT2-pretraining-RoPE',\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "      'architecture': GPT2_variant,\n",
    "      'dataset': 'OpenWebText',\n",
    "      'max_steps': max_steps,\n",
    "      'batch_size': batch_size,\n",
    "      'dtype': dtype,\n",
    "      'param_dtype': param_dtype,\n",
    "      'init_learning_rate': init_learning_rate,\n",
    "      'num_transformer_blocks': num_transformer_blocks,\n",
    "      'seqlen': seqlen,\n",
    "      'embed_dim': embed_dim,\n",
    "      'num_heads': num_heads,\n",
    "      'feed_forward_dim': feed_forward_dim,\n",
    "      'max_steps': max_steps,\n",
    "      'batch_size': batch_size,\n",
    "      'weight_decay': weight_decay\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, \"train.bin\")):\n",
    "    os.system(\"kaggle datasets download -d windmaple/openwebtext-gpt2 -p ./data --unzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fd4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "# From: https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/train.py#L116\n",
    "def get_batch(train_or_eval = \"train\"):\n",
    "\n",
    "    data = train_data if train_or_eval == \"train\" else val_data\n",
    "\n",
    "    ix = np.random.randint(0, len(data) - seqlen, (batch_size,))\n",
    "    x = np.stack([(data[i:i+seqlen]).astype(np.int64) for i in ix])\n",
    "    y = np.stack([(data[i+1:i+1+seqlen]).astype(np.int64) for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def loss_fn(model, batch):\n",
    "    logits = model(batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
    "    optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfdb9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_RoPE(rngs=nnx.Rngs(0))\n",
    "p_sizes = jax.tree.map(lambda p: p.size if isinstance(p, jnp.ndarray) else 0, nnx.state(model))\n",
    "import operator\n",
    "print(f\"Number of model parameters: {jax.tree.reduce(operator.add, p_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc67b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = optax.cosine_decay_schedule(\n",
    "  init_value=init_learning_rate,\n",
    "  decay_steps=max_steps\n",
    ")\n",
    "optax_chain = optax.chain(\n",
    "  optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
    ")\n",
    "optimizer = nnx.ModelAndOptimizer(model, optax_chain)\n",
    "\n",
    "train_metrics = nnx.metrics.Average('loss')\n",
    "val_metrics = nnx.metrics.Average('val_loss')\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "start_prompt = \"Once upon a time\"\n",
    "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
    "print(f\"Initial generated text:\")\n",
    "generated_text = model.generate_text(\n",
    "    seqlen//10, start_tokens\n",
    ")\n",
    "\n",
    "\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "  'val_loss': []\n",
    "}\n",
    "\n",
    "step = 0\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    input_batch, target_batch = get_batch(\"train\")\n",
    "    if len(input_batch) % len(jax.devices()) != 0: continue  # skip the remaining elements\n",
    "    train_step(model, optimizer, train_metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
    "\n",
    "    if step % 200 == 0:\n",
    "      train_loss = float(train_metrics.compute())\n",
    "      metrics_history['train_loss'].append(train_loss)\n",
    "\n",
    "      elapsed_time = time.time() - start_time\n",
    "      print(f\"Step {step + 1}, Training loss: {train_loss}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "      # eval step\n",
    "      input_val_batch, target_val_batch = get_batch('val')\n",
    "      loss, logits = loss_fn(model, jax.device_put((input_val_batch, target_val_batch), None))\n",
    "      val_metrics.update(val_loss=loss, logits=logits)\n",
    "      val_loss = float(val_metrics.compute())\n",
    "      metrics_history['val_loss'].append(val_loss)\n",
    "      wandb.log(data={'val_loss': val_loss, 'train_loss': train_loss}, step=step)\n",
    "      print(f\"Step {step + 1}, Validation loss: {val_loss}\")\n",
    "      train_metrics.reset()\n",
    "      val_metrics.reset()\n",
    "\n",
    "      start_time = time.time()\n",
    "    step += 1\n",
    "\n",
    "    if step > max_steps:\n",
    "      break\n",
    "\n",
    "# Final text generation\n",
    "print(f\"Final generated text:\")\n",
    "generated_text = model.generate_text(\n",
    "    seqlen//10, start_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(metrics_history['train_loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad35b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax.checkpoint as orbax\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "home = Path.home()\n",
    "checkpoint_path = os.path.join(str(home), \"checkpoints\")\n",
    "\n",
    "# make sure the folder is empty and usable\n",
    "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
    "\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "train_state = nnx.state(model)\n",
    "checkpointer.save(checkpoint_path, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1830c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
    "state = nnx.state(model)\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "state = checkpointer.restore(checkpoint_path, item=state)\n",
    "nnx.update(model, state)\n",
    "\n",
    "generated_text = model.generate_text(\n",
    "    seqlen//10, start_tokens\n",
    ")\n",
    "print(f\"Restored model generated text:\\n{generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
