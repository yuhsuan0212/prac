{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "373a1f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import optax, orbax\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "import numpy as np\n",
    "import tiktoken, time, wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa0188c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternative data and model parallel\n",
    "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((1, 1)), ('batch', 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea8ec3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "708224fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
    "if GPT2_variant == \"GPT2-medium\":\n",
    "    num_transformer_blocks = 24\n",
    "    seqlen = 1024\n",
    "    embed_dim = 1024\n",
    "    num_heads = 16\n",
    "    feed_forward_dim = 4 * embed_dim\n",
    "    batch_size = 32  # Can only run on TPU v3+\n",
    "else: ## Assume GPT2 otherwise\n",
    "    num_transformer_blocks = 12\n",
    "    seqlen = 1024\n",
    "    embed_dim = 768\n",
    "    num_heads = 12\n",
    "    feed_forward_dim = 4 * embed_dim\n",
    "    batch_size = 32 # TPU v3\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "max_steps = 600000*12//batch_size\n",
    "init_learning_rate = 5e-4\n",
    "weight_decay = 1e-1\n",
    "top_k = 10\n",
    "dtype = jnp.bfloat16\n",
    "param_dtype = jnp.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4405bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(seq_len):\n",
    "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
    "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), None),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "                                         dtype=dtype,\n",
    "                                         param_dtype=param_dtype,\n",
    "                                         rngs=rngs)\n",
    "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
    "                                          in_features=embed_dim,\n",
    "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), None),\n",
    "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "                                          dtype=dtype,\n",
    "                                          param_dtype=param_dtype,\n",
    "                                          rngs=rngs)\n",
    "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
    "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), None),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "                                         dtype=dtype,\n",
    "                                         param_dtype=param_dtype,\n",
    "                                         rngs=rngs)\n",
    "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
    "                                  out_features=ff_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), None),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "                                  dtype=dtype,\n",
    "                                  param_dtype=param_dtype,\n",
    "                                  rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
    "                                  out_features=embed_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), None),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "                                  dtype=dtype,\n",
    "                                  param_dtype=param_dtype,\n",
    "                                  rngs=rngs)\n",
    "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        input_shape = inputs.shape\n",
    "        bs, seq_len, emb_sz = input_shape\n",
    "\n",
    "        attention_output = self.mha(\n",
    "            inputs_q=self.layer_norm1(inputs),\n",
    "            mask=causal_attention_mask(seq_len),\n",
    "            decode=False,\n",
    "        )\n",
    "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
    "\n",
    "        # MLP\n",
    "        mlp_output = self.linear1(self.layer_norm2(x))\n",
    "        mlp_output = nnx.gelu(mlp_output)\n",
    "        mlp_output = self.linear2(mlp_output)\n",
    "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
    "\n",
    "        return x + mlp_output\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(nnx.Module):\n",
    "\n",
    "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
    "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
    "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
    "        position_embedding = self.pos_emb(positions)\n",
    "        token_embedding = self.token_emb(x)\n",
    "        return self.token_emb, token_embedding+position_embedding\n",
    "\n",
    "\n",
    "class GPT2(nnx.Module):\n",
    "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, num_heads: int, rate: float, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "                    seqlen, vocab_size, embed_dim, rngs=rngs\n",
    "                )\n",
    "        self.dropout = nnx.Dropout(rate=rate)\n",
    "\n",
    "        self.transformer_blocks = nnx.List([\n",
    "            TransformerBlock(embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                    num_features=embed_dim,\n",
    "                                    scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), None),\n",
    "                                    bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), None),\n",
    "                                    dtype=dtype,\n",
    "                                    param_dtype=param_dtype,\n",
    "                                    rngs=rngs)\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        token_embedding, x = self.embedding_layer(inputs)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        x = self.layer_norm(x)\n",
    "        # Weights tying\n",
    "        outputs = token_embedding.attend(x)\n",
    "        return outputs\n",
    "\n",
    "    @nnx.jit\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "        logits = nnx.softmax(logits)\n",
    "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
    "\n",
    "    @nnx.jit\n",
    "    def generate_step(self, padded_tokens, sample_index):\n",
    "        logits = self(padded_tokens)\n",
    "        next_token = self.sample_from(logits[0][sample_index])\n",
    "        return next_token\n",
    "\n",
    "    def generate_text(self, max_tokens, start_tokens):\n",
    "        generated = []\n",
    "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
    "        for i in range(max_tokens):\n",
    "            sample_index = len(start_tokens) + len(generated) - 1\n",
    "            # TODO: use attention masking for better efficiency\n",
    "            padded_tokens = jnp.array((start_tokens + generated + [0] * (seqlen - len(start_tokens) - len(generated))))[None, :]\n",
    "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
    "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
    "              break\n",
    "            generated.append(next_token)\n",
    "            # decode and print next_token\n",
    "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
    "        return tokenizer.decode(start_tokens + generated)\n",
    "\n",
    "def create_model(rngs):\n",
    "    return GPT2(seqlen, vocab_size, embed_dim, num_heads, dropout_rate, feed_forward_dim, num_transformer_blocks, rngs=rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "864a42ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>11.5088</td></tr><tr><td>val_loss</td><td>10.0625</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-oath-2</strong> at: <a href='https://wandb.ai/kuanglong777/GPT2-pretraining/runs/ev72iizf' target=\"_blank\">https://wandb.ai/kuanglong777/GPT2-pretraining/runs/ev72iizf</a><br> View project at: <a href='https://wandb.ai/kuanglong777/GPT2-pretraining' target=\"_blank\">https://wandb.ai/kuanglong777/GPT2-pretraining</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250930_024128-ev72iizf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/prac/wandb/run-20250930_024642-wdls9di1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kuanglong777/GPT2-pretraining/runs/wdls9di1' target=\"_blank\">dainty-pond-3</a></strong> to <a href='https://wandb.ai/kuanglong777/GPT2-pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kuanglong777/GPT2-pretraining' target=\"_blank\">https://wandb.ai/kuanglong777/GPT2-pretraining</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kuanglong777/GPT2-pretraining/runs/wdls9di1' target=\"_blank\">https://wandb.ai/kuanglong777/GPT2-pretraining/runs/wdls9di1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kuanglong777/GPT2-pretraining/runs/wdls9di1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7579cf713350>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project='GPT2-pretraining',\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "      'architecture': GPT2_variant,\n",
    "      'dataset': 'OpenWebText',\n",
    "      'max_steps': max_steps,\n",
    "      'batch_size': batch_size,\n",
    "      'dtype': dtype,\n",
    "      'param_dtype': param_dtype,\n",
    "      'init_learning_rate': init_learning_rate,\n",
    "      'num_transformer_blocks': num_transformer_blocks,\n",
    "      'seqlen': seqlen,\n",
    "      'embed_dim': embed_dim,\n",
    "      'num_heads': num_heads,\n",
    "      'feed_forward_dim': feed_forward_dim,\n",
    "      'max_steps': max_steps,\n",
    "      'batch_size': batch_size,\n",
    "      'weight_decay': weight_decay\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d0ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, \"train.bin\")):\n",
    "    os.system(\"kaggle datasets download -d windmaple/openwebtext-gpt2 -p ./data --unzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "262fd4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "# From: https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/train.py#L116\n",
    "def get_batch(train_or_eval = \"train\"):\n",
    "\n",
    "    data = train_data if train_or_eval == \"train\" else val_data\n",
    "\n",
    "    ix = np.random.randint(0, len(data) - seqlen, (batch_size,))\n",
    "    x = np.stack([(data[i:i+seqlen]).astype(np.int64) for i in ix])\n",
    "    y = np.stack([(data[i+1:i+1+seqlen]).astype(np.int64) for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1dbd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def loss_fn(model, batch):\n",
    "    logits = model(batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
    "    optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cfdb9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 124439808\n"
     ]
    }
   ],
   "source": [
    "model = create_model(rngs=nnx.Rngs(0))\n",
    "\n",
    "p_sizes = jax.tree.map(lambda p: p.size if isinstance(p, jnp.ndarray) else 0, nnx.state(model))\n",
    "import operator\n",
    "print(f\"Number of model parameters: {jax.tree.reduce(operator.add, p_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fc67b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial generated text:\n",
      "Once upon a time secured PNG flattened principalsukuurances secureduku secureduku securedukuuku monopol Gothic monopoluku monopol Gothic Caucas monopol secureduku Gothic Gothic Riding Gothic Caucas monopolurreduku Caucas Mel Meluku Caucas monopol Gothicurredusions possess possess grizzusions Gothicukuuku Caucas monopolurredilt Caucasurredreditshengurreduku Caucas monopol Caucas response Xanderredits Also Alsoredits DE Xander Xanderurred monopolurred DEurredredits grizzurred Alsoredits DEreditsurredredits DE Caucas grizzredits DE guarantee encompassesredits encompasses grizz encompasses grizz Lutheran serotonin serotonin serotonin encompasses encompasses grizzStep 1, Training loss: 11.512091636657715, Elapsed Time: 19.24 seconds\n",
      "Step 1, Validation loss: 10.0\n",
      "Step 201, Training loss: 7.126930236816406, Elapsed Time: 67.58 seconds\n",
      "Step 201, Validation loss: 6.53125\n",
      "Step 401, Training loss: 6.3200812339782715, Elapsed Time: 50.62 seconds\n",
      "Step 401, Validation loss: 6.0625\n",
      "Step 601, Training loss: 6.074289321899414, Elapsed Time: 49.70 seconds\n",
      "Step 601, Validation loss: 5.90625\n",
      "Step 801, Training loss: 5.869531631469727, Elapsed Time: 50.60 seconds\n",
      "Step 801, Validation loss: 5.875\n",
      "Step 1001, Training loss: 5.655073642730713, Elapsed Time: 50.23 seconds\n",
      "Step 1001, Validation loss: 5.5625\n",
      "Step 1201, Training loss: 5.465212821960449, Elapsed Time: 49.22 seconds\n",
      "Step 1201, Validation loss: 5.46875\n",
      "Step 1401, Training loss: 5.290008544921875, Elapsed Time: 50.60 seconds\n",
      "Step 1401, Validation loss: 5.15625\n",
      "Step 1601, Training loss: 5.135811805725098, Elapsed Time: 50.04 seconds\n",
      "Step 1601, Validation loss: 5.15625\n",
      "Step 1801, Training loss: 5.010371685028076, Elapsed Time: 51.29 seconds\n",
      "Step 1801, Validation loss: 4.90625\n",
      "Step 2001, Training loss: 4.886466979980469, Elapsed Time: 52.09 seconds\n",
      "Step 2001, Validation loss: 4.75\n",
      "Step 2201, Training loss: 4.763071537017822, Elapsed Time: 50.37 seconds\n",
      "Step 2201, Validation loss: 4.65625\n",
      "Step 2401, Training loss: 4.643667221069336, Elapsed Time: 50.50 seconds\n",
      "Step 2401, Validation loss: 4.625\n",
      "Step 2601, Training loss: 4.561718940734863, Elapsed Time: 49.76 seconds\n",
      "Step 2601, Validation loss: 4.625\n",
      "Step 2801, Training loss: 4.492053508758545, Elapsed Time: 49.35 seconds\n",
      "Step 2801, Validation loss: 4.40625\n",
      "Step 3001, Training loss: 4.444037914276123, Elapsed Time: 49.35 seconds\n",
      "Step 3001, Validation loss: 4.5\n",
      "Step 3201, Training loss: 4.382781982421875, Elapsed Time: 48.94 seconds\n",
      "Step 3201, Validation loss: 4.25\n",
      "Step 3401, Training loss: 4.338492393493652, Elapsed Time: 48.98 seconds\n",
      "Step 3401, Validation loss: 4.3125\n",
      "Step 3601, Training loss: 4.291966915130615, Elapsed Time: 47.67 seconds\n",
      "Step 3601, Validation loss: 4.21875\n",
      "Step 3801, Training loss: 4.255937099456787, Elapsed Time: 47.42 seconds\n",
      "Step 3801, Validation loss: 4.28125\n",
      "Step 4001, Training loss: 4.232266902923584, Elapsed Time: 47.24 seconds\n",
      "Step 4001, Validation loss: 4.15625\n",
      "Step 4201, Training loss: 4.1977620124816895, Elapsed Time: 46.89 seconds\n",
      "Step 4201, Validation loss: 4.15625\n",
      "Step 4401, Training loss: 4.168002128601074, Elapsed Time: 47.10 seconds\n",
      "Step 4401, Validation loss: 4.125\n",
      "Step 4601, Training loss: 4.146422863006592, Elapsed Time: 47.96 seconds\n",
      "Step 4601, Validation loss: 4.15625\n",
      "Step 4801, Training loss: 4.125262260437012, Elapsed Time: 47.86 seconds\n",
      "Step 4801, Validation loss: 4.0625\n",
      "Step 5001, Training loss: 4.084826469421387, Elapsed Time: 46.07 seconds\n",
      "Step 5001, Validation loss: 4.15625\n",
      "Step 5201, Training loss: 4.064914703369141, Elapsed Time: 48.12 seconds\n",
      "Step 5201, Validation loss: 4.03125\n",
      "Step 5401, Training loss: 4.040982723236084, Elapsed Time: 46.34 seconds\n",
      "Step 5401, Validation loss: 4.09375\n",
      "Step 5601, Training loss: 4.032593727111816, Elapsed Time: 46.62 seconds\n",
      "Step 5601, Validation loss: 4.03125\n",
      "Step 5801, Training loss: 4.002296447753906, Elapsed Time: 45.83 seconds\n",
      "Step 5801, Validation loss: 4.0625\n",
      "Step 6001, Training loss: 3.9919795989990234, Elapsed Time: 46.19 seconds\n",
      "Step 6001, Validation loss: 3.953125\n",
      "Step 6201, Training loss: 3.973080635070801, Elapsed Time: 45.78 seconds\n",
      "Step 6201, Validation loss: 4.0625\n",
      "Step 6401, Training loss: 3.9517982006073, Elapsed Time: 46.42 seconds\n",
      "Step 6401, Validation loss: 3.984375\n",
      "Step 6601, Training loss: 3.9464917182922363, Elapsed Time: 46.13 seconds\n",
      "Step 6601, Validation loss: 3.9375\n",
      "Step 6801, Training loss: 3.930172920227051, Elapsed Time: 46.08 seconds\n",
      "Step 6801, Validation loss: 3.9375\n",
      "Step 7001, Training loss: 3.9325473308563232, Elapsed Time: 45.78 seconds\n",
      "Step 7001, Validation loss: 3.875\n",
      "Step 7201, Training loss: 3.9107396602630615, Elapsed Time: 45.78 seconds\n",
      "Step 7201, Validation loss: 3.796875\n",
      "Step 7401, Training loss: 3.8951170444488525, Elapsed Time: 45.92 seconds\n",
      "Step 7401, Validation loss: 3.96875\n",
      "Step 7601, Training loss: 3.8821399211883545, Elapsed Time: 46.01 seconds\n",
      "Step 7601, Validation loss: 3.890625\n",
      "Step 7801, Training loss: 3.8652358055114746, Elapsed Time: 45.53 seconds\n",
      "Step 7801, Validation loss: 3.8125\n",
      "Step 8001, Training loss: 3.85947322845459, Elapsed Time: 46.71 seconds\n",
      "Step 8001, Validation loss: 3.8125\n",
      "Step 8201, Training loss: 3.8392088413238525, Elapsed Time: 46.44 seconds\n",
      "Step 8201, Validation loss: 3.75\n",
      "Step 8401, Training loss: 3.8311991691589355, Elapsed Time: 46.69 seconds\n",
      "Step 8401, Validation loss: 3.890625\n",
      "Step 8601, Training loss: 3.81825852394104, Elapsed Time: 46.11 seconds\n",
      "Step 8601, Validation loss: 3.953125\n",
      "Step 8801, Training loss: 3.8191170692443848, Elapsed Time: 46.86 seconds\n",
      "Step 8801, Validation loss: 3.875\n",
      "Step 9001, Training loss: 3.81097674369812, Elapsed Time: 46.08 seconds\n",
      "Step 9001, Validation loss: 3.828125\n",
      "Step 9201, Training loss: 3.798516035079956, Elapsed Time: 45.76 seconds\n",
      "Step 9201, Validation loss: 3.75\n",
      "Step 9401, Training loss: 3.7954607009887695, Elapsed Time: 46.30 seconds\n",
      "Step 9401, Validation loss: 3.734375\n",
      "Step 9601, Training loss: 3.783289670944214, Elapsed Time: 45.63 seconds\n",
      "Step 9601, Validation loss: 3.828125\n",
      "Step 9801, Training loss: 3.7743632793426514, Elapsed Time: 46.23 seconds\n",
      "Step 9801, Validation loss: 3.75\n",
      "Step 10001, Training loss: 3.7654483318328857, Elapsed Time: 46.99 seconds\n",
      "Step 10001, Validation loss: 3.8125\n",
      "Step 10201, Training loss: 3.7640209197998047, Elapsed Time: 46.12 seconds\n",
      "Step 10201, Validation loss: 3.734375\n",
      "Step 10401, Training loss: 3.760894298553467, Elapsed Time: 45.65 seconds\n",
      "Step 10401, Validation loss: 3.765625\n",
      "Step 10601, Training loss: 3.753865957260132, Elapsed Time: 45.83 seconds\n",
      "Step 10601, Validation loss: 3.796875\n",
      "Step 10801, Training loss: 3.7340734004974365, Elapsed Time: 45.93 seconds\n",
      "Step 10801, Validation loss: 3.640625\n",
      "Step 11001, Training loss: 3.736269474029541, Elapsed Time: 45.66 seconds\n",
      "Step 11001, Validation loss: 3.75\n",
      "Step 11201, Training loss: 3.7324635982513428, Elapsed Time: 45.66 seconds\n",
      "Step 11201, Validation loss: 3.65625\n",
      "Step 11401, Training loss: 3.7269845008850098, Elapsed Time: 45.93 seconds\n",
      "Step 11401, Validation loss: 3.734375\n",
      "Step 11601, Training loss: 3.71751070022583, Elapsed Time: 49.10 seconds\n",
      "Step 11601, Validation loss: 3.765625\n",
      "Step 11801, Training loss: 3.705289840698242, Elapsed Time: 46.12 seconds\n",
      "Step 11801, Validation loss: 3.671875\n",
      "Step 12001, Training loss: 3.7221457958221436, Elapsed Time: 45.89 seconds\n",
      "Step 12001, Validation loss: 3.671875\n",
      "Step 12201, Training loss: 3.699323892593384, Elapsed Time: 45.77 seconds\n",
      "Step 12201, Validation loss: 3.75\n",
      "Step 12401, Training loss: 3.6965835094451904, Elapsed Time: 46.47 seconds\n",
      "Step 12401, Validation loss: 3.703125\n",
      "Step 12601, Training loss: 3.6930363178253174, Elapsed Time: 45.68 seconds\n",
      "Step 12601, Validation loss: 3.78125\n",
      "Step 12801, Training loss: 3.6846392154693604, Elapsed Time: 46.04 seconds\n",
      "Step 12801, Validation loss: 3.609375\n",
      "Step 13001, Training loss: 3.683441638946533, Elapsed Time: 45.67 seconds\n",
      "Step 13001, Validation loss: 3.671875\n",
      "Step 13201, Training loss: 3.6791999340057373, Elapsed Time: 46.75 seconds\n",
      "Step 13201, Validation loss: 3.671875\n",
      "Step 13401, Training loss: 3.6642143726348877, Elapsed Time: 46.49 seconds\n",
      "Step 13401, Validation loss: 3.765625\n",
      "Step 13601, Training loss: 3.669616222381592, Elapsed Time: 48.56 seconds\n",
      "Step 13601, Validation loss: 3.671875\n",
      "Step 13801, Training loss: 3.658067464828491, Elapsed Time: 47.47 seconds\n",
      "Step 13801, Validation loss: 3.734375\n",
      "Step 14001, Training loss: 3.6604483127593994, Elapsed Time: 52.06 seconds\n",
      "Step 14001, Validation loss: 3.578125\n",
      "Step 14201, Training loss: 3.653118133544922, Elapsed Time: 54.03 seconds\n",
      "Step 14201, Validation loss: 3.640625\n",
      "Step 14401, Training loss: 3.6509761810302734, Elapsed Time: 53.57 seconds\n",
      "Step 14401, Validation loss: 3.578125\n",
      "Step 14601, Training loss: 3.649042844772339, Elapsed Time: 53.29 seconds\n",
      "Step 14601, Validation loss: 3.640625\n",
      "Step 14801, Training loss: 3.6607909202575684, Elapsed Time: 53.82 seconds\n",
      "Step 14801, Validation loss: 3.8125\n",
      "Step 15001, Training loss: 3.6430513858795166, Elapsed Time: 50.07 seconds\n",
      "Step 15001, Validation loss: 3.515625\n",
      "Step 15201, Training loss: 3.63832426071167, Elapsed Time: 47.96 seconds\n",
      "Step 15201, Validation loss: 3.625\n",
      "Step 15401, Training loss: 3.6336774826049805, Elapsed Time: 47.62 seconds\n",
      "Step 15401, Validation loss: 3.734375\n",
      "Step 15601, Training loss: 3.63309383392334, Elapsed Time: 47.09 seconds\n",
      "Step 15601, Validation loss: 3.609375\n",
      "Step 15801, Training loss: 3.6222386360168457, Elapsed Time: 47.74 seconds\n",
      "Step 15801, Validation loss: 3.671875\n",
      "Step 16001, Training loss: 3.624112844467163, Elapsed Time: 46.27 seconds\n",
      "Step 16001, Validation loss: 3.765625\n",
      "Step 16201, Training loss: 3.6218202114105225, Elapsed Time: 46.75 seconds\n",
      "Step 16201, Validation loss: 3.609375\n",
      "Step 16401, Training loss: 3.6218364238739014, Elapsed Time: 50.36 seconds\n",
      "Step 16401, Validation loss: 3.671875\n",
      "Step 16601, Training loss: 3.6173970699310303, Elapsed Time: 50.06 seconds\n",
      "Step 16601, Validation loss: 3.59375\n",
      "Step 16801, Training loss: 3.6140596866607666, Elapsed Time: 50.06 seconds\n",
      "Step 16801, Validation loss: 3.515625\n",
      "Step 17001, Training loss: 3.6120827198028564, Elapsed Time: 49.59 seconds\n",
      "Step 17001, Validation loss: 3.703125\n",
      "Step 17201, Training loss: 3.606215238571167, Elapsed Time: 50.70 seconds\n",
      "Step 17201, Validation loss: 3.703125\n",
      "Step 17401, Training loss: 3.6047565937042236, Elapsed Time: 50.04 seconds\n",
      "Step 17401, Validation loss: 3.515625\n",
      "Step 17601, Training loss: 3.59726619720459, Elapsed Time: 46.36 seconds\n",
      "Step 17601, Validation loss: 3.703125\n",
      "Step 17801, Training loss: 3.598397731781006, Elapsed Time: 46.76 seconds\n",
      "Step 17801, Validation loss: 3.578125\n",
      "Step 18001, Training loss: 3.5893566608428955, Elapsed Time: 47.13 seconds\n",
      "Step 18001, Validation loss: 3.59375\n",
      "Step 18201, Training loss: 3.5882441997528076, Elapsed Time: 46.52 seconds\n",
      "Step 18201, Validation loss: 3.6875\n",
      "Step 18401, Training loss: 3.591824769973755, Elapsed Time: 46.52 seconds\n",
      "Step 18401, Validation loss: 3.515625\n",
      "Step 18601, Training loss: 3.588048219680786, Elapsed Time: 46.52 seconds\n",
      "Step 18601, Validation loss: 3.609375\n",
      "Step 18801, Training loss: 3.581040859222412, Elapsed Time: 46.52 seconds\n",
      "Step 18801, Validation loss: 3.59375\n",
      "Step 19001, Training loss: 3.5847392082214355, Elapsed Time: 46.80 seconds\n",
      "Step 19001, Validation loss: 3.5\n",
      "Step 19201, Training loss: 3.5749566555023193, Elapsed Time: 46.17 seconds\n",
      "Step 19201, Validation loss: 3.703125\n",
      "Step 19401, Training loss: 3.577956199645996, Elapsed Time: 46.51 seconds\n",
      "Step 19401, Validation loss: 3.625\n",
      "Step 19601, Training loss: 3.5746920108795166, Elapsed Time: 46.81 seconds\n",
      "Step 19601, Validation loss: 3.609375\n",
      "Step 19801, Training loss: 3.574009418487549, Elapsed Time: 46.41 seconds\n",
      "Step 19801, Validation loss: 3.5625\n",
      "Step 20001, Training loss: 3.565483570098877, Elapsed Time: 46.77 seconds\n",
      "Step 20001, Validation loss: 3.609375\n",
      "Step 20201, Training loss: 3.5732014179229736, Elapsed Time: 47.46 seconds\n",
      "Step 20201, Validation loss: 3.59375\n",
      "Step 20401, Training loss: 3.556797981262207, Elapsed Time: 47.56 seconds\n",
      "Step 20401, Validation loss: 3.53125\n",
      "Step 20601, Training loss: 3.5574941635131836, Elapsed Time: 46.33 seconds\n",
      "Step 20601, Validation loss: 3.5\n",
      "Step 20801, Training loss: 3.5660064220428467, Elapsed Time: 46.71 seconds\n",
      "Step 20801, Validation loss: 3.546875\n",
      "Step 21001, Training loss: 3.552572011947632, Elapsed Time: 46.19 seconds\n",
      "Step 21001, Validation loss: 3.578125\n",
      "Step 21201, Training loss: 3.557422637939453, Elapsed Time: 46.60 seconds\n",
      "Step 21201, Validation loss: 3.5\n",
      "Step 21401, Training loss: 3.548369884490967, Elapsed Time: 46.30 seconds\n",
      "Step 21401, Validation loss: 3.609375\n",
      "Step 21601, Training loss: 3.553729772567749, Elapsed Time: 46.69 seconds\n",
      "Step 21601, Validation loss: 3.578125\n",
      "Step 21801, Training loss: 3.5448951721191406, Elapsed Time: 47.17 seconds\n",
      "Step 21801, Validation loss: 3.609375\n",
      "Step 22001, Training loss: 3.5410826206207275, Elapsed Time: 46.81 seconds\n",
      "Step 22001, Validation loss: 3.484375\n",
      "Step 22201, Training loss: 3.5436110496520996, Elapsed Time: 47.38 seconds\n",
      "Step 22201, Validation loss: 3.546875\n",
      "Step 22401, Training loss: 3.5544941425323486, Elapsed Time: 47.06 seconds\n",
      "Step 22401, Validation loss: 3.5625\n",
      "Step 22601, Training loss: 3.541574001312256, Elapsed Time: 46.48 seconds\n",
      "Step 22601, Validation loss: 3.421875\n",
      "Step 22801, Training loss: 3.5395824909210205, Elapsed Time: 46.95 seconds\n",
      "Step 22801, Validation loss: 3.5625\n",
      "Step 23001, Training loss: 3.54547119140625, Elapsed Time: 46.73 seconds\n",
      "Step 23001, Validation loss: 3.484375\n",
      "Step 23201, Training loss: 3.5344128608703613, Elapsed Time: 46.52 seconds\n",
      "Step 23201, Validation loss: 3.59375\n",
      "Step 23401, Training loss: 3.5476481914520264, Elapsed Time: 47.12 seconds\n",
      "Step 23401, Validation loss: 3.453125\n",
      "Step 23601, Training loss: 3.5292794704437256, Elapsed Time: 46.77 seconds\n",
      "Step 23601, Validation loss: 3.796875\n",
      "Step 23801, Training loss: 3.5342636108398438, Elapsed Time: 46.32 seconds\n",
      "Step 23801, Validation loss: 3.578125\n",
      "Step 24001, Training loss: 3.5314412117004395, Elapsed Time: 46.77 seconds\n",
      "Step 24001, Validation loss: 3.546875\n",
      "Step 24201, Training loss: 3.5286004543304443, Elapsed Time: 47.05 seconds\n",
      "Step 24201, Validation loss: 3.578125\n",
      "Step 24401, Training loss: 3.521663188934326, Elapsed Time: 46.98 seconds\n",
      "Step 24401, Validation loss: 3.59375\n",
      "Step 24601, Training loss: 3.5150325298309326, Elapsed Time: 46.40 seconds\n",
      "Step 24601, Validation loss: 3.453125\n",
      "Step 24801, Training loss: 3.519838571548462, Elapsed Time: 46.74 seconds\n",
      "Step 24801, Validation loss: 3.546875\n",
      "Step 25001, Training loss: 3.523228168487549, Elapsed Time: 46.37 seconds\n",
      "Step 25001, Validation loss: 3.46875\n",
      "Step 25201, Training loss: 3.5288968086242676, Elapsed Time: 46.34 seconds\n",
      "Step 25201, Validation loss: 3.546875\n",
      "Step 25401, Training loss: 3.5117673873901367, Elapsed Time: 46.30 seconds\n",
      "Step 25401, Validation loss: 3.53125\n",
      "Step 25601, Training loss: 3.520965099334717, Elapsed Time: 46.87 seconds\n",
      "Step 25601, Validation loss: 3.546875\n",
      "Step 25801, Training loss: 3.5096516609191895, Elapsed Time: 48.10 seconds\n",
      "Step 25801, Validation loss: 3.46875\n",
      "Step 26001, Training loss: 3.504268169403076, Elapsed Time: 50.89 seconds\n",
      "Step 26001, Validation loss: 3.53125\n",
      "Step 26201, Training loss: 3.5156893730163574, Elapsed Time: 49.69 seconds\n",
      "Step 26201, Validation loss: 3.40625\n",
      "Step 26401, Training loss: 3.525392770767212, Elapsed Time: 46.95 seconds\n",
      "Step 26401, Validation loss: 3.5\n",
      "Step 26601, Training loss: 3.516528606414795, Elapsed Time: 47.17 seconds\n",
      "Step 26601, Validation loss: 3.359375\n",
      "Step 26801, Training loss: 3.506598711013794, Elapsed Time: 47.61 seconds\n",
      "Step 26801, Validation loss: 3.484375\n",
      "Step 27001, Training loss: 3.5027010440826416, Elapsed Time: 50.29 seconds\n",
      "Step 27001, Validation loss: 3.5625\n",
      "Step 27201, Training loss: 3.507122039794922, Elapsed Time: 50.22 seconds\n",
      "Step 27201, Validation loss: 3.578125\n",
      "Step 27401, Training loss: 3.5003573894500732, Elapsed Time: 54.53 seconds\n",
      "Step 27401, Validation loss: 3.40625\n",
      "Step 27601, Training loss: 3.5061748027801514, Elapsed Time: 54.74 seconds\n",
      "Step 27601, Validation loss: 3.53125\n",
      "Step 27801, Training loss: 3.5044000148773193, Elapsed Time: 54.42 seconds\n",
      "Step 27801, Validation loss: 3.5625\n",
      "Step 28001, Training loss: 3.493825674057007, Elapsed Time: 54.47 seconds\n",
      "Step 28001, Validation loss: 3.578125\n",
      "Step 28201, Training loss: 3.499497890472412, Elapsed Time: 54.47 seconds\n",
      "Step 28201, Validation loss: 3.484375\n",
      "Step 28401, Training loss: 3.4963722229003906, Elapsed Time: 54.51 seconds\n",
      "Step 28401, Validation loss: 3.40625\n",
      "Step 28601, Training loss: 3.497319459915161, Elapsed Time: 55.79 seconds\n",
      "Step 28601, Validation loss: 3.375\n",
      "Step 28801, Training loss: 3.4899730682373047, Elapsed Time: 54.45 seconds\n",
      "Step 28801, Validation loss: 3.4375\n",
      "Step 29001, Training loss: 3.490119218826294, Elapsed Time: 52.55 seconds\n",
      "Step 29001, Validation loss: 3.515625\n",
      "Step 29201, Training loss: 3.4915177822113037, Elapsed Time: 45.93 seconds\n",
      "Step 29201, Validation loss: 3.546875\n",
      "Step 29401, Training loss: 3.4929046630859375, Elapsed Time: 47.08 seconds\n",
      "Step 29401, Validation loss: 3.375\n",
      "Step 29601, Training loss: 3.494837760925293, Elapsed Time: 45.82 seconds\n",
      "Step 29601, Validation loss: 3.609375\n",
      "Step 29801, Training loss: 3.4895293712615967, Elapsed Time: 45.73 seconds\n",
      "Step 29801, Validation loss: 3.421875\n",
      "Step 30001, Training loss: 3.4876506328582764, Elapsed Time: 45.87 seconds\n",
      "Step 30001, Validation loss: 3.40625\n",
      "Step 30201, Training loss: 3.478080987930298, Elapsed Time: 45.98 seconds\n",
      "Step 30201, Validation loss: 3.421875\n",
      "Step 30401, Training loss: 3.4813859462738037, Elapsed Time: 46.59 seconds\n",
      "Step 30401, Validation loss: 3.40625\n",
      "Step 30601, Training loss: 3.494880199432373, Elapsed Time: 46.67 seconds\n",
      "Step 30601, Validation loss: 3.484375\n",
      "Step 30801, Training loss: 3.489807605743408, Elapsed Time: 45.89 seconds\n",
      "Step 30801, Validation loss: 3.40625\n",
      "Step 31001, Training loss: 3.4923155307769775, Elapsed Time: 46.18 seconds\n",
      "Step 31001, Validation loss: 3.453125\n",
      "Step 31201, Training loss: 3.478947877883911, Elapsed Time: 45.80 seconds\n",
      "Step 31201, Validation loss: 3.546875\n",
      "Step 31401, Training loss: 3.477712392807007, Elapsed Time: 47.19 seconds\n",
      "Step 31401, Validation loss: 3.4375\n",
      "Step 31601, Training loss: 3.4834401607513428, Elapsed Time: 46.38 seconds\n",
      "Step 31601, Validation loss: 3.5\n",
      "Step 31801, Training loss: 3.4808342456817627, Elapsed Time: 47.28 seconds\n",
      "Step 31801, Validation loss: 3.53125\n",
      "Step 32001, Training loss: 3.4751124382019043, Elapsed Time: 46.09 seconds\n",
      "Step 32001, Validation loss: 3.53125\n",
      "Step 32201, Training loss: 3.4759089946746826, Elapsed Time: 45.57 seconds\n",
      "Step 32201, Validation loss: 3.4375\n",
      "Step 32401, Training loss: 3.4642977714538574, Elapsed Time: 45.42 seconds\n",
      "Step 32401, Validation loss: 3.4375\n",
      "Step 32601, Training loss: 3.475184440612793, Elapsed Time: 46.11 seconds\n",
      "Step 32601, Validation loss: 3.546875\n",
      "Step 32801, Training loss: 3.4655652046203613, Elapsed Time: 46.05 seconds\n",
      "Step 32801, Validation loss: 3.46875\n",
      "Step 33001, Training loss: 3.472304105758667, Elapsed Time: 45.88 seconds\n",
      "Step 33001, Validation loss: 3.4375\n",
      "Step 33201, Training loss: 3.4737355709075928, Elapsed Time: 46.05 seconds\n",
      "Step 33201, Validation loss: 3.53125\n",
      "Step 33401, Training loss: 3.4782824516296387, Elapsed Time: 46.00 seconds\n",
      "Step 33401, Validation loss: 3.421875\n",
      "Step 33601, Training loss: 3.469719886779785, Elapsed Time: 45.94 seconds\n",
      "Step 33601, Validation loss: 3.375\n",
      "Step 33801, Training loss: 3.460916757583618, Elapsed Time: 45.58 seconds\n",
      "Step 33801, Validation loss: 3.40625\n",
      "Step 34001, Training loss: 3.4580986499786377, Elapsed Time: 46.66 seconds\n",
      "Step 34001, Validation loss: 3.375\n",
      "Step 34201, Training loss: 3.4574873447418213, Elapsed Time: 45.55 seconds\n",
      "Step 34201, Validation loss: 3.46875\n",
      "Step 34401, Training loss: 3.4644951820373535, Elapsed Time: 46.36 seconds\n",
      "Step 34401, Validation loss: 3.390625\n",
      "Step 34601, Training loss: 3.462616443634033, Elapsed Time: 45.84 seconds\n",
      "Step 34601, Validation loss: 3.484375\n",
      "Step 34801, Training loss: 3.4601693153381348, Elapsed Time: 45.94 seconds\n",
      "Step 34801, Validation loss: 3.46875\n",
      "Step 35001, Training loss: 3.457062005996704, Elapsed Time: 45.96 seconds\n",
      "Step 35001, Validation loss: 3.46875\n",
      "Step 35201, Training loss: 3.4624416828155518, Elapsed Time: 46.71 seconds\n",
      "Step 35201, Validation loss: 3.5\n",
      "Step 35401, Training loss: 3.4560353755950928, Elapsed Time: 46.87 seconds\n",
      "Step 35401, Validation loss: 3.421875\n",
      "Step 35601, Training loss: 3.4649548530578613, Elapsed Time: 46.87 seconds\n",
      "Step 35601, Validation loss: 3.5\n",
      "Step 35801, Training loss: 3.4607248306274414, Elapsed Time: 46.52 seconds\n",
      "Step 35801, Validation loss: 3.515625\n",
      "Step 36001, Training loss: 3.45808744430542, Elapsed Time: 46.30 seconds\n",
      "Step 36001, Validation loss: 3.515625\n",
      "Step 36201, Training loss: 3.457150459289551, Elapsed Time: 46.18 seconds\n",
      "Step 36201, Validation loss: 3.625\n",
      "Step 36401, Training loss: 3.4471187591552734, Elapsed Time: 46.05 seconds\n",
      "Step 36401, Validation loss: 3.296875\n",
      "Step 36601, Training loss: 3.455186367034912, Elapsed Time: 46.61 seconds\n",
      "Step 36601, Validation loss: 3.546875\n",
      "Step 36801, Training loss: 3.460801601409912, Elapsed Time: 45.82 seconds\n",
      "Step 36801, Validation loss: 3.546875\n",
      "Step 37001, Training loss: 3.453638792037964, Elapsed Time: 45.76 seconds\n",
      "Step 37001, Validation loss: 3.484375\n",
      "Step 37201, Training loss: 3.4458093643188477, Elapsed Time: 45.91 seconds\n",
      "Step 37201, Validation loss: 3.390625\n",
      "Step 37401, Training loss: 3.448590040206909, Elapsed Time: 46.26 seconds\n",
      "Step 37401, Validation loss: 3.40625\n",
      "Step 37601, Training loss: 3.44992995262146, Elapsed Time: 46.26 seconds\n",
      "Step 37601, Validation loss: 3.390625\n",
      "Step 37801, Training loss: 3.4490127563476562, Elapsed Time: 46.30 seconds\n",
      "Step 37801, Validation loss: 3.46875\n",
      "Step 38001, Training loss: 3.4451141357421875, Elapsed Time: 46.21 seconds\n",
      "Step 38001, Validation loss: 3.46875\n",
      "Step 38201, Training loss: 3.4428672790527344, Elapsed Time: 46.55 seconds\n",
      "Step 38201, Validation loss: 3.546875\n",
      "Step 38401, Training loss: 3.441721200942993, Elapsed Time: 45.62 seconds\n",
      "Step 38401, Validation loss: 3.328125\n",
      "Step 38601, Training loss: 3.4392051696777344, Elapsed Time: 46.20 seconds\n",
      "Step 38601, Validation loss: 3.4375\n",
      "Step 38801, Training loss: 3.4332029819488525, Elapsed Time: 45.81 seconds\n",
      "Step 38801, Validation loss: 3.390625\n",
      "Step 39001, Training loss: 3.4358067512512207, Elapsed Time: 45.91 seconds\n",
      "Step 39001, Validation loss: 3.40625\n",
      "Step 39201, Training loss: 3.43493914604187, Elapsed Time: 46.02 seconds\n",
      "Step 39201, Validation loss: 3.484375\n",
      "Step 39401, Training loss: 3.4375667572021484, Elapsed Time: 45.72 seconds\n",
      "Step 39401, Validation loss: 3.5\n",
      "Step 39601, Training loss: 3.442019462585449, Elapsed Time: 45.87 seconds\n",
      "Step 39601, Validation loss: 3.375\n",
      "Step 39801, Training loss: 3.446439027786255, Elapsed Time: 46.41 seconds\n",
      "Step 39801, Validation loss: 3.421875\n",
      "Step 40001, Training loss: 3.4478836059570312, Elapsed Time: 45.97 seconds\n",
      "Step 40001, Validation loss: 3.453125\n",
      "Step 40201, Training loss: 3.436241388320923, Elapsed Time: 45.91 seconds\n",
      "Step 40201, Validation loss: 3.53125\n",
      "Step 40401, Training loss: 3.4427199363708496, Elapsed Time: 47.00 seconds\n",
      "Step 40401, Validation loss: 3.484375\n",
      "Step 40601, Training loss: 3.4352900981903076, Elapsed Time: 45.82 seconds\n",
      "Step 40601, Validation loss: 3.453125\n",
      "Step 40801, Training loss: 3.433877468109131, Elapsed Time: 45.75 seconds\n",
      "Step 40801, Validation loss: 3.328125\n",
      "Step 41001, Training loss: 3.4331817626953125, Elapsed Time: 45.34 seconds\n",
      "Step 41001, Validation loss: 3.5\n",
      "Step 41201, Training loss: 3.4366414546966553, Elapsed Time: 46.05 seconds\n",
      "Step 41201, Validation loss: 3.46875\n",
      "Step 41401, Training loss: 3.433912992477417, Elapsed Time: 46.01 seconds\n",
      "Step 41401, Validation loss: 3.484375\n",
      "Step 41601, Training loss: 3.424882411956787, Elapsed Time: 46.24 seconds\n",
      "Step 41601, Validation loss: 3.484375\n",
      "Step 41801, Training loss: 3.428007125854492, Elapsed Time: 45.58 seconds\n",
      "Step 41801, Validation loss: 3.484375\n",
      "Step 42001, Training loss: 3.437859535217285, Elapsed Time: 46.47 seconds\n",
      "Step 42001, Validation loss: 3.53125\n",
      "Step 42201, Training loss: 3.4171102046966553, Elapsed Time: 46.24 seconds\n",
      "Step 42201, Validation loss: 3.46875\n",
      "Step 42401, Training loss: 3.4344213008880615, Elapsed Time: 46.50 seconds\n",
      "Step 42401, Validation loss: 3.484375\n",
      "Step 42601, Training loss: 3.431197166442871, Elapsed Time: 46.21 seconds\n",
      "Step 42601, Validation loss: 3.40625\n",
      "Step 42801, Training loss: 3.4305789470672607, Elapsed Time: 45.82 seconds\n",
      "Step 42801, Validation loss: 3.484375\n",
      "Step 43001, Training loss: 3.436520576477051, Elapsed Time: 45.66 seconds\n",
      "Step 43001, Validation loss: 3.46875\n",
      "Step 43201, Training loss: 3.4218320846557617, Elapsed Time: 45.58 seconds\n",
      "Step 43201, Validation loss: 3.484375\n",
      "Step 43401, Training loss: 3.4236106872558594, Elapsed Time: 46.30 seconds\n",
      "Step 43401, Validation loss: 3.421875\n",
      "Step 43601, Training loss: 3.417055606842041, Elapsed Time: 46.20 seconds\n",
      "Step 43601, Validation loss: 3.390625\n",
      "Step 43801, Training loss: 3.4291157722473145, Elapsed Time: 46.34 seconds\n",
      "Step 43801, Validation loss: 3.34375\n",
      "Step 44001, Training loss: 3.4215519428253174, Elapsed Time: 46.78 seconds\n",
      "Step 44001, Validation loss: 3.296875\n",
      "Step 44201, Training loss: 3.420191764831543, Elapsed Time: 46.44 seconds\n",
      "Step 44201, Validation loss: 3.421875\n",
      "Step 44401, Training loss: 3.424954414367676, Elapsed Time: 46.91 seconds\n",
      "Step 44401, Validation loss: 3.375\n",
      "Step 44601, Training loss: 3.414418935775757, Elapsed Time: 46.16 seconds\n",
      "Step 44601, Validation loss: 3.453125\n",
      "Step 44801, Training loss: 3.416980266571045, Elapsed Time: 45.84 seconds\n",
      "Step 44801, Validation loss: 3.5\n",
      "Step 45001, Training loss: 3.4027016162872314, Elapsed Time: 48.55 seconds\n",
      "Step 45001, Validation loss: 3.375\n",
      "Step 45201, Training loss: 3.4100639820098877, Elapsed Time: 47.14 seconds\n",
      "Step 45201, Validation loss: 3.40625\n",
      "Step 45401, Training loss: 3.4230356216430664, Elapsed Time: 47.50 seconds\n",
      "Step 45401, Validation loss: 3.484375\n",
      "Step 45601, Training loss: 3.4136717319488525, Elapsed Time: 46.45 seconds\n",
      "Step 45601, Validation loss: 3.390625\n",
      "Step 45801, Training loss: 3.4148364067077637, Elapsed Time: 46.00 seconds\n",
      "Step 45801, Validation loss: 3.28125\n",
      "Step 46001, Training loss: 3.416313648223877, Elapsed Time: 46.39 seconds\n",
      "Step 46001, Validation loss: 3.515625\n",
      "Step 46201, Training loss: 3.415316343307495, Elapsed Time: 46.11 seconds\n",
      "Step 46201, Validation loss: 3.46875\n",
      "Step 46401, Training loss: 3.4124789237976074, Elapsed Time: 47.29 seconds\n",
      "Step 46401, Validation loss: 3.328125\n",
      "Step 46601, Training loss: 3.407616138458252, Elapsed Time: 46.93 seconds\n",
      "Step 46601, Validation loss: 3.453125\n",
      "Step 46801, Training loss: 3.41445255279541, Elapsed Time: 46.84 seconds\n",
      "Step 46801, Validation loss: 3.40625\n",
      "Step 47001, Training loss: 3.4059417247772217, Elapsed Time: 46.61 seconds\n",
      "Step 47001, Validation loss: 3.421875\n",
      "Step 47201, Training loss: 3.4172887802124023, Elapsed Time: 46.71 seconds\n",
      "Step 47201, Validation loss: 3.375\n",
      "Step 47401, Training loss: 3.4052159786224365, Elapsed Time: 48.54 seconds\n",
      "Step 47401, Validation loss: 3.40625\n",
      "Step 47601, Training loss: 3.4119746685028076, Elapsed Time: 47.12 seconds\n",
      "Step 47601, Validation loss: 3.46875\n",
      "Step 47801, Training loss: 3.40401554107666, Elapsed Time: 46.89 seconds\n",
      "Step 47801, Validation loss: 3.4375\n",
      "Step 48001, Training loss: 3.404999256134033, Elapsed Time: 45.84 seconds\n",
      "Step 48001, Validation loss: 3.296875\n",
      "Step 48201, Training loss: 3.4074862003326416, Elapsed Time: 47.50 seconds\n",
      "Step 48201, Validation loss: 3.453125\n",
      "Step 48401, Training loss: 3.41487979888916, Elapsed Time: 46.26 seconds\n",
      "Step 48401, Validation loss: 3.40625\n",
      "Step 48601, Training loss: 3.4078238010406494, Elapsed Time: 46.21 seconds\n",
      "Step 48601, Validation loss: 3.359375\n",
      "Step 48801, Training loss: 3.414790153503418, Elapsed Time: 45.98 seconds\n",
      "Step 48801, Validation loss: 3.390625\n",
      "Step 49001, Training loss: 3.403085947036743, Elapsed Time: 45.90 seconds\n",
      "Step 49001, Validation loss: 3.4375\n",
      "Step 49201, Training loss: 3.400144577026367, Elapsed Time: 46.27 seconds\n",
      "Step 49201, Validation loss: 3.46875\n",
      "Step 49401, Training loss: 3.4069387912750244, Elapsed Time: 46.00 seconds\n",
      "Step 49401, Validation loss: 3.484375\n",
      "Step 49601, Training loss: 3.40511155128479, Elapsed Time: 47.19 seconds\n",
      "Step 49601, Validation loss: 3.46875\n",
      "Step 49801, Training loss: 3.402200222015381, Elapsed Time: 45.98 seconds\n",
      "Step 49801, Validation loss: 3.40625\n",
      "Step 50001, Training loss: 3.4026591777801514, Elapsed Time: 46.76 seconds\n",
      "Step 50001, Validation loss: 3.421875\n",
      "Step 50201, Training loss: 3.4004430770874023, Elapsed Time: 45.91 seconds\n",
      "Step 50201, Validation loss: 3.453125\n",
      "Step 50401, Training loss: 3.4074504375457764, Elapsed Time: 46.52 seconds\n",
      "Step 50401, Validation loss: 3.28125\n",
      "Step 50601, Training loss: 3.397679090499878, Elapsed Time: 45.98 seconds\n",
      "Step 50601, Validation loss: 3.46875\n",
      "Step 50801, Training loss: 3.399163246154785, Elapsed Time: 46.85 seconds\n",
      "Step 50801, Validation loss: 3.390625\n",
      "Step 51001, Training loss: 3.399113178253174, Elapsed Time: 45.29 seconds\n",
      "Step 51001, Validation loss: 3.453125\n",
      "Step 51201, Training loss: 3.3932719230651855, Elapsed Time: 45.64 seconds\n",
      "Step 51201, Validation loss: 3.390625\n",
      "Step 51401, Training loss: 3.403225898742676, Elapsed Time: 45.29 seconds\n",
      "Step 51401, Validation loss: 3.4375\n",
      "Step 51601, Training loss: 3.3990330696105957, Elapsed Time: 48.87 seconds\n",
      "Step 51601, Validation loss: 3.5\n",
      "Step 51801, Training loss: 3.39451003074646, Elapsed Time: 50.80 seconds\n",
      "Step 51801, Validation loss: 3.34375\n",
      "Step 52001, Training loss: 3.4088451862335205, Elapsed Time: 45.24 seconds\n",
      "Step 52001, Validation loss: 3.359375\n",
      "Step 52201, Training loss: 3.3965001106262207, Elapsed Time: 46.18 seconds\n",
      "Step 52201, Validation loss: 3.546875\n",
      "Step 52401, Training loss: 3.3991787433624268, Elapsed Time: 45.74 seconds\n",
      "Step 52401, Validation loss: 3.4375\n",
      "Step 52601, Training loss: 3.3887434005737305, Elapsed Time: 45.76 seconds\n",
      "Step 52601, Validation loss: 3.390625\n",
      "Step 52801, Training loss: 3.398092031478882, Elapsed Time: 45.47 seconds\n",
      "Step 52801, Validation loss: 3.34375\n",
      "Step 53001, Training loss: 3.396021842956543, Elapsed Time: 45.70 seconds\n",
      "Step 53001, Validation loss: 3.4375\n",
      "Step 53201, Training loss: 3.3874504566192627, Elapsed Time: 45.49 seconds\n",
      "Step 53201, Validation loss: 3.296875\n",
      "Step 53401, Training loss: 3.3889901638031006, Elapsed Time: 45.85 seconds\n",
      "Step 53401, Validation loss: 3.4375\n",
      "Step 53601, Training loss: 3.3962013721466064, Elapsed Time: 45.12 seconds\n",
      "Step 53601, Validation loss: 3.359375\n",
      "Step 53801, Training loss: 3.389091968536377, Elapsed Time: 45.29 seconds\n",
      "Step 53801, Validation loss: 3.34375\n",
      "Step 54001, Training loss: 3.385544776916504, Elapsed Time: 44.85 seconds\n",
      "Step 54001, Validation loss: 3.40625\n",
      "Step 54201, Training loss: 3.3910000324249268, Elapsed Time: 45.85 seconds\n",
      "Step 54201, Validation loss: 3.3125\n",
      "Step 54401, Training loss: 3.387953519821167, Elapsed Time: 45.41 seconds\n",
      "Step 54401, Validation loss: 3.421875\n",
      "Step 54601, Training loss: 3.3923115730285645, Elapsed Time: 45.28 seconds\n",
      "Step 54601, Validation loss: 3.34375\n",
      "Step 54801, Training loss: 3.3885915279388428, Elapsed Time: 45.28 seconds\n",
      "Step 54801, Validation loss: 3.328125\n",
      "Step 55001, Training loss: 3.3881351947784424, Elapsed Time: 46.04 seconds\n",
      "Step 55001, Validation loss: 3.359375\n",
      "Step 55201, Training loss: 3.379960060119629, Elapsed Time: 46.31 seconds\n",
      "Step 55201, Validation loss: 3.375\n",
      "Step 55401, Training loss: 3.385021209716797, Elapsed Time: 45.17 seconds\n",
      "Step 55401, Validation loss: 3.421875\n",
      "Step 55601, Training loss: 3.3810436725616455, Elapsed Time: 45.20 seconds\n",
      "Step 55601, Validation loss: 3.4375\n",
      "Step 55801, Training loss: 3.385824203491211, Elapsed Time: 45.47 seconds\n",
      "Step 55801, Validation loss: 3.515625\n",
      "Step 56001, Training loss: 3.384622097015381, Elapsed Time: 45.06 seconds\n",
      "Step 56001, Validation loss: 3.34375\n",
      "Step 56201, Training loss: 3.3785226345062256, Elapsed Time: 45.49 seconds\n",
      "Step 56201, Validation loss: 3.390625\n",
      "Step 56401, Training loss: 3.3814849853515625, Elapsed Time: 45.04 seconds\n",
      "Step 56401, Validation loss: 3.53125\n",
      "Step 56601, Training loss: 3.3814713954925537, Elapsed Time: 45.53 seconds\n",
      "Step 56601, Validation loss: 3.453125\n",
      "Step 56801, Training loss: 3.3889143466949463, Elapsed Time: 45.27 seconds\n",
      "Step 56801, Validation loss: 3.390625\n",
      "Step 57001, Training loss: 3.3883073329925537, Elapsed Time: 45.79 seconds\n",
      "Step 57001, Validation loss: 3.46875\n",
      "Step 57201, Training loss: 3.3793842792510986, Elapsed Time: 45.07 seconds\n",
      "Step 57201, Validation loss: 3.390625\n",
      "Step 57401, Training loss: 3.3862881660461426, Elapsed Time: 45.18 seconds\n",
      "Step 57401, Validation loss: 3.4375\n",
      "Step 57601, Training loss: 3.3864941596984863, Elapsed Time: 46.14 seconds\n",
      "Step 57601, Validation loss: 3.359375\n",
      "Step 57801, Training loss: 3.383725881576538, Elapsed Time: 45.05 seconds\n",
      "Step 57801, Validation loss: 3.296875\n",
      "Step 58001, Training loss: 3.381579875946045, Elapsed Time: 46.00 seconds\n",
      "Step 58001, Validation loss: 3.40625\n",
      "Step 58201, Training loss: 3.385798215866089, Elapsed Time: 45.03 seconds\n",
      "Step 58201, Validation loss: 3.34375\n",
      "Step 58401, Training loss: 3.3772377967834473, Elapsed Time: 45.24 seconds\n",
      "Step 58401, Validation loss: 3.359375\n",
      "Step 58601, Training loss: 3.3729515075683594, Elapsed Time: 45.10 seconds\n",
      "Step 58601, Validation loss: 3.484375\n",
      "Step 58801, Training loss: 3.377830982208252, Elapsed Time: 45.43 seconds\n",
      "Step 58801, Validation loss: 3.375\n",
      "Step 59001, Training loss: 3.3805174827575684, Elapsed Time: 45.33 seconds\n",
      "Step 59001, Validation loss: 3.390625\n",
      "Step 59201, Training loss: 3.3731911182403564, Elapsed Time: 45.66 seconds\n",
      "Step 59201, Validation loss: 3.40625\n",
      "Step 59401, Training loss: 3.3785645961761475, Elapsed Time: 45.24 seconds\n",
      "Step 59401, Validation loss: 3.421875\n",
      "Step 59601, Training loss: 3.3774406909942627, Elapsed Time: 45.55 seconds\n",
      "Step 59601, Validation loss: 3.46875\n",
      "Step 59801, Training loss: 3.3706483840942383, Elapsed Time: 45.15 seconds\n",
      "Step 59801, Validation loss: 3.421875\n",
      "Step 60001, Training loss: 3.3794984817504883, Elapsed Time: 45.22 seconds\n",
      "Step 60001, Validation loss: 3.375\n",
      "Step 60201, Training loss: 3.378754138946533, Elapsed Time: 45.86 seconds\n",
      "Step 60201, Validation loss: 3.40625\n",
      "Step 60401, Training loss: 3.372244119644165, Elapsed Time: 45.40 seconds\n",
      "Step 60401, Validation loss: 3.375\n",
      "Step 60601, Training loss: 3.3699092864990234, Elapsed Time: 45.26 seconds\n",
      "Step 60601, Validation loss: 3.328125\n",
      "Step 60801, Training loss: 3.3635520935058594, Elapsed Time: 45.40 seconds\n",
      "Step 60801, Validation loss: 3.328125\n",
      "Step 61001, Training loss: 3.3765578269958496, Elapsed Time: 45.10 seconds\n",
      "Step 61001, Validation loss: 3.328125\n",
      "Step 61201, Training loss: 3.370509147644043, Elapsed Time: 45.66 seconds\n",
      "Step 61201, Validation loss: 3.34375\n",
      "Step 61401, Training loss: 3.367725133895874, Elapsed Time: 44.96 seconds\n",
      "Step 61401, Validation loss: 3.359375\n",
      "Step 61601, Training loss: 3.364895820617676, Elapsed Time: 45.83 seconds\n",
      "Step 61601, Validation loss: 3.375\n",
      "Step 61801, Training loss: 3.371265411376953, Elapsed Time: 45.38 seconds\n",
      "Step 61801, Validation loss: 3.328125\n",
      "Step 62001, Training loss: 3.364222526550293, Elapsed Time: 45.97 seconds\n",
      "Step 62001, Validation loss: 3.421875\n",
      "Step 62201, Training loss: 3.365180253982544, Elapsed Time: 44.92 seconds\n",
      "Step 62201, Validation loss: 3.421875\n",
      "Step 62401, Training loss: 3.3623499870300293, Elapsed Time: 45.23 seconds\n",
      "Step 62401, Validation loss: 3.21875\n",
      "Step 62601, Training loss: 3.362422466278076, Elapsed Time: 44.98 seconds\n",
      "Step 62601, Validation loss: 3.359375\n",
      "Step 62801, Training loss: 3.365339994430542, Elapsed Time: 46.11 seconds\n",
      "Step 62801, Validation loss: 3.40625\n",
      "Step 63001, Training loss: 3.3695552349090576, Elapsed Time: 46.11 seconds\n",
      "Step 63001, Validation loss: 3.421875\n",
      "Step 63201, Training loss: 3.3688714504241943, Elapsed Time: 45.34 seconds\n",
      "Step 63201, Validation loss: 3.328125\n",
      "Step 63401, Training loss: 3.373946189880371, Elapsed Time: 45.09 seconds\n",
      "Step 63401, Validation loss: 3.34375\n",
      "Step 63601, Training loss: 3.3720266819000244, Elapsed Time: 45.30 seconds\n",
      "Step 63601, Validation loss: 3.515625\n",
      "Step 63801, Training loss: 3.364086151123047, Elapsed Time: 45.18 seconds\n",
      "Step 63801, Validation loss: 3.375\n",
      "Step 64001, Training loss: 3.361720085144043, Elapsed Time: 46.07 seconds\n",
      "Step 64001, Validation loss: 3.375\n",
      "Step 64201, Training loss: 3.369306802749634, Elapsed Time: 45.16 seconds\n",
      "Step 64201, Validation loss: 3.46875\n",
      "Step 64401, Training loss: 3.35832142829895, Elapsed Time: 45.28 seconds\n",
      "Step 64401, Validation loss: 3.359375\n",
      "Step 64601, Training loss: 3.356437921524048, Elapsed Time: 46.03 seconds\n",
      "Step 64601, Validation loss: 3.28125\n",
      "Step 64801, Training loss: 3.3646233081817627, Elapsed Time: 44.85 seconds\n",
      "Step 64801, Validation loss: 3.40625\n",
      "Step 65001, Training loss: 3.358614444732666, Elapsed Time: 44.94 seconds\n",
      "Step 65001, Validation loss: 3.421875\n",
      "Step 65201, Training loss: 3.365365743637085, Elapsed Time: 45.57 seconds\n",
      "Step 65201, Validation loss: 3.40625\n",
      "Step 65401, Training loss: 3.3644979000091553, Elapsed Time: 45.22 seconds\n",
      "Step 65401, Validation loss: 3.4375\n",
      "Step 65601, Training loss: 3.350518226623535, Elapsed Time: 45.60 seconds\n",
      "Step 65601, Validation loss: 3.34375\n",
      "Step 65801, Training loss: 3.348987340927124, Elapsed Time: 45.01 seconds\n",
      "Step 65801, Validation loss: 3.390625\n",
      "Step 66001, Training loss: 3.356095790863037, Elapsed Time: 45.43 seconds\n",
      "Step 66001, Validation loss: 3.34375\n",
      "Step 66201, Training loss: 3.349888563156128, Elapsed Time: 45.02 seconds\n",
      "Step 66201, Validation loss: 3.265625\n",
      "Step 66401, Training loss: 3.3554177284240723, Elapsed Time: 45.54 seconds\n",
      "Step 66401, Validation loss: 3.34375\n",
      "Step 66601, Training loss: 3.355391502380371, Elapsed Time: 44.86 seconds\n",
      "Step 66601, Validation loss: 3.34375\n",
      "Step 66801, Training loss: 3.353867769241333, Elapsed Time: 45.57 seconds\n",
      "Step 66801, Validation loss: 3.4375\n",
      "Step 67001, Training loss: 3.3587756156921387, Elapsed Time: 44.99 seconds\n",
      "Step 67001, Validation loss: 3.390625\n",
      "Step 67201, Training loss: 3.363483428955078, Elapsed Time: 45.53 seconds\n",
      "Step 67201, Validation loss: 3.5\n",
      "Step 67401, Training loss: 3.356499671936035, Elapsed Time: 45.46 seconds\n",
      "Step 67401, Validation loss: 3.328125\n",
      "Step 67601, Training loss: 3.3561089038848877, Elapsed Time: 45.45 seconds\n",
      "Step 67601, Validation loss: 3.359375\n",
      "Step 67801, Training loss: 3.3482472896575928, Elapsed Time: 44.93 seconds\n",
      "Step 67801, Validation loss: 3.328125\n",
      "Step 68001, Training loss: 3.3574960231781006, Elapsed Time: 45.44 seconds\n",
      "Step 68001, Validation loss: 3.484375\n",
      "Step 68201, Training loss: 3.3524067401885986, Elapsed Time: 45.15 seconds\n",
      "Step 68201, Validation loss: 3.3125\n",
      "Step 68401, Training loss: 3.3557729721069336, Elapsed Time: 45.61 seconds\n",
      "Step 68401, Validation loss: 3.328125\n",
      "Step 68601, Training loss: 3.350400686264038, Elapsed Time: 44.92 seconds\n",
      "Step 68601, Validation loss: 3.390625\n",
      "Step 68801, Training loss: 3.351984739303589, Elapsed Time: 45.61 seconds\n",
      "Step 68801, Validation loss: 3.296875\n",
      "Step 69001, Training loss: 3.3466567993164062, Elapsed Time: 45.52 seconds\n",
      "Step 69001, Validation loss: 3.328125\n",
      "Step 69201, Training loss: 3.350148916244507, Elapsed Time: 45.24 seconds\n",
      "Step 69201, Validation loss: 3.390625\n",
      "Step 69401, Training loss: 3.344510316848755, Elapsed Time: 45.17 seconds\n",
      "Step 69401, Validation loss: 3.328125\n",
      "Step 69601, Training loss: 3.354560136795044, Elapsed Time: 45.57 seconds\n",
      "Step 69601, Validation loss: 3.46875\n",
      "Step 69801, Training loss: 3.34678053855896, Elapsed Time: 45.22 seconds\n",
      "Step 69801, Validation loss: 3.28125\n",
      "Step 70001, Training loss: 3.348606586456299, Elapsed Time: 45.51 seconds\n",
      "Step 70001, Validation loss: 3.484375\n",
      "Step 70201, Training loss: 3.346285820007324, Elapsed Time: 44.90 seconds\n",
      "Step 70201, Validation loss: 3.3125\n",
      "Step 70401, Training loss: 3.3459384441375732, Elapsed Time: 45.69 seconds\n",
      "Step 70401, Validation loss: 3.25\n",
      "Step 70601, Training loss: 3.3499107360839844, Elapsed Time: 45.66 seconds\n",
      "Step 70601, Validation loss: 3.46875\n",
      "Step 70801, Training loss: 3.345669984817505, Elapsed Time: 45.42 seconds\n",
      "Step 70801, Validation loss: 3.296875\n",
      "Step 71001, Training loss: 3.3504433631896973, Elapsed Time: 44.98 seconds\n",
      "Step 71001, Validation loss: 3.390625\n",
      "Step 71201, Training loss: 3.345003366470337, Elapsed Time: 45.42 seconds\n",
      "Step 71201, Validation loss: 3.40625\n",
      "Step 71401, Training loss: 3.3486950397491455, Elapsed Time: 44.81 seconds\n",
      "Step 71401, Validation loss: 3.390625\n",
      "Step 71601, Training loss: 3.3411781787872314, Elapsed Time: 45.36 seconds\n",
      "Step 71601, Validation loss: 3.234375\n",
      "Step 71801, Training loss: 3.3431191444396973, Elapsed Time: 44.90 seconds\n",
      "Step 71801, Validation loss: 3.3125\n",
      "Step 72001, Training loss: 3.3397722244262695, Elapsed Time: 45.34 seconds\n",
      "Step 72001, Validation loss: 3.34375\n",
      "Step 72201, Training loss: 3.3443267345428467, Elapsed Time: 45.90 seconds\n",
      "Step 72201, Validation loss: 3.375\n",
      "Step 72401, Training loss: 3.3423352241516113, Elapsed Time: 45.59 seconds\n",
      "Step 72401, Validation loss: 3.34375\n",
      "Step 72601, Training loss: 3.3452062606811523, Elapsed Time: 45.31 seconds\n",
      "Step 72601, Validation loss: 3.359375\n",
      "Step 72801, Training loss: 3.3426084518432617, Elapsed Time: 46.14 seconds\n",
      "Step 72801, Validation loss: 3.421875\n",
      "Step 73001, Training loss: 3.3478643894195557, Elapsed Time: 45.73 seconds\n",
      "Step 73001, Validation loss: 3.328125\n",
      "Step 73201, Training loss: 3.3428637981414795, Elapsed Time: 45.15 seconds\n",
      "Step 73201, Validation loss: 3.3125\n",
      "Step 73401, Training loss: 3.345841646194458, Elapsed Time: 45.06 seconds\n",
      "Step 73401, Validation loss: 3.40625\n",
      "Step 73601, Training loss: 3.3415603637695312, Elapsed Time: 45.31 seconds\n",
      "Step 73601, Validation loss: 3.34375\n",
      "Step 73801, Training loss: 3.3397488594055176, Elapsed Time: 45.79 seconds\n",
      "Step 73801, Validation loss: 3.34375\n",
      "Step 74001, Training loss: 3.33695125579834, Elapsed Time: 45.90 seconds\n",
      "Step 74001, Validation loss: 3.328125\n",
      "Step 74201, Training loss: 3.337320566177368, Elapsed Time: 45.43 seconds\n",
      "Step 74201, Validation loss: 3.328125\n",
      "Step 74401, Training loss: 3.340045928955078, Elapsed Time: 45.30 seconds\n",
      "Step 74401, Validation loss: 3.296875\n",
      "Step 74601, Training loss: 3.3377668857574463, Elapsed Time: 45.07 seconds\n",
      "Step 74601, Validation loss: 3.40625\n",
      "Step 74801, Training loss: 3.341081380844116, Elapsed Time: 45.29 seconds\n",
      "Step 74801, Validation loss: 3.359375\n",
      "Step 75001, Training loss: 3.3411903381347656, Elapsed Time: 45.08 seconds\n",
      "Step 75001, Validation loss: 3.375\n",
      "Step 75201, Training loss: 3.3352553844451904, Elapsed Time: 45.06 seconds\n",
      "Step 75201, Validation loss: 3.234375\n",
      "Step 75401, Training loss: 3.3309381008148193, Elapsed Time: 45.32 seconds\n",
      "Step 75401, Validation loss: 3.359375\n",
      "Step 75601, Training loss: 3.3353545665740967, Elapsed Time: 45.11 seconds\n",
      "Step 75601, Validation loss: 3.328125\n",
      "Step 75801, Training loss: 3.3337996006011963, Elapsed Time: 45.10 seconds\n",
      "Step 75801, Validation loss: 3.375\n",
      "Step 76001, Training loss: 3.345430850982666, Elapsed Time: 45.28 seconds\n",
      "Step 76001, Validation loss: 3.328125\n",
      "Step 76201, Training loss: 3.332350015640259, Elapsed Time: 45.37 seconds\n",
      "Step 76201, Validation loss: 3.28125\n",
      "Step 76401, Training loss: 3.34362530708313, Elapsed Time: 45.93 seconds\n",
      "Step 76401, Validation loss: 3.3125\n",
      "Step 76601, Training loss: 3.3316760063171387, Elapsed Time: 45.47 seconds\n",
      "Step 76601, Validation loss: 3.3125\n",
      "Step 76801, Training loss: 3.329768180847168, Elapsed Time: 45.74 seconds\n",
      "Step 76801, Validation loss: 3.375\n",
      "Step 77001, Training loss: 3.3290398120880127, Elapsed Time: 45.44 seconds\n",
      "Step 77001, Validation loss: 3.3125\n",
      "Step 77201, Training loss: 3.332084894180298, Elapsed Time: 46.09 seconds\n",
      "Step 77201, Validation loss: 3.296875\n",
      "Step 77401, Training loss: 3.333089590072632, Elapsed Time: 46.07 seconds\n",
      "Step 77401, Validation loss: 3.28125\n",
      "Step 77601, Training loss: 3.335353374481201, Elapsed Time: 45.21 seconds\n",
      "Step 77601, Validation loss: 3.4375\n",
      "Step 77801, Training loss: 3.3346469402313232, Elapsed Time: 45.52 seconds\n",
      "Step 77801, Validation loss: 3.328125\n",
      "Step 78001, Training loss: 3.3373069763183594, Elapsed Time: 44.96 seconds\n",
      "Step 78001, Validation loss: 3.296875\n",
      "Step 78201, Training loss: 3.33078932762146, Elapsed Time: 45.33 seconds\n",
      "Step 78201, Validation loss: 3.21875\n",
      "Step 78401, Training loss: 3.3338170051574707, Elapsed Time: 45.30 seconds\n",
      "Step 78401, Validation loss: 3.34375\n",
      "Step 78601, Training loss: 3.3385767936706543, Elapsed Time: 45.46 seconds\n",
      "Step 78601, Validation loss: 3.375\n",
      "Step 78801, Training loss: 3.3309240341186523, Elapsed Time: 45.23 seconds\n",
      "Step 78801, Validation loss: 3.359375\n",
      "Step 79001, Training loss: 3.324140787124634, Elapsed Time: 45.34 seconds\n",
      "Step 79001, Validation loss: 3.375\n",
      "Step 79201, Training loss: 3.339627504348755, Elapsed Time: 45.13 seconds\n",
      "Step 79201, Validation loss: 3.359375\n",
      "Step 79401, Training loss: 3.3320422172546387, Elapsed Time: 46.30 seconds\n",
      "Step 79401, Validation loss: 3.421875\n",
      "Step 79601, Training loss: 3.323169708251953, Elapsed Time: 44.96 seconds\n",
      "Step 79601, Validation loss: 3.28125\n",
      "Step 79801, Training loss: 3.3226940631866455, Elapsed Time: 45.65 seconds\n",
      "Step 79801, Validation loss: 3.375\n",
      "Step 80001, Training loss: 3.319608449935913, Elapsed Time: 45.42 seconds\n",
      "Step 80001, Validation loss: 3.4375\n",
      "Step 80201, Training loss: 3.3248021602630615, Elapsed Time: 45.89 seconds\n",
      "Step 80201, Validation loss: 3.40625\n",
      "Step 80401, Training loss: 3.3268444538116455, Elapsed Time: 45.12 seconds\n",
      "Step 80401, Validation loss: 3.3125\n",
      "Step 80601, Training loss: 3.323429584503174, Elapsed Time: 45.33 seconds\n",
      "Step 80601, Validation loss: 3.28125\n",
      "Step 80801, Training loss: 3.321350574493408, Elapsed Time: 45.15 seconds\n",
      "Step 80801, Validation loss: 3.359375\n",
      "Step 81001, Training loss: 3.3277499675750732, Elapsed Time: 45.72 seconds\n",
      "Step 81001, Validation loss: 3.421875\n",
      "Step 81201, Training loss: 3.321587085723877, Elapsed Time: 44.99 seconds\n",
      "Step 81201, Validation loss: 3.390625\n",
      "Step 81401, Training loss: 3.321367025375366, Elapsed Time: 45.50 seconds\n",
      "Step 81401, Validation loss: 3.265625\n",
      "Step 81601, Training loss: 3.330390214920044, Elapsed Time: 45.02 seconds\n",
      "Step 81601, Validation loss: 3.265625\n",
      "Step 81801, Training loss: 3.32857346534729, Elapsed Time: 45.12 seconds\n",
      "Step 81801, Validation loss: 3.328125\n",
      "Step 82001, Training loss: 3.331491231918335, Elapsed Time: 45.14 seconds\n",
      "Step 82001, Validation loss: 3.375\n",
      "Step 82201, Training loss: 3.32271671295166, Elapsed Time: 45.12 seconds\n",
      "Step 82201, Validation loss: 3.421875\n",
      "Step 82401, Training loss: 3.328474998474121, Elapsed Time: 45.17 seconds\n",
      "Step 82401, Validation loss: 3.34375\n",
      "Step 82601, Training loss: 3.31485652923584, Elapsed Time: 45.36 seconds\n",
      "Step 82601, Validation loss: 3.375\n",
      "Step 82801, Training loss: 3.3250091075897217, Elapsed Time: 45.31 seconds\n",
      "Step 82801, Validation loss: 3.296875\n",
      "Step 83001, Training loss: 3.319200277328491, Elapsed Time: 45.68 seconds\n",
      "Step 83001, Validation loss: 3.328125\n",
      "Step 83201, Training loss: 3.324849843978882, Elapsed Time: 45.99 seconds\n",
      "Step 83201, Validation loss: 3.359375\n",
      "Step 83401, Training loss: 3.316535472869873, Elapsed Time: 45.16 seconds\n",
      "Step 83401, Validation loss: 3.28125\n",
      "Step 83601, Training loss: 3.31921648979187, Elapsed Time: 45.39 seconds\n",
      "Step 83601, Validation loss: 3.3125\n",
      "Step 83801, Training loss: 3.3213255405426025, Elapsed Time: 45.71 seconds\n",
      "Step 83801, Validation loss: 3.359375\n",
      "Step 84001, Training loss: 3.317342758178711, Elapsed Time: 44.96 seconds\n",
      "Step 84001, Validation loss: 3.296875\n",
      "Step 84201, Training loss: 3.3158175945281982, Elapsed Time: 45.31 seconds\n",
      "Step 84201, Validation loss: 3.359375\n",
      "Step 84401, Training loss: 3.304755687713623, Elapsed Time: 44.88 seconds\n",
      "Step 84401, Validation loss: 3.40625\n",
      "Step 84601, Training loss: 3.3085482120513916, Elapsed Time: 45.86 seconds\n",
      "Step 84601, Validation loss: 3.4375\n",
      "Step 84801, Training loss: 3.3071846961975098, Elapsed Time: 44.95 seconds\n",
      "Step 84801, Validation loss: 3.234375\n",
      "Step 85001, Training loss: 3.313199758529663, Elapsed Time: 44.91 seconds\n",
      "Step 85001, Validation loss: 3.21875\n",
      "Step 85201, Training loss: 3.3125932216644287, Elapsed Time: 45.59 seconds\n",
      "Step 85201, Validation loss: 3.265625\n",
      "Step 85401, Training loss: 3.3099963665008545, Elapsed Time: 44.84 seconds\n",
      "Step 85401, Validation loss: 3.34375\n",
      "Step 85601, Training loss: 3.311474323272705, Elapsed Time: 46.12 seconds\n",
      "Step 85601, Validation loss: 3.265625\n",
      "Step 85801, Training loss: 3.321485757827759, Elapsed Time: 45.04 seconds\n",
      "Step 85801, Validation loss: 3.34375\n",
      "Step 86001, Training loss: 3.311816930770874, Elapsed Time: 45.45 seconds\n",
      "Step 86001, Validation loss: 3.25\n",
      "Step 86201, Training loss: 3.314737558364868, Elapsed Time: 45.04 seconds\n",
      "Step 86201, Validation loss: 3.296875\n",
      "Step 86401, Training loss: 3.3113887310028076, Elapsed Time: 45.30 seconds\n",
      "Step 86401, Validation loss: 3.3125\n",
      "Step 86601, Training loss: 3.301670551300049, Elapsed Time: 44.63 seconds\n",
      "Step 86601, Validation loss: 3.328125\n",
      "Step 86801, Training loss: 3.311768054962158, Elapsed Time: 45.47 seconds\n",
      "Step 86801, Validation loss: 3.296875\n",
      "Step 87001, Training loss: 3.3130857944488525, Elapsed Time: 45.04 seconds\n",
      "Step 87001, Validation loss: 3.328125\n",
      "Step 87201, Training loss: 3.3048794269561768, Elapsed Time: 45.41 seconds\n",
      "Step 87201, Validation loss: 3.265625\n",
      "Step 87401, Training loss: 3.308896780014038, Elapsed Time: 44.88 seconds\n",
      "Step 87401, Validation loss: 3.21875\n",
      "Step 87601, Training loss: 3.316786050796509, Elapsed Time: 45.93 seconds\n",
      "Step 87601, Validation loss: 3.390625\n",
      "Step 87801, Training loss: 3.294374942779541, Elapsed Time: 44.66 seconds\n",
      "Step 87801, Validation loss: 3.328125\n",
      "Step 88001, Training loss: 3.313093900680542, Elapsed Time: 45.66 seconds\n",
      "Step 88001, Validation loss: 3.34375\n",
      "Step 88201, Training loss: 3.309591054916382, Elapsed Time: 44.62 seconds\n",
      "Step 88201, Validation loss: 3.21875\n",
      "Step 88401, Training loss: 3.3099467754364014, Elapsed Time: 45.30 seconds\n",
      "Step 88401, Validation loss: 3.234375\n",
      "Step 88601, Training loss: 3.2995474338531494, Elapsed Time: 45.32 seconds\n",
      "Step 88601, Validation loss: 3.25\n",
      "Step 88801, Training loss: 3.310042142868042, Elapsed Time: 45.60 seconds\n",
      "Step 88801, Validation loss: 3.375\n",
      "Step 89001, Training loss: 3.305938720703125, Elapsed Time: 44.84 seconds\n",
      "Step 89001, Validation loss: 3.25\n",
      "Step 89201, Training loss: 3.297964334487915, Elapsed Time: 45.63 seconds\n",
      "Step 89201, Validation loss: 3.375\n",
      "Step 89401, Training loss: 3.299170732498169, Elapsed Time: 44.68 seconds\n",
      "Step 89401, Validation loss: 3.3125\n",
      "Step 89601, Training loss: 3.3012309074401855, Elapsed Time: 45.00 seconds\n",
      "Step 89601, Validation loss: 3.421875\n",
      "Step 89801, Training loss: 3.3046634197235107, Elapsed Time: 45.41 seconds\n",
      "Step 89801, Validation loss: 3.28125\n",
      "Step 90001, Training loss: 3.306277275085449, Elapsed Time: 44.63 seconds\n",
      "Step 90001, Validation loss: 3.390625\n",
      "Step 90201, Training loss: 3.304509401321411, Elapsed Time: 45.62 seconds\n",
      "Step 90201, Validation loss: 3.3125\n",
      "Step 90401, Training loss: 3.3112688064575195, Elapsed Time: 45.19 seconds\n",
      "Step 90401, Validation loss: 3.421875\n",
      "Step 90601, Training loss: 3.29793381690979, Elapsed Time: 45.40 seconds\n",
      "Step 90601, Validation loss: 3.453125\n",
      "Step 90801, Training loss: 3.297837734222412, Elapsed Time: 44.68 seconds\n",
      "Step 90801, Validation loss: 3.375\n",
      "Step 91001, Training loss: 3.306196451187134, Elapsed Time: 46.06 seconds\n",
      "Step 91001, Validation loss: 3.390625\n",
      "Step 91201, Training loss: 3.300431966781616, Elapsed Time: 45.16 seconds\n",
      "Step 91201, Validation loss: 3.28125\n",
      "Step 91401, Training loss: 3.2998247146606445, Elapsed Time: 45.40 seconds\n",
      "Step 91401, Validation loss: 3.203125\n",
      "Step 91601, Training loss: 3.3031041622161865, Elapsed Time: 45.26 seconds\n",
      "Step 91601, Validation loss: 3.28125\n",
      "Step 91801, Training loss: 3.2987124919891357, Elapsed Time: 46.03 seconds\n",
      "Step 91801, Validation loss: 3.25\n",
      "Step 92001, Training loss: 3.3032078742980957, Elapsed Time: 45.38 seconds\n",
      "Step 92001, Validation loss: 3.25\n",
      "Step 92201, Training loss: 3.295607566833496, Elapsed Time: 46.34 seconds\n",
      "Step 92201, Validation loss: 3.375\n",
      "Step 92401, Training loss: 3.2887485027313232, Elapsed Time: 45.64 seconds\n",
      "Step 92401, Validation loss: 3.296875\n",
      "Step 92601, Training loss: 3.295996904373169, Elapsed Time: 45.38 seconds\n",
      "Step 92601, Validation loss: 3.3125\n",
      "Step 92801, Training loss: 3.295344829559326, Elapsed Time: 44.86 seconds\n",
      "Step 92801, Validation loss: 3.359375\n",
      "Step 93001, Training loss: 3.3049733638763428, Elapsed Time: 45.50 seconds\n",
      "Step 93001, Validation loss: 3.265625\n",
      "Step 93201, Training loss: 3.2929506301879883, Elapsed Time: 44.67 seconds\n",
      "Step 93201, Validation loss: 3.4375\n",
      "Step 93401, Training loss: 3.2986040115356445, Elapsed Time: 45.48 seconds\n",
      "Step 93401, Validation loss: 3.390625\n",
      "Step 93601, Training loss: 3.298271656036377, Elapsed Time: 45.31 seconds\n",
      "Step 93601, Validation loss: 3.140625\n",
      "Step 93801, Training loss: 3.291478395462036, Elapsed Time: 45.37 seconds\n",
      "Step 93801, Validation loss: 3.296875\n",
      "Step 94001, Training loss: 3.2947568893432617, Elapsed Time: 44.98 seconds\n",
      "Step 94001, Validation loss: 3.375\n",
      "Step 94201, Training loss: 3.290581703186035, Elapsed Time: 44.73 seconds\n",
      "Step 94201, Validation loss: 3.34375\n",
      "Step 94401, Training loss: 3.300128936767578, Elapsed Time: 45.87 seconds\n",
      "Step 94401, Validation loss: 3.265625\n",
      "Step 94601, Training loss: 3.295116424560547, Elapsed Time: 45.70 seconds\n",
      "Step 94601, Validation loss: 3.296875\n",
      "Step 94801, Training loss: 3.289095401763916, Elapsed Time: 46.33 seconds\n",
      "Step 94801, Validation loss: 3.21875\n",
      "Step 95001, Training loss: 3.2919692993164062, Elapsed Time: 44.81 seconds\n",
      "Step 95001, Validation loss: 3.1875\n",
      "Step 95201, Training loss: 3.290287733078003, Elapsed Time: 45.61 seconds\n",
      "Step 95201, Validation loss: 3.359375\n",
      "Step 95401, Training loss: 3.2950499057769775, Elapsed Time: 44.75 seconds\n",
      "Step 95401, Validation loss: 3.359375\n",
      "Step 95601, Training loss: 3.2863969802856445, Elapsed Time: 45.49 seconds\n",
      "Step 95601, Validation loss: 3.375\n",
      "Step 95801, Training loss: 3.2976291179656982, Elapsed Time: 44.94 seconds\n",
      "Step 95801, Validation loss: 3.375\n",
      "Step 96001, Training loss: 3.28627347946167, Elapsed Time: 45.52 seconds\n",
      "Step 96001, Validation loss: 3.21875\n",
      "Step 96201, Training loss: 3.285978317260742, Elapsed Time: 44.91 seconds\n",
      "Step 96201, Validation loss: 3.296875\n",
      "Step 96401, Training loss: 3.282932996749878, Elapsed Time: 46.14 seconds\n",
      "Step 96401, Validation loss: 3.296875\n",
      "Step 96601, Training loss: 3.2734553813934326, Elapsed Time: 45.09 seconds\n",
      "Step 96601, Validation loss: 3.34375\n",
      "Step 96801, Training loss: 3.2801263332366943, Elapsed Time: 47.69 seconds\n",
      "Step 96801, Validation loss: 3.296875\n",
      "Step 97001, Training loss: 3.2861592769622803, Elapsed Time: 45.55 seconds\n",
      "Step 97001, Validation loss: 3.359375\n",
      "Step 97201, Training loss: 3.2849221229553223, Elapsed Time: 45.72 seconds\n",
      "Step 97201, Validation loss: 3.375\n",
      "Step 97401, Training loss: 3.2962887287139893, Elapsed Time: 44.80 seconds\n",
      "Step 97401, Validation loss: 3.390625\n",
      "Step 97601, Training loss: 3.29699444770813, Elapsed Time: 46.56 seconds\n",
      "Step 97601, Validation loss: 3.3125\n",
      "Step 97801, Training loss: 3.2922732830047607, Elapsed Time: 45.72 seconds\n",
      "Step 97801, Validation loss: 3.25\n",
      "Step 98001, Training loss: 3.2836215496063232, Elapsed Time: 45.54 seconds\n",
      "Step 98001, Validation loss: 3.296875\n",
      "Step 98201, Training loss: 3.2803423404693604, Elapsed Time: 45.10 seconds\n",
      "Step 98201, Validation loss: 3.171875\n",
      "Step 98401, Training loss: 3.2779030799865723, Elapsed Time: 46.18 seconds\n",
      "Step 98401, Validation loss: 3.1875\n",
      "Step 98601, Training loss: 3.276742458343506, Elapsed Time: 44.75 seconds\n",
      "Step 98601, Validation loss: 3.21875\n",
      "Step 98801, Training loss: 3.275808334350586, Elapsed Time: 46.05 seconds\n",
      "Step 98801, Validation loss: 3.25\n",
      "Step 99001, Training loss: 3.2853026390075684, Elapsed Time: 44.84 seconds\n",
      "Step 99001, Validation loss: 3.421875\n",
      "Step 99201, Training loss: 3.2775423526763916, Elapsed Time: 45.83 seconds\n",
      "Step 99201, Validation loss: 3.421875\n",
      "Step 99401, Training loss: 3.2874701023101807, Elapsed Time: 44.78 seconds\n",
      "Step 99401, Validation loss: 3.21875\n",
      "Step 99601, Training loss: 3.2722082138061523, Elapsed Time: 46.52 seconds\n",
      "Step 99601, Validation loss: 3.375\n",
      "Step 99801, Training loss: 3.2857611179351807, Elapsed Time: 44.74 seconds\n",
      "Step 99801, Validation loss: 3.203125\n",
      "Step 100001, Training loss: 3.285738229751587, Elapsed Time: 45.35 seconds\n",
      "Step 100001, Validation loss: 3.34375\n",
      "Step 100201, Training loss: 3.2830920219421387, Elapsed Time: 45.57 seconds\n",
      "Step 100201, Validation loss: 3.1875\n",
      "Step 100401, Training loss: 3.2779715061187744, Elapsed Time: 45.99 seconds\n",
      "Step 100401, Validation loss: 3.28125\n",
      "Step 100601, Training loss: 3.274536609649658, Elapsed Time: 45.88 seconds\n",
      "Step 100601, Validation loss: 3.34375\n",
      "Step 100801, Training loss: 3.2785770893096924, Elapsed Time: 45.23 seconds\n",
      "Step 100801, Validation loss: 3.25\n",
      "Step 101001, Training loss: 3.2778890132904053, Elapsed Time: 45.48 seconds\n",
      "Step 101001, Validation loss: 3.28125\n",
      "Step 101201, Training loss: 3.280266761779785, Elapsed Time: 45.50 seconds\n",
      "Step 101201, Validation loss: 3.3125\n",
      "Step 101401, Training loss: 3.283963203430176, Elapsed Time: 45.46 seconds\n",
      "Step 101401, Validation loss: 3.203125\n",
      "Step 101601, Training loss: 3.2843379974365234, Elapsed Time: 44.68 seconds\n",
      "Step 101601, Validation loss: 3.34375\n",
      "Step 101801, Training loss: 3.266909599304199, Elapsed Time: 45.71 seconds\n",
      "Step 101801, Validation loss: 3.359375\n",
      "Step 102001, Training loss: 3.274540901184082, Elapsed Time: 45.28 seconds\n",
      "Step 102001, Validation loss: 3.25\n",
      "Step 102201, Training loss: 3.2717690467834473, Elapsed Time: 45.74 seconds\n",
      "Step 102201, Validation loss: 3.28125\n",
      "Step 102401, Training loss: 3.269693613052368, Elapsed Time: 44.84 seconds\n",
      "Step 102401, Validation loss: 3.3125\n",
      "Step 102601, Training loss: 3.2789175510406494, Elapsed Time: 45.76 seconds\n",
      "Step 102601, Validation loss: 3.3125\n",
      "Step 102801, Training loss: 3.2814738750457764, Elapsed Time: 44.61 seconds\n",
      "Step 102801, Validation loss: 3.25\n",
      "Step 103001, Training loss: 3.2692248821258545, Elapsed Time: 45.73 seconds\n",
      "Step 103001, Validation loss: 3.34375\n",
      "Step 103201, Training loss: 3.273301601409912, Elapsed Time: 44.98 seconds\n",
      "Step 103201, Validation loss: 3.265625\n",
      "Step 103401, Training loss: 3.269787311553955, Elapsed Time: 45.60 seconds\n",
      "Step 103401, Validation loss: 3.375\n",
      "Step 103601, Training loss: 3.26979398727417, Elapsed Time: 44.84 seconds\n",
      "Step 103601, Validation loss: 3.203125\n",
      "Step 103801, Training loss: 3.281249523162842, Elapsed Time: 45.92 seconds\n",
      "Step 103801, Validation loss: 3.34375\n",
      "Step 104001, Training loss: 3.2657198905944824, Elapsed Time: 44.72 seconds\n",
      "Step 104001, Validation loss: 3.21875\n",
      "Step 104201, Training loss: 3.276186466217041, Elapsed Time: 45.68 seconds\n",
      "Step 104201, Validation loss: 3.21875\n",
      "Step 104401, Training loss: 3.269850015640259, Elapsed Time: 45.26 seconds\n",
      "Step 104401, Validation loss: 3.359375\n",
      "Step 104601, Training loss: 3.268310546875, Elapsed Time: 46.56 seconds\n",
      "Step 104601, Validation loss: 3.21875\n",
      "Step 104801, Training loss: 3.266907215118408, Elapsed Time: 46.19 seconds\n",
      "Step 104801, Validation loss: 3.25\n",
      "Step 105001, Training loss: 3.277768850326538, Elapsed Time: 45.74 seconds\n",
      "Step 105001, Validation loss: 3.25\n",
      "Step 105201, Training loss: 3.2690277099609375, Elapsed Time: 44.67 seconds\n",
      "Step 105201, Validation loss: 3.40625\n",
      "Step 105401, Training loss: 3.2687137126922607, Elapsed Time: 44.89 seconds\n",
      "Step 105401, Validation loss: 3.265625\n",
      "Step 105601, Training loss: 3.261937141418457, Elapsed Time: 46.15 seconds\n",
      "Step 105601, Validation loss: 3.1875\n",
      "Step 105801, Training loss: 3.2705941200256348, Elapsed Time: 45.34 seconds\n",
      "Step 105801, Validation loss: 3.359375\n",
      "Step 106001, Training loss: 3.2697176933288574, Elapsed Time: 46.23 seconds\n",
      "Step 106001, Validation loss: 3.296875\n",
      "Step 106201, Training loss: 3.2636163234710693, Elapsed Time: 44.98 seconds\n",
      "Step 106201, Validation loss: 3.421875\n",
      "Step 106401, Training loss: 3.2633748054504395, Elapsed Time: 45.44 seconds\n",
      "Step 106401, Validation loss: 3.359375\n",
      "Step 106601, Training loss: 3.2638137340545654, Elapsed Time: 44.62 seconds\n",
      "Step 106601, Validation loss: 3.265625\n",
      "Step 106801, Training loss: 3.2617483139038086, Elapsed Time: 45.51 seconds\n",
      "Step 106801, Validation loss: 3.28125\n",
      "Step 107001, Training loss: 3.2620420455932617, Elapsed Time: 44.78 seconds\n",
      "Step 107001, Validation loss: 3.34375\n",
      "Step 107201, Training loss: 3.257263422012329, Elapsed Time: 45.71 seconds\n",
      "Step 107201, Validation loss: 3.3125\n",
      "Step 107401, Training loss: 3.2598063945770264, Elapsed Time: 44.84 seconds\n",
      "Step 107401, Validation loss: 3.25\n",
      "Step 107601, Training loss: 3.266254186630249, Elapsed Time: 45.94 seconds\n",
      "Step 107601, Validation loss: 3.28125\n",
      "Step 107801, Training loss: 3.2643795013427734, Elapsed Time: 45.05 seconds\n",
      "Step 107801, Validation loss: 3.28125\n",
      "Step 108001, Training loss: 3.2630116939544678, Elapsed Time: 45.93 seconds\n",
      "Step 108001, Validation loss: 3.3125\n",
      "Step 108201, Training loss: 3.258096933364868, Elapsed Time: 45.40 seconds\n",
      "Step 108201, Validation loss: 3.21875\n",
      "Step 108401, Training loss: 3.263686180114746, Elapsed Time: 46.06 seconds\n",
      "Step 108401, Validation loss: 3.296875\n",
      "Step 108601, Training loss: 3.2681570053100586, Elapsed Time: 44.68 seconds\n",
      "Step 108601, Validation loss: 3.28125\n",
      "Step 108801, Training loss: 3.2623512744903564, Elapsed Time: 46.46 seconds\n",
      "Step 108801, Validation loss: 3.3125\n",
      "Step 109001, Training loss: 3.2583534717559814, Elapsed Time: 44.70 seconds\n",
      "Step 109001, Validation loss: 3.296875\n",
      "Step 109201, Training loss: 3.2605721950531006, Elapsed Time: 45.57 seconds\n",
      "Step 109201, Validation loss: 3.140625\n",
      "Step 109401, Training loss: 3.263376474380493, Elapsed Time: 44.76 seconds\n",
      "Step 109401, Validation loss: 3.265625\n",
      "Step 109601, Training loss: 3.2542927265167236, Elapsed Time: 44.75 seconds\n",
      "Step 109601, Validation loss: 3.25\n",
      "Step 109801, Training loss: 3.2520394325256348, Elapsed Time: 45.82 seconds\n",
      "Step 109801, Validation loss: 3.203125\n",
      "Step 110001, Training loss: 3.254094362258911, Elapsed Time: 45.18 seconds\n",
      "Step 110001, Validation loss: 3.296875\n",
      "Step 110201, Training loss: 3.2556545734405518, Elapsed Time: 45.40 seconds\n",
      "Step 110201, Validation loss: 3.265625\n",
      "Step 110401, Training loss: 3.2548060417175293, Elapsed Time: 44.62 seconds\n",
      "Step 110401, Validation loss: 3.171875\n",
      "Step 110601, Training loss: 3.2575464248657227, Elapsed Time: 45.91 seconds\n",
      "Step 110601, Validation loss: 3.171875\n",
      "Step 110801, Training loss: 3.2472965717315674, Elapsed Time: 44.58 seconds\n",
      "Step 110801, Validation loss: 3.203125\n",
      "Step 111001, Training loss: 3.2535738945007324, Elapsed Time: 45.85 seconds\n",
      "Step 111001, Validation loss: 3.171875\n",
      "Step 111201, Training loss: 3.251131296157837, Elapsed Time: 44.67 seconds\n",
      "Step 111201, Validation loss: 3.3125\n",
      "Step 111401, Training loss: 3.254216194152832, Elapsed Time: 45.44 seconds\n",
      "Step 111401, Validation loss: 3.3125\n",
      "Step 111601, Training loss: 3.2542428970336914, Elapsed Time: 45.14 seconds\n",
      "Step 111601, Validation loss: 3.046875\n",
      "Step 111801, Training loss: 3.2528645992279053, Elapsed Time: 46.14 seconds\n",
      "Step 111801, Validation loss: 3.375\n",
      "Step 112001, Training loss: 3.249021291732788, Elapsed Time: 45.15 seconds\n",
      "Step 112001, Validation loss: 3.296875\n",
      "Step 112201, Training loss: 3.2472829818725586, Elapsed Time: 46.03 seconds\n",
      "Step 112201, Validation loss: 3.125\n",
      "Step 112401, Training loss: 3.265289545059204, Elapsed Time: 44.87 seconds\n",
      "Step 112401, Validation loss: 3.234375\n",
      "Step 112601, Training loss: 3.2508184909820557, Elapsed Time: 45.44 seconds\n",
      "Step 112601, Validation loss: 3.328125\n",
      "Step 112801, Training loss: 3.2502851486206055, Elapsed Time: 44.62 seconds\n",
      "Step 112801, Validation loss: 3.234375\n",
      "Step 113001, Training loss: 3.242316722869873, Elapsed Time: 45.59 seconds\n",
      "Step 113001, Validation loss: 3.328125\n",
      "Step 113201, Training loss: 3.259910821914673, Elapsed Time: 44.77 seconds\n",
      "Step 113201, Validation loss: 3.171875\n",
      "Step 113401, Training loss: 3.249403953552246, Elapsed Time: 46.22 seconds\n",
      "Step 113401, Validation loss: 3.171875\n",
      "Step 113601, Training loss: 3.25423264503479, Elapsed Time: 44.49 seconds\n",
      "Step 113601, Validation loss: 3.296875\n",
      "Step 113801, Training loss: 3.248680353164673, Elapsed Time: 45.96 seconds\n",
      "Step 113801, Validation loss: 3.140625\n",
      "Step 114001, Training loss: 3.2490599155426025, Elapsed Time: 45.48 seconds\n",
      "Step 114001, Validation loss: 3.375\n",
      "Step 114201, Training loss: 3.2498295307159424, Elapsed Time: 45.68 seconds\n",
      "Step 114201, Validation loss: 3.203125\n",
      "Step 114401, Training loss: 3.245774507522583, Elapsed Time: 45.84 seconds\n",
      "Step 114401, Validation loss: 3.25\n",
      "Step 114601, Training loss: 3.2353086471557617, Elapsed Time: 45.20 seconds\n",
      "Step 114601, Validation loss: 3.296875\n",
      "Step 114801, Training loss: 3.236597776412964, Elapsed Time: 45.83 seconds\n",
      "Step 114801, Validation loss: 3.296875\n",
      "Step 115001, Training loss: 3.2458865642547607, Elapsed Time: 44.94 seconds\n",
      "Step 115001, Validation loss: 3.390625\n",
      "Step 115201, Training loss: 3.2405571937561035, Elapsed Time: 46.22 seconds\n",
      "Step 115201, Validation loss: 3.234375\n",
      "Step 115401, Training loss: 3.2449939250946045, Elapsed Time: 44.83 seconds\n",
      "Step 115401, Validation loss: 3.171875\n",
      "Step 115601, Training loss: 3.234424114227295, Elapsed Time: 45.44 seconds\n",
      "Step 115601, Validation loss: 3.203125\n",
      "Step 115801, Training loss: 3.245012044906616, Elapsed Time: 44.66 seconds\n",
      "Step 115801, Validation loss: 3.1875\n",
      "Step 116001, Training loss: 3.237351417541504, Elapsed Time: 45.70 seconds\n",
      "Step 116001, Validation loss: 3.359375\n",
      "Step 116201, Training loss: 3.243882417678833, Elapsed Time: 45.41 seconds\n",
      "Step 116201, Validation loss: 3.203125\n",
      "Step 116401, Training loss: 3.2480626106262207, Elapsed Time: 45.46 seconds\n",
      "Step 116401, Validation loss: 3.171875\n",
      "Step 116601, Training loss: 3.2474255561828613, Elapsed Time: 45.15 seconds\n",
      "Step 116601, Validation loss: 3.234375\n",
      "Step 116801, Training loss: 3.2391293048858643, Elapsed Time: 45.49 seconds\n",
      "Step 116801, Validation loss: 3.125\n",
      "Step 117001, Training loss: 3.25069522857666, Elapsed Time: 45.02 seconds\n",
      "Step 117001, Validation loss: 3.3125\n",
      "Step 117201, Training loss: 3.241044759750366, Elapsed Time: 45.71 seconds\n",
      "Step 117201, Validation loss: 3.265625\n",
      "Step 117401, Training loss: 3.236121654510498, Elapsed Time: 44.70 seconds\n",
      "Step 117401, Validation loss: 3.3125\n",
      "Step 117601, Training loss: 3.240415334701538, Elapsed Time: 45.75 seconds\n",
      "Step 117601, Validation loss: 3.296875\n",
      "Step 117801, Training loss: 3.2302498817443848, Elapsed Time: 45.17 seconds\n",
      "Step 117801, Validation loss: 3.203125\n",
      "Step 118001, Training loss: 3.239046573638916, Elapsed Time: 45.15 seconds\n",
      "Step 118001, Validation loss: 3.21875\n",
      "Step 118201, Training loss: 3.237603187561035, Elapsed Time: 45.53 seconds\n",
      "Step 118201, Validation loss: 3.265625\n",
      "Step 118401, Training loss: 3.2287940979003906, Elapsed Time: 45.09 seconds\n",
      "Step 118401, Validation loss: 3.34375\n",
      "Step 118601, Training loss: 3.2286455631256104, Elapsed Time: 45.07 seconds\n",
      "Step 118601, Validation loss: 3.21875\n",
      "Step 118801, Training loss: 3.237227201461792, Elapsed Time: 44.62 seconds\n",
      "Step 118801, Validation loss: 3.234375\n",
      "Step 119001, Training loss: 3.2351057529449463, Elapsed Time: 45.63 seconds\n",
      "Step 119001, Validation loss: 3.171875\n",
      "Step 119201, Training loss: 3.243110179901123, Elapsed Time: 44.61 seconds\n",
      "Step 119201, Validation loss: 3.265625\n",
      "Step 119401, Training loss: 3.236295461654663, Elapsed Time: 45.68 seconds\n",
      "Step 119401, Validation loss: 3.171875\n",
      "Step 119601, Training loss: 3.2335219383239746, Elapsed Time: 44.91 seconds\n",
      "Step 119601, Validation loss: 3.28125\n",
      "Step 119801, Training loss: 3.2353463172912598, Elapsed Time: 45.57 seconds\n",
      "Step 119801, Validation loss: 3.21875\n",
      "Step 120001, Training loss: 3.2329559326171875, Elapsed Time: 44.72 seconds\n",
      "Step 120001, Validation loss: 3.34375\n",
      "Step 120201, Training loss: 3.2399773597717285, Elapsed Time: 45.72 seconds\n",
      "Step 120201, Validation loss: 3.203125\n",
      "Step 120401, Training loss: 3.236575126647949, Elapsed Time: 44.75 seconds\n",
      "Step 120401, Validation loss: 3.171875\n",
      "Step 120601, Training loss: 3.229675531387329, Elapsed Time: 45.59 seconds\n",
      "Step 120601, Validation loss: 3.265625\n",
      "Step 120801, Training loss: 3.228924512863159, Elapsed Time: 44.69 seconds\n",
      "Step 120801, Validation loss: 3.21875\n",
      "Step 121001, Training loss: 3.23736310005188, Elapsed Time: 45.65 seconds\n",
      "Step 121001, Validation loss: 3.328125\n",
      "Step 121201, Training loss: 3.2261178493499756, Elapsed Time: 44.69 seconds\n",
      "Step 121201, Validation loss: 3.359375\n",
      "Step 121401, Training loss: 3.2350008487701416, Elapsed Time: 45.01 seconds\n",
      "Step 121401, Validation loss: 3.203125\n",
      "Step 121601, Training loss: 3.233698844909668, Elapsed Time: 45.39 seconds\n",
      "Step 121601, Validation loss: 3.125\n",
      "Step 121801, Training loss: 3.233582019805908, Elapsed Time: 44.74 seconds\n",
      "Step 121801, Validation loss: 3.28125\n",
      "Step 122001, Training loss: 3.223358392715454, Elapsed Time: 45.73 seconds\n",
      "Step 122001, Validation loss: 3.28125\n",
      "Step 122201, Training loss: 3.2289140224456787, Elapsed Time: 44.90 seconds\n",
      "Step 122201, Validation loss: 3.15625\n",
      "Step 122401, Training loss: 3.224405527114868, Elapsed Time: 45.93 seconds\n",
      "Step 122401, Validation loss: 3.265625\n",
      "Step 122601, Training loss: 3.225661516189575, Elapsed Time: 44.83 seconds\n",
      "Step 122601, Validation loss: 3.09375\n",
      "Step 122801, Training loss: 3.2155332565307617, Elapsed Time: 45.82 seconds\n",
      "Step 122801, Validation loss: 3.28125\n",
      "Step 123001, Training loss: 3.217804193496704, Elapsed Time: 44.72 seconds\n",
      "Step 123001, Validation loss: 3.21875\n",
      "Step 123201, Training loss: 3.21993350982666, Elapsed Time: 45.56 seconds\n",
      "Step 123201, Validation loss: 3.34375\n",
      "Step 123401, Training loss: 3.2286219596862793, Elapsed Time: 44.63 seconds\n",
      "Step 123401, Validation loss: 3.1875\n",
      "Step 123601, Training loss: 3.2209980487823486, Elapsed Time: 45.46 seconds\n",
      "Step 123601, Validation loss: 3.28125\n",
      "Step 123801, Training loss: 3.224869966506958, Elapsed Time: 44.87 seconds\n",
      "Step 123801, Validation loss: 3.125\n",
      "Step 124001, Training loss: 3.220541000366211, Elapsed Time: 45.69 seconds\n",
      "Step 124001, Validation loss: 3.203125\n",
      "Step 124201, Training loss: 3.218316078186035, Elapsed Time: 45.98 seconds\n",
      "Step 124201, Validation loss: 3.1875\n",
      "Step 124401, Training loss: 3.2270665168762207, Elapsed Time: 45.73 seconds\n",
      "Step 124401, Validation loss: 3.140625\n",
      "Step 124601, Training loss: 3.226579189300537, Elapsed Time: 45.14 seconds\n",
      "Step 124601, Validation loss: 3.25\n",
      "Step 124801, Training loss: 3.221262216567993, Elapsed Time: 45.94 seconds\n",
      "Step 124801, Validation loss: 3.28125\n",
      "Step 125001, Training loss: 3.22514009475708, Elapsed Time: 44.93 seconds\n",
      "Step 125001, Validation loss: 3.203125\n",
      "Step 125201, Training loss: 3.217761516571045, Elapsed Time: 45.18 seconds\n",
      "Step 125201, Validation loss: 3.3125\n",
      "Step 125401, Training loss: 3.22753643989563, Elapsed Time: 45.57 seconds\n",
      "Step 125401, Validation loss: 3.171875\n",
      "Step 125601, Training loss: 3.216378688812256, Elapsed Time: 45.27 seconds\n",
      "Step 125601, Validation loss: 3.28125\n",
      "Step 125801, Training loss: 3.2074129581451416, Elapsed Time: 46.45 seconds\n",
      "Step 125801, Validation loss: 3.359375\n",
      "Step 126001, Training loss: 3.212503671646118, Elapsed Time: 44.84 seconds\n",
      "Step 126001, Validation loss: 3.28125\n",
      "Step 126201, Training loss: 3.225796699523926, Elapsed Time: 45.64 seconds\n",
      "Step 126201, Validation loss: 3.28125\n",
      "Step 126401, Training loss: 3.221240997314453, Elapsed Time: 44.70 seconds\n",
      "Step 126401, Validation loss: 3.140625\n",
      "Step 126601, Training loss: 3.21439790725708, Elapsed Time: 45.76 seconds\n",
      "Step 126601, Validation loss: 3.203125\n",
      "Step 126801, Training loss: 3.211467742919922, Elapsed Time: 45.52 seconds\n",
      "Step 126801, Validation loss: 3.25\n",
      "Step 127001, Training loss: 3.2112386226654053, Elapsed Time: 45.59 seconds\n",
      "Step 127001, Validation loss: 3.171875\n",
      "Step 127201, Training loss: 3.224012613296509, Elapsed Time: 44.69 seconds\n",
      "Step 127201, Validation loss: 3.25\n",
      "Step 127401, Training loss: 3.207697868347168, Elapsed Time: 45.77 seconds\n",
      "Step 127401, Validation loss: 3.421875\n",
      "Step 127601, Training loss: 3.2168338298797607, Elapsed Time: 44.73 seconds\n",
      "Step 127601, Validation loss: 3.0625\n",
      "Step 127801, Training loss: 3.2057528495788574, Elapsed Time: 45.51 seconds\n",
      "Step 127801, Validation loss: 3.21875\n",
      "Step 128001, Training loss: 3.215189218521118, Elapsed Time: 46.53 seconds\n",
      "Step 128001, Validation loss: 3.3125\n",
      "Step 128201, Training loss: 3.2185940742492676, Elapsed Time: 46.89 seconds\n",
      "Step 128201, Validation loss: 3.234375\n",
      "Step 128401, Training loss: 3.2094242572784424, Elapsed Time: 46.00 seconds\n",
      "Step 128401, Validation loss: 3.109375\n",
      "Step 128601, Training loss: 3.2037999629974365, Elapsed Time: 46.31 seconds\n",
      "Step 128601, Validation loss: 3.234375\n",
      "Step 128801, Training loss: 3.2118337154388428, Elapsed Time: 47.18 seconds\n",
      "Step 128801, Validation loss: 3.265625\n",
      "Step 129001, Training loss: 3.2133028507232666, Elapsed Time: 47.09 seconds\n",
      "Step 129001, Validation loss: 3.328125\n",
      "Step 129201, Training loss: 3.2126529216766357, Elapsed Time: 45.03 seconds\n",
      "Step 129201, Validation loss: 3.203125\n",
      "Step 129401, Training loss: 3.2161436080932617, Elapsed Time: 46.33 seconds\n",
      "Step 129401, Validation loss: 3.203125\n",
      "Step 129601, Training loss: 3.212329626083374, Elapsed Time: 44.71 seconds\n",
      "Step 129601, Validation loss: 3.296875\n",
      "Step 129801, Training loss: 3.206918239593506, Elapsed Time: 45.76 seconds\n",
      "Step 129801, Validation loss: 3.21875\n",
      "Step 130001, Training loss: 3.202075719833374, Elapsed Time: 48.31 seconds\n",
      "Step 130001, Validation loss: 3.09375\n",
      "Step 130201, Training loss: 3.212763547897339, Elapsed Time: 45.78 seconds\n",
      "Step 130201, Validation loss: 3.25\n",
      "Step 130401, Training loss: 3.2014951705932617, Elapsed Time: 45.51 seconds\n",
      "Step 130401, Validation loss: 3.140625\n",
      "Step 130601, Training loss: 3.2012035846710205, Elapsed Time: 45.58 seconds\n",
      "Step 130601, Validation loss: 3.1875\n",
      "Step 130801, Training loss: 3.2043325901031494, Elapsed Time: 44.97 seconds\n",
      "Step 130801, Validation loss: 3.203125\n",
      "Step 131001, Training loss: 3.2124319076538086, Elapsed Time: 45.96 seconds\n",
      "Step 131001, Validation loss: 3.3125\n",
      "Step 131201, Training loss: 3.210618734359741, Elapsed Time: 45.05 seconds\n",
      "Step 131201, Validation loss: 3.140625\n",
      "Step 131401, Training loss: 3.204970121383667, Elapsed Time: 46.01 seconds\n",
      "Step 131401, Validation loss: 3.328125\n",
      "Step 131601, Training loss: 3.208756923675537, Elapsed Time: 44.64 seconds\n",
      "Step 131601, Validation loss: 3.25\n",
      "Step 131801, Training loss: 3.198208808898926, Elapsed Time: 46.10 seconds\n",
      "Step 131801, Validation loss: 3.171875\n",
      "Step 132001, Training loss: 3.2087996006011963, Elapsed Time: 44.99 seconds\n",
      "Step 132001, Validation loss: 3.3125\n",
      "Step 132201, Training loss: 3.197850227355957, Elapsed Time: 45.40 seconds\n",
      "Step 132201, Validation loss: 3.265625\n",
      "Step 132401, Training loss: 3.2035038471221924, Elapsed Time: 45.75 seconds\n",
      "Step 132401, Validation loss: 3.1875\n",
      "Step 132601, Training loss: 3.1968307495117188, Elapsed Time: 44.74 seconds\n",
      "Step 132601, Validation loss: 3.25\n",
      "Step 132801, Training loss: 3.204122543334961, Elapsed Time: 45.58 seconds\n",
      "Step 132801, Validation loss: 3.265625\n",
      "Step 133001, Training loss: 3.213837146759033, Elapsed Time: 44.73 seconds\n",
      "Step 133001, Validation loss: 3.25\n",
      "Step 133201, Training loss: 3.1982944011688232, Elapsed Time: 45.61 seconds\n",
      "Step 133201, Validation loss: 3.25\n",
      "Step 133401, Training loss: 3.199436664581299, Elapsed Time: 45.20 seconds\n",
      "Step 133401, Validation loss: 3.21875\n",
      "Step 133601, Training loss: 3.199007272720337, Elapsed Time: 45.82 seconds\n",
      "Step 133601, Validation loss: 3.078125\n",
      "Step 133801, Training loss: 3.204575777053833, Elapsed Time: 44.60 seconds\n",
      "Step 133801, Validation loss: 3.203125\n",
      "Step 134001, Training loss: 3.201836109161377, Elapsed Time: 45.66 seconds\n",
      "Step 134001, Validation loss: 3.21875\n",
      "Step 134201, Training loss: 3.194746971130371, Elapsed Time: 44.74 seconds\n",
      "Step 134201, Validation loss: 3.3125\n",
      "Step 134401, Training loss: 3.1922264099121094, Elapsed Time: 45.44 seconds\n",
      "Step 134401, Validation loss: 3.1875\n",
      "Step 134601, Training loss: 3.2022526264190674, Elapsed Time: 45.04 seconds\n",
      "Step 134601, Validation loss: 3.171875\n",
      "Step 134801, Training loss: 3.194871664047241, Elapsed Time: 45.50 seconds\n",
      "Step 134801, Validation loss: 3.203125\n",
      "Step 135001, Training loss: 3.19252872467041, Elapsed Time: 45.55 seconds\n",
      "Step 135001, Validation loss: 3.1875\n",
      "Step 135201, Training loss: 3.193058490753174, Elapsed Time: 46.00 seconds\n",
      "Step 135201, Validation loss: 3.125\n",
      "Step 135401, Training loss: 3.1971077919006348, Elapsed Time: 45.35 seconds\n",
      "Step 135401, Validation loss: 3.265625\n",
      "Step 135601, Training loss: 3.193329095840454, Elapsed Time: 45.80 seconds\n",
      "Step 135601, Validation loss: 3.234375\n",
      "Step 135801, Training loss: 3.1896278858184814, Elapsed Time: 44.84 seconds\n",
      "Step 135801, Validation loss: 3.171875\n",
      "Step 136001, Training loss: 3.201374053955078, Elapsed Time: 44.63 seconds\n",
      "Step 136001, Validation loss: 3.0\n",
      "Step 136201, Training loss: 3.1968352794647217, Elapsed Time: 45.85 seconds\n",
      "Step 136201, Validation loss: 3.0625\n",
      "Step 136401, Training loss: 3.192682981491089, Elapsed Time: 44.65 seconds\n",
      "Step 136401, Validation loss: 3.109375\n",
      "Step 136601, Training loss: 3.1860198974609375, Elapsed Time: 46.19 seconds\n",
      "Step 136601, Validation loss: 3.265625\n",
      "Step 136801, Training loss: 3.195497989654541, Elapsed Time: 44.66 seconds\n",
      "Step 136801, Validation loss: 3.171875\n",
      "Step 137001, Training loss: 3.1959147453308105, Elapsed Time: 46.14 seconds\n",
      "Step 137001, Validation loss: 3.140625\n",
      "Step 137201, Training loss: 3.1842405796051025, Elapsed Time: 44.60 seconds\n",
      "Step 137201, Validation loss: 3.203125\n",
      "Step 137401, Training loss: 3.1917474269866943, Elapsed Time: 45.55 seconds\n",
      "Step 137401, Validation loss: 3.234375\n",
      "Step 137601, Training loss: 3.1861462593078613, Elapsed Time: 44.89 seconds\n",
      "Step 137601, Validation loss: 3.125\n",
      "Step 137801, Training loss: 3.182725667953491, Elapsed Time: 45.85 seconds\n",
      "Step 137801, Validation loss: 3.078125\n",
      "Step 138001, Training loss: 3.1826820373535156, Elapsed Time: 44.89 seconds\n",
      "Step 138001, Validation loss: 3.109375\n",
      "Step 138201, Training loss: 3.190619707107544, Elapsed Time: 45.94 seconds\n",
      "Step 138201, Validation loss: 3.25\n",
      "Step 138401, Training loss: 3.1863057613372803, Elapsed Time: 44.66 seconds\n",
      "Step 138401, Validation loss: 3.09375\n",
      "Step 138601, Training loss: 3.190739393234253, Elapsed Time: 45.93 seconds\n",
      "Step 138601, Validation loss: 3.25\n",
      "Step 138801, Training loss: 3.1916441917419434, Elapsed Time: 45.82 seconds\n",
      "Step 138801, Validation loss: 3.1875\n",
      "Step 139001, Training loss: 3.1907434463500977, Elapsed Time: 45.65 seconds\n",
      "Step 139001, Validation loss: 3.21875\n",
      "Step 139201, Training loss: 3.185656785964966, Elapsed Time: 44.65 seconds\n",
      "Step 139201, Validation loss: 3.015625\n",
      "Step 139401, Training loss: 3.1924049854278564, Elapsed Time: 46.39 seconds\n",
      "Step 139401, Validation loss: 3.15625\n",
      "Step 139601, Training loss: 3.180800676345825, Elapsed Time: 44.85 seconds\n",
      "Step 139601, Validation loss: 3.15625\n",
      "Step 139801, Training loss: 3.1836349964141846, Elapsed Time: 45.00 seconds\n",
      "Step 139801, Validation loss: 3.078125\n",
      "Step 140001, Training loss: 3.185201644897461, Elapsed Time: 45.57 seconds\n",
      "Step 140001, Validation loss: 3.21875\n",
      "Step 140201, Training loss: 3.1819472312927246, Elapsed Time: 45.06 seconds\n",
      "Step 140201, Validation loss: 3.21875\n",
      "Step 140401, Training loss: 3.181656837463379, Elapsed Time: 46.08 seconds\n",
      "Step 140401, Validation loss: 3.203125\n",
      "Step 140601, Training loss: 3.1797573566436768, Elapsed Time: 45.33 seconds\n",
      "Step 140601, Validation loss: 3.1875\n",
      "Step 140801, Training loss: 3.1814661026000977, Elapsed Time: 47.48 seconds\n",
      "Step 140801, Validation loss: 3.09375\n",
      "Step 141001, Training loss: 3.1818718910217285, Elapsed Time: 45.65 seconds\n",
      "Step 141001, Validation loss: 3.09375\n",
      "Step 141201, Training loss: 3.1789517402648926, Elapsed Time: 45.46 seconds\n",
      "Step 141201, Validation loss: 3.203125\n",
      "Step 141401, Training loss: 3.1746737957000732, Elapsed Time: 45.04 seconds\n",
      "Step 141401, Validation loss: 3.203125\n",
      "Step 141601, Training loss: 3.184536933898926, Elapsed Time: 45.90 seconds\n",
      "Step 141601, Validation loss: 3.03125\n",
      "Step 141801, Training loss: 3.1786043643951416, Elapsed Time: 44.98 seconds\n",
      "Step 141801, Validation loss: 3.234375\n",
      "Step 142001, Training loss: 3.173741102218628, Elapsed Time: 45.78 seconds\n",
      "Step 142001, Validation loss: 3.15625\n",
      "Step 142201, Training loss: 3.1818673610687256, Elapsed Time: 45.16 seconds\n",
      "Step 142201, Validation loss: 3.140625\n",
      "Step 142401, Training loss: 3.18058180809021, Elapsed Time: 45.69 seconds\n",
      "Step 142401, Validation loss: 3.015625\n",
      "Step 142601, Training loss: 3.1687374114990234, Elapsed Time: 44.98 seconds\n",
      "Step 142601, Validation loss: 3.125\n",
      "Step 142801, Training loss: 3.1706042289733887, Elapsed Time: 45.78 seconds\n",
      "Step 142801, Validation loss: 3.078125\n",
      "Step 143001, Training loss: 3.1750328540802, Elapsed Time: 44.82 seconds\n",
      "Step 143001, Validation loss: 3.21875\n",
      "Step 143201, Training loss: 3.17000675201416, Elapsed Time: 45.48 seconds\n",
      "Step 143201, Validation loss: 3.125\n",
      "Step 143401, Training loss: 3.1821746826171875, Elapsed Time: 44.97 seconds\n",
      "Step 143401, Validation loss: 3.21875\n",
      "Step 143601, Training loss: 3.16737961769104, Elapsed Time: 45.48 seconds\n",
      "Step 143601, Validation loss: 3.09375\n",
      "Step 143801, Training loss: 3.1817352771759033, Elapsed Time: 44.71 seconds\n",
      "Step 143801, Validation loss: 3.1875\n",
      "Step 144001, Training loss: 3.173717498779297, Elapsed Time: 45.24 seconds\n",
      "Step 144001, Validation loss: 3.21875\n",
      "Step 144201, Training loss: 3.168351173400879, Elapsed Time: 45.30 seconds\n",
      "Step 144201, Validation loss: 3.21875\n",
      "Step 144401, Training loss: 3.1802611351013184, Elapsed Time: 45.00 seconds\n",
      "Step 144401, Validation loss: 3.234375\n",
      "Step 144601, Training loss: 3.1638717651367188, Elapsed Time: 46.07 seconds\n",
      "Step 144601, Validation loss: 3.15625\n",
      "Step 144801, Training loss: 3.161057949066162, Elapsed Time: 45.11 seconds\n",
      "Step 144801, Validation loss: 3.140625\n",
      "Step 145001, Training loss: 3.1729588508605957, Elapsed Time: 46.30 seconds\n",
      "Step 145001, Validation loss: 3.234375\n",
      "Step 145201, Training loss: 3.171503782272339, Elapsed Time: 45.01 seconds\n",
      "Step 145201, Validation loss: 3.203125\n",
      "Step 145401, Training loss: 3.174647569656372, Elapsed Time: 47.19 seconds\n",
      "Step 145401, Validation loss: 3.140625\n",
      "Step 145601, Training loss: 3.1775317192077637, Elapsed Time: 46.05 seconds\n",
      "Step 145601, Validation loss: 3.125\n",
      "Step 145801, Training loss: 3.171293020248413, Elapsed Time: 45.83 seconds\n",
      "Step 145801, Validation loss: 3.21875\n",
      "Step 146001, Training loss: 3.1743624210357666, Elapsed Time: 45.24 seconds\n",
      "Step 146001, Validation loss: 3.21875\n",
      "Step 146201, Training loss: 3.166494131088257, Elapsed Time: 45.44 seconds\n",
      "Step 146201, Validation loss: 3.234375\n",
      "Step 146401, Training loss: 3.171086549758911, Elapsed Time: 44.89 seconds\n",
      "Step 146401, Validation loss: 3.21875\n",
      "Step 146601, Training loss: 3.171787977218628, Elapsed Time: 46.05 seconds\n",
      "Step 146601, Validation loss: 3.203125\n",
      "Step 146801, Training loss: 3.158693552017212, Elapsed Time: 44.62 seconds\n",
      "Step 146801, Validation loss: 3.203125\n",
      "Step 147001, Training loss: 3.169093608856201, Elapsed Time: 46.16 seconds\n",
      "Step 147001, Validation loss: 3.125\n",
      "Step 147201, Training loss: 3.162555456161499, Elapsed Time: 45.48 seconds\n",
      "Step 147201, Validation loss: 3.046875\n",
      "Step 147401, Training loss: 3.1683948040008545, Elapsed Time: 45.70 seconds\n",
      "Step 147401, Validation loss: 3.125\n",
      "Step 147601, Training loss: 3.1620700359344482, Elapsed Time: 44.77 seconds\n",
      "Step 147601, Validation loss: 3.203125\n",
      "Step 147801, Training loss: 3.158956527709961, Elapsed Time: 45.66 seconds\n",
      "Step 147801, Validation loss: 3.28125\n",
      "Step 148001, Training loss: 3.1659791469573975, Elapsed Time: 44.99 seconds\n",
      "Step 148001, Validation loss: 3.171875\n",
      "Step 148201, Training loss: 3.1662487983703613, Elapsed Time: 45.79 seconds\n",
      "Step 148201, Validation loss: 3.28125\n",
      "Step 148401, Training loss: 3.161487579345703, Elapsed Time: 46.84 seconds\n",
      "Step 148401, Validation loss: 3.234375\n",
      "Step 148601, Training loss: 3.1614222526550293, Elapsed Time: 44.94 seconds\n",
      "Step 148601, Validation loss: 3.21875\n",
      "Step 148801, Training loss: 3.1622602939605713, Elapsed Time: 45.04 seconds\n",
      "Step 148801, Validation loss: 3.1875\n",
      "Step 149001, Training loss: 3.1484086513519287, Elapsed Time: 45.21 seconds\n",
      "Step 149001, Validation loss: 3.1875\n",
      "Step 149201, Training loss: 3.1653099060058594, Elapsed Time: 46.62 seconds\n",
      "Step 149201, Validation loss: 3.25\n",
      "Step 149401, Training loss: 3.1585474014282227, Elapsed Time: 46.45 seconds\n",
      "Step 149401, Validation loss: 3.125\n",
      "Step 149601, Training loss: 3.161128520965576, Elapsed Time: 45.65 seconds\n",
      "Step 149601, Validation loss: 3.171875\n",
      "Step 149801, Training loss: 3.1499552726745605, Elapsed Time: 44.85 seconds\n",
      "Step 149801, Validation loss: 3.09375\n",
      "Step 150001, Training loss: 3.1494765281677246, Elapsed Time: 45.33 seconds\n",
      "Step 150001, Validation loss: 3.09375\n",
      "Step 150201, Training loss: 3.1664886474609375, Elapsed Time: 45.59 seconds\n",
      "Step 150201, Validation loss: 3.109375\n",
      "Step 150401, Training loss: 3.150832414627075, Elapsed Time: 46.35 seconds\n",
      "Step 150401, Validation loss: 3.21875\n",
      "Step 150601, Training loss: 3.164759397506714, Elapsed Time: 44.76 seconds\n",
      "Step 150601, Validation loss: 3.15625\n",
      "Step 150801, Training loss: 3.1670846939086914, Elapsed Time: 45.71 seconds\n",
      "Step 150801, Validation loss: 3.203125\n",
      "Step 151001, Training loss: 3.1594200134277344, Elapsed Time: 44.67 seconds\n",
      "Step 151001, Validation loss: 3.140625\n",
      "Step 151201, Training loss: 3.1596474647521973, Elapsed Time: 46.00 seconds\n",
      "Step 151201, Validation loss: 3.015625\n",
      "Step 151401, Training loss: 3.1537411212921143, Elapsed Time: 45.14 seconds\n",
      "Step 151401, Validation loss: 3.140625\n",
      "Step 151601, Training loss: 3.1514317989349365, Elapsed Time: 45.43 seconds\n",
      "Step 151601, Validation loss: 3.15625\n",
      "Step 151801, Training loss: 3.147353172302246, Elapsed Time: 45.77 seconds\n",
      "Step 151801, Validation loss: 3.171875\n",
      "Step 152001, Training loss: 3.153684377670288, Elapsed Time: 45.11 seconds\n",
      "Step 152001, Validation loss: 3.21875\n",
      "Step 152201, Training loss: 3.148064136505127, Elapsed Time: 44.95 seconds\n",
      "Step 152201, Validation loss: 3.15625\n",
      "Step 152401, Training loss: 3.15091609954834, Elapsed Time: 45.90 seconds\n",
      "Step 152401, Validation loss: 3.046875\n",
      "Step 152601, Training loss: 3.1497018337249756, Elapsed Time: 45.30 seconds\n",
      "Step 152601, Validation loss: 3.140625\n",
      "Step 152801, Training loss: 3.1496999263763428, Elapsed Time: 45.11 seconds\n",
      "Step 152801, Validation loss: 3.140625\n",
      "Step 153001, Training loss: 3.1561100482940674, Elapsed Time: 44.67 seconds\n",
      "Step 153001, Validation loss: 3.140625\n",
      "Step 153201, Training loss: 3.14941143989563, Elapsed Time: 46.13 seconds\n",
      "Step 153201, Validation loss: 3.078125\n",
      "Step 153401, Training loss: 3.145303964614868, Elapsed Time: 45.27 seconds\n",
      "Step 153401, Validation loss: 3.21875\n",
      "Step 153601, Training loss: 3.153475046157837, Elapsed Time: 44.69 seconds\n",
      "Step 153601, Validation loss: 3.1875\n",
      "Step 153801, Training loss: 3.150146484375, Elapsed Time: 45.69 seconds\n",
      "Step 153801, Validation loss: 3.25\n",
      "Step 154001, Training loss: 3.145496129989624, Elapsed Time: 44.73 seconds\n",
      "Step 154001, Validation loss: 3.171875\n",
      "Step 154201, Training loss: 3.15350079536438, Elapsed Time: 44.98 seconds\n",
      "Step 154201, Validation loss: 3.171875\n",
      "Step 154401, Training loss: 3.1486165523529053, Elapsed Time: 45.63 seconds\n",
      "Step 154401, Validation loss: 3.125\n",
      "Step 154601, Training loss: 3.1422085762023926, Elapsed Time: 45.17 seconds\n",
      "Step 154601, Validation loss: 3.125\n",
      "Step 154801, Training loss: 3.150712013244629, Elapsed Time: 44.65 seconds\n",
      "Step 154801, Validation loss: 3.1875\n",
      "Step 155001, Training loss: 3.143502712249756, Elapsed Time: 45.72 seconds\n",
      "Step 155001, Validation loss: 3.046875\n",
      "Step 155201, Training loss: 3.1456298828125, Elapsed Time: 44.76 seconds\n",
      "Step 155201, Validation loss: 3.1875\n",
      "Step 155401, Training loss: 3.1519763469696045, Elapsed Time: 45.74 seconds\n",
      "Step 155401, Validation loss: 3.125\n",
      "Step 155601, Training loss: 3.1481380462646484, Elapsed Time: 45.67 seconds\n",
      "Step 155601, Validation loss: 3.296875\n",
      "Step 155801, Training loss: 3.150977373123169, Elapsed Time: 45.66 seconds\n",
      "Step 155801, Validation loss: 3.109375\n",
      "Step 156001, Training loss: 3.1448848247528076, Elapsed Time: 45.14 seconds\n",
      "Step 156001, Validation loss: 3.203125\n",
      "Step 156201, Training loss: 3.1438534259796143, Elapsed Time: 45.96 seconds\n",
      "Step 156201, Validation loss: 3.09375\n",
      "Step 156401, Training loss: 3.1366968154907227, Elapsed Time: 44.26 seconds\n",
      "Step 156401, Validation loss: 3.078125\n",
      "Step 156601, Training loss: 3.147536516189575, Elapsed Time: 45.96 seconds\n",
      "Step 156601, Validation loss: 3.171875\n",
      "Step 156801, Training loss: 3.133382558822632, Elapsed Time: 45.15 seconds\n",
      "Step 156801, Validation loss: 3.03125\n",
      "Step 157001, Training loss: 3.149517774581909, Elapsed Time: 44.86 seconds\n",
      "Step 157001, Validation loss: 3.125\n",
      "Step 157201, Training loss: 3.147458076477051, Elapsed Time: 45.68 seconds\n",
      "Step 157201, Validation loss: 3.09375\n",
      "Step 157401, Training loss: 3.1390435695648193, Elapsed Time: 45.91 seconds\n",
      "Step 157401, Validation loss: 3.25\n",
      "Step 157601, Training loss: 3.138932228088379, Elapsed Time: 46.55 seconds\n",
      "Step 157601, Validation loss: 3.21875\n",
      "Step 157801, Training loss: 3.139862060546875, Elapsed Time: 45.10 seconds\n",
      "Step 157801, Validation loss: 3.09375\n",
      "Step 158001, Training loss: 3.136927843093872, Elapsed Time: 45.84 seconds\n",
      "Step 158001, Validation loss: 3.140625\n",
      "Step 158201, Training loss: 3.13649320602417, Elapsed Time: 44.75 seconds\n",
      "Step 158201, Validation loss: 3.1875\n",
      "Step 158401, Training loss: 3.135011911392212, Elapsed Time: 45.79 seconds\n",
      "Step 158401, Validation loss: 3.21875\n",
      "Step 158601, Training loss: 3.137221574783325, Elapsed Time: 45.11 seconds\n",
      "Step 158601, Validation loss: 3.109375\n",
      "Step 158801, Training loss: 3.1358158588409424, Elapsed Time: 45.80 seconds\n",
      "Step 158801, Validation loss: 3.1875\n",
      "Step 159001, Training loss: 3.123196601867676, Elapsed Time: 45.25 seconds\n",
      "Step 159001, Validation loss: 3.15625\n",
      "Step 159201, Training loss: 3.130964994430542, Elapsed Time: 45.06 seconds\n",
      "Step 159201, Validation loss: 3.265625\n",
      "Step 159401, Training loss: 3.134322166442871, Elapsed Time: 44.74 seconds\n",
      "Step 159401, Validation loss: 3.0625\n",
      "Step 159601, Training loss: 3.1285715103149414, Elapsed Time: 45.51 seconds\n",
      "Step 159601, Validation loss: 3.09375\n",
      "Step 159801, Training loss: 3.1270909309387207, Elapsed Time: 44.91 seconds\n",
      "Step 159801, Validation loss: 3.265625\n",
      "Step 160001, Training loss: 3.1322474479675293, Elapsed Time: 45.97 seconds\n",
      "Step 160001, Validation loss: 3.15625\n",
      "Step 160201, Training loss: 3.133519411087036, Elapsed Time: 44.68 seconds\n",
      "Step 160201, Validation loss: 3.21875\n",
      "Step 160401, Training loss: 3.1311466693878174, Elapsed Time: 46.02 seconds\n",
      "Step 160401, Validation loss: 3.109375\n",
      "Step 160601, Training loss: 3.1221489906311035, Elapsed Time: 46.19 seconds\n",
      "Step 160601, Validation loss: 3.171875\n",
      "Step 160801, Training loss: 3.129471778869629, Elapsed Time: 47.04 seconds\n",
      "Step 160801, Validation loss: 3.140625\n",
      "Step 161001, Training loss: 3.1257035732269287, Elapsed Time: 45.01 seconds\n",
      "Step 161001, Validation loss: 3.15625\n",
      "Step 161201, Training loss: 3.131056547164917, Elapsed Time: 45.69 seconds\n",
      "Step 161201, Validation loss: 3.25\n",
      "Step 161401, Training loss: 3.1279327869415283, Elapsed Time: 44.89 seconds\n",
      "Step 161401, Validation loss: 2.953125\n",
      "Step 161601, Training loss: 3.127943754196167, Elapsed Time: 45.35 seconds\n",
      "Step 161601, Validation loss: 3.09375\n",
      "Step 161801, Training loss: 3.1362504959106445, Elapsed Time: 45.55 seconds\n",
      "Step 161801, Validation loss: 3.21875\n",
      "Step 162001, Training loss: 3.132145404815674, Elapsed Time: 45.18 seconds\n",
      "Step 162001, Validation loss: 3.09375\n",
      "Step 162201, Training loss: 3.123417854309082, Elapsed Time: 45.88 seconds\n",
      "Step 162201, Validation loss: 3.265625\n",
      "Step 162401, Training loss: 3.1264593601226807, Elapsed Time: 44.74 seconds\n",
      "Step 162401, Validation loss: 3.234375\n",
      "Step 162601, Training loss: 3.126481533050537, Elapsed Time: 45.89 seconds\n",
      "Step 162601, Validation loss: 3.0\n",
      "Step 162801, Training loss: 3.132807493209839, Elapsed Time: 44.75 seconds\n",
      "Step 162801, Validation loss: 3.25\n",
      "Step 163001, Training loss: 3.1265273094177246, Elapsed Time: 46.57 seconds\n",
      "Step 163001, Validation loss: 3.109375\n",
      "Step 163201, Training loss: 3.1229639053344727, Elapsed Time: 45.15 seconds\n",
      "Step 163201, Validation loss: 3.140625\n",
      "Step 163401, Training loss: 3.1225030422210693, Elapsed Time: 45.95 seconds\n",
      "Step 163401, Validation loss: 3.1875\n",
      "Step 163601, Training loss: 3.1204845905303955, Elapsed Time: 45.67 seconds\n",
      "Step 163601, Validation loss: 3.15625\n",
      "Step 163801, Training loss: 3.1365747451782227, Elapsed Time: 45.62 seconds\n",
      "Step 163801, Validation loss: 3.140625\n",
      "Step 164001, Training loss: 3.124009132385254, Elapsed Time: 45.36 seconds\n",
      "Step 164001, Validation loss: 3.078125\n",
      "Step 164201, Training loss: 3.1051270961761475, Elapsed Time: 45.80 seconds\n",
      "Step 164201, Validation loss: 3.25\n",
      "Step 164401, Training loss: 3.123906135559082, Elapsed Time: 44.66 seconds\n",
      "Step 164401, Validation loss: 3.125\n",
      "Step 164601, Training loss: 3.1236679553985596, Elapsed Time: 45.68 seconds\n",
      "Step 164601, Validation loss: 3.15625\n",
      "Step 164801, Training loss: 3.1226391792297363, Elapsed Time: 44.86 seconds\n",
      "Step 164801, Validation loss: 3.15625\n",
      "Step 165001, Training loss: 3.1185340881347656, Elapsed Time: 46.17 seconds\n",
      "Step 165001, Validation loss: 3.171875\n",
      "Step 165201, Training loss: 3.1112043857574463, Elapsed Time: 45.51 seconds\n",
      "Step 165201, Validation loss: 3.15625\n",
      "Step 165401, Training loss: 3.123364210128784, Elapsed Time: 45.64 seconds\n",
      "Step 165401, Validation loss: 3.09375\n",
      "Step 165601, Training loss: 3.125934600830078, Elapsed Time: 45.67 seconds\n",
      "Step 165601, Validation loss: 3.046875\n",
      "Step 165801, Training loss: 3.1244757175445557, Elapsed Time: 45.92 seconds\n",
      "Step 165801, Validation loss: 3.203125\n",
      "Step 166001, Training loss: 3.120368003845215, Elapsed Time: 44.64 seconds\n",
      "Step 166001, Validation loss: 3.1875\n",
      "Step 166201, Training loss: 3.1161718368530273, Elapsed Time: 45.09 seconds\n",
      "Step 166201, Validation loss: 3.109375\n",
      "Step 166401, Training loss: 3.1160452365875244, Elapsed Time: 45.20 seconds\n",
      "Step 166401, Validation loss: 3.140625\n",
      "Step 166601, Training loss: 3.12595272064209, Elapsed Time: 45.06 seconds\n",
      "Step 166601, Validation loss: 3.25\n",
      "Step 166801, Training loss: 3.117784261703491, Elapsed Time: 45.55 seconds\n",
      "Step 166801, Validation loss: 3.09375\n",
      "Step 167001, Training loss: 3.106537342071533, Elapsed Time: 45.03 seconds\n",
      "Step 167001, Validation loss: 3.03125\n",
      "Step 167201, Training loss: 3.114635467529297, Elapsed Time: 45.53 seconds\n",
      "Step 167201, Validation loss: 3.15625\n",
      "Step 167401, Training loss: 3.1192965507507324, Elapsed Time: 45.57 seconds\n",
      "Step 167401, Validation loss: 3.109375\n",
      "Step 167601, Training loss: 3.113163709640503, Elapsed Time: 45.68 seconds\n",
      "Step 167601, Validation loss: 3.140625\n",
      "Step 167801, Training loss: 3.108492374420166, Elapsed Time: 44.65 seconds\n",
      "Step 167801, Validation loss: 3.171875\n",
      "Step 168001, Training loss: 3.123853921890259, Elapsed Time: 46.14 seconds\n",
      "Step 168001, Validation loss: 3.0625\n",
      "Step 168201, Training loss: 3.1151390075683594, Elapsed Time: 45.05 seconds\n",
      "Step 168201, Validation loss: 3.0625\n",
      "Step 168401, Training loss: 3.1123783588409424, Elapsed Time: 45.80 seconds\n",
      "Step 168401, Validation loss: 3.3125\n",
      "Step 168601, Training loss: 3.1165478229522705, Elapsed Time: 44.85 seconds\n",
      "Step 168601, Validation loss: 3.0\n",
      "Step 168801, Training loss: 3.114192485809326, Elapsed Time: 45.51 seconds\n",
      "Step 168801, Validation loss: 3.125\n",
      "Step 169001, Training loss: 3.1079494953155518, Elapsed Time: 44.63 seconds\n",
      "Step 169001, Validation loss: 3.125\n",
      "Step 169201, Training loss: 3.113980531692505, Elapsed Time: 45.70 seconds\n",
      "Step 169201, Validation loss: 3.109375\n",
      "Step 169401, Training loss: 3.1017909049987793, Elapsed Time: 44.97 seconds\n",
      "Step 169401, Validation loss: 3.21875\n",
      "Step 169601, Training loss: 3.1127045154571533, Elapsed Time: 45.82 seconds\n",
      "Step 169601, Validation loss: 3.171875\n",
      "Step 169801, Training loss: 3.103083372116089, Elapsed Time: 44.78 seconds\n",
      "Step 169801, Validation loss: 3.125\n",
      "Step 170001, Training loss: 3.1018059253692627, Elapsed Time: 45.02 seconds\n",
      "Step 170001, Validation loss: 3.0625\n",
      "Step 170201, Training loss: 3.1064085960388184, Elapsed Time: 45.62 seconds\n",
      "Step 170201, Validation loss: 3.125\n",
      "Step 170401, Training loss: 3.10690975189209, Elapsed Time: 44.83 seconds\n",
      "Step 170401, Validation loss: 3.03125\n",
      "Step 170601, Training loss: 3.104597806930542, Elapsed Time: 46.16 seconds\n",
      "Step 170601, Validation loss: 3.125\n",
      "Step 170801, Training loss: 3.109626293182373, Elapsed Time: 44.65 seconds\n",
      "Step 170801, Validation loss: 3.15625\n",
      "Step 171001, Training loss: 3.1089706420898438, Elapsed Time: 46.02 seconds\n",
      "Step 171001, Validation loss: 3.21875\n",
      "Step 171201, Training loss: 3.1133930683135986, Elapsed Time: 44.64 seconds\n",
      "Step 171201, Validation loss: 3.078125\n",
      "Step 171401, Training loss: 3.0938720703125, Elapsed Time: 46.01 seconds\n",
      "Step 171401, Validation loss: 3.171875\n",
      "Step 171601, Training loss: 3.1063125133514404, Elapsed Time: 45.13 seconds\n",
      "Step 171601, Validation loss: 3.21875\n",
      "Step 171801, Training loss: 3.10083270072937, Elapsed Time: 45.69 seconds\n",
      "Step 171801, Validation loss: 3.09375\n",
      "Step 172001, Training loss: 3.1019716262817383, Elapsed Time: 44.75 seconds\n",
      "Step 172001, Validation loss: 3.125\n",
      "Step 172201, Training loss: 3.098238229751587, Elapsed Time: 45.56 seconds\n",
      "Step 172201, Validation loss: 3.140625\n",
      "Step 172401, Training loss: 3.0988662242889404, Elapsed Time: 44.96 seconds\n",
      "Step 172401, Validation loss: 2.984375\n",
      "Step 172601, Training loss: 3.1043598651885986, Elapsed Time: 45.82 seconds\n",
      "Step 172601, Validation loss: 3.09375\n",
      "Step 172801, Training loss: 3.105172634124756, Elapsed Time: 45.19 seconds\n",
      "Step 172801, Validation loss: 3.15625\n",
      "Step 173001, Training loss: 3.107102632522583, Elapsed Time: 46.50 seconds\n",
      "Step 173001, Validation loss: 3.125\n",
      "Step 173201, Training loss: 3.0949127674102783, Elapsed Time: 45.03 seconds\n",
      "Step 173201, Validation loss: 3.078125\n",
      "Step 173401, Training loss: 3.1028759479522705, Elapsed Time: 46.50 seconds\n",
      "Step 173401, Validation loss: 3.125\n",
      "Step 173601, Training loss: 3.1036629676818848, Elapsed Time: 45.40 seconds\n",
      "Step 173601, Validation loss: 3.25\n",
      "Step 173801, Training loss: 3.099273204803467, Elapsed Time: 45.30 seconds\n",
      "Step 173801, Validation loss: 3.09375\n",
      "Step 174001, Training loss: 3.104015827178955, Elapsed Time: 44.44 seconds\n",
      "Step 174001, Validation loss: 3.078125\n",
      "Step 174201, Training loss: 3.104236125946045, Elapsed Time: 45.33 seconds\n",
      "Step 174201, Validation loss: 3.125\n",
      "Step 174401, Training loss: 3.09330153465271, Elapsed Time: 45.51 seconds\n",
      "Step 174401, Validation loss: 3.125\n",
      "Step 174601, Training loss: 3.099963426589966, Elapsed Time: 45.32 seconds\n",
      "Step 174601, Validation loss: 3.1875\n",
      "Step 174801, Training loss: 3.1021666526794434, Elapsed Time: 45.70 seconds\n",
      "Step 174801, Validation loss: 3.03125\n",
      "Step 175001, Training loss: 3.0972793102264404, Elapsed Time: 45.49 seconds\n",
      "Step 175001, Validation loss: 3.078125\n",
      "Step 175201, Training loss: 3.097452402114868, Elapsed Time: 45.86 seconds\n",
      "Step 175201, Validation loss: 3.125\n",
      "Step 175401, Training loss: 3.091081142425537, Elapsed Time: 44.88 seconds\n",
      "Step 175401, Validation loss: 3.0625\n",
      "Step 175601, Training loss: 3.0970404148101807, Elapsed Time: 45.61 seconds\n",
      "Step 175601, Validation loss: 3.03125\n",
      "Step 175801, Training loss: 3.104944944381714, Elapsed Time: 45.16 seconds\n",
      "Step 175801, Validation loss: 3.0625\n",
      "Step 176001, Training loss: 3.0886778831481934, Elapsed Time: 45.54 seconds\n",
      "Step 176001, Validation loss: 2.96875\n",
      "Step 176201, Training loss: 3.0930068492889404, Elapsed Time: 44.65 seconds\n",
      "Step 176201, Validation loss: 3.015625\n",
      "Step 176401, Training loss: 3.0859148502349854, Elapsed Time: 45.89 seconds\n",
      "Step 176401, Validation loss: 3.15625\n",
      "Step 176601, Training loss: 3.1012706756591797, Elapsed Time: 44.89 seconds\n",
      "Step 176601, Validation loss: 3.171875\n",
      "Step 176801, Training loss: 3.09366774559021, Elapsed Time: 45.74 seconds\n",
      "Step 176801, Validation loss: 3.0625\n",
      "Step 177001, Training loss: 3.0913078784942627, Elapsed Time: 45.45 seconds\n",
      "Step 177001, Validation loss: 3.015625\n",
      "Step 177201, Training loss: 3.0916008949279785, Elapsed Time: 45.65 seconds\n",
      "Step 177201, Validation loss: 3.21875\n",
      "Step 177401, Training loss: 3.088304281234741, Elapsed Time: 45.23 seconds\n",
      "Step 177401, Validation loss: 3.078125\n",
      "Step 177601, Training loss: 3.0909957885742188, Elapsed Time: 44.29 seconds\n",
      "Step 177601, Validation loss: 3.0625\n",
      "Step 177801, Training loss: 3.091722249984741, Elapsed Time: 45.75 seconds\n",
      "Step 177801, Validation loss: 3.109375\n",
      "Step 178001, Training loss: 3.096271514892578, Elapsed Time: 44.66 seconds\n",
      "Step 178001, Validation loss: 3.15625\n",
      "Step 178201, Training loss: 3.0808205604553223, Elapsed Time: 45.52 seconds\n",
      "Step 178201, Validation loss: 3.0625\n",
      "Step 178401, Training loss: 3.0912678241729736, Elapsed Time: 45.22 seconds\n",
      "Step 178401, Validation loss: 3.171875\n",
      "Step 178601, Training loss: 3.088599443435669, Elapsed Time: 45.91 seconds\n",
      "Step 178601, Validation loss: 3.109375\n",
      "Step 178801, Training loss: 3.084192991256714, Elapsed Time: 44.98 seconds\n",
      "Step 178801, Validation loss: 3.109375\n",
      "Step 179001, Training loss: 3.0930933952331543, Elapsed Time: 45.92 seconds\n",
      "Step 179001, Validation loss: 3.15625\n",
      "Step 179201, Training loss: 3.0792078971862793, Elapsed Time: 44.84 seconds\n",
      "Step 179201, Validation loss: 3.171875\n",
      "Step 179401, Training loss: 3.0785298347473145, Elapsed Time: 45.58 seconds\n",
      "Step 179401, Validation loss: 3.171875\n",
      "Step 179601, Training loss: 3.0747222900390625, Elapsed Time: 44.68 seconds\n",
      "Step 179601, Validation loss: 3.125\n",
      "Step 179801, Training loss: 3.0906505584716797, Elapsed Time: 46.21 seconds\n",
      "Step 179801, Validation loss: 3.1875\n",
      "Step 180001, Training loss: 3.0901544094085693, Elapsed Time: 45.43 seconds\n",
      "Step 180001, Validation loss: 3.109375\n",
      "Step 180201, Training loss: 3.0811052322387695, Elapsed Time: 45.45 seconds\n",
      "Step 180201, Validation loss: 3.171875\n",
      "Step 180401, Training loss: 3.084418773651123, Elapsed Time: 45.02 seconds\n",
      "Step 180401, Validation loss: 3.015625\n",
      "Step 180601, Training loss: 3.08097767829895, Elapsed Time: 45.51 seconds\n",
      "Step 180601, Validation loss: 3.09375\n",
      "Step 180801, Training loss: 3.076139450073242, Elapsed Time: 44.84 seconds\n",
      "Step 180801, Validation loss: 3.0625\n",
      "Step 181001, Training loss: 3.0871407985687256, Elapsed Time: 45.79 seconds\n",
      "Step 181001, Validation loss: 3.078125\n",
      "Step 181201, Training loss: 3.0777926445007324, Elapsed Time: 44.94 seconds\n",
      "Step 181201, Validation loss: 3.171875\n",
      "Step 181401, Training loss: 3.0833053588867188, Elapsed Time: 44.84 seconds\n",
      "Step 181401, Validation loss: 2.875\n",
      "Step 181601, Training loss: 3.0768349170684814, Elapsed Time: 45.92 seconds\n",
      "Step 181601, Validation loss: 3.046875\n",
      "Step 181801, Training loss: 3.072479248046875, Elapsed Time: 45.49 seconds\n",
      "Step 181801, Validation loss: 3.125\n",
      "Step 182001, Training loss: 3.0754354000091553, Elapsed Time: 46.53 seconds\n",
      "Step 182001, Validation loss: 3.0625\n",
      "Step 182201, Training loss: 3.0741517543792725, Elapsed Time: 44.87 seconds\n",
      "Step 182201, Validation loss: 3.21875\n",
      "Step 182401, Training loss: 3.079725503921509, Elapsed Time: 45.87 seconds\n",
      "Step 182401, Validation loss: 3.03125\n",
      "Step 182601, Training loss: 3.072563409805298, Elapsed Time: 45.67 seconds\n",
      "Step 182601, Validation loss: 3.015625\n",
      "Step 182801, Training loss: 3.082963466644287, Elapsed Time: 45.47 seconds\n",
      "Step 182801, Validation loss: 3.03125\n",
      "Step 183001, Training loss: 3.078037977218628, Elapsed Time: 44.70 seconds\n",
      "Step 183001, Validation loss: 3.15625\n",
      "Step 183201, Training loss: 3.0768752098083496, Elapsed Time: 45.89 seconds\n",
      "Step 183201, Validation loss: 3.046875\n",
      "Step 183401, Training loss: 3.0790648460388184, Elapsed Time: 45.49 seconds\n",
      "Step 183401, Validation loss: 3.140625\n",
      "Step 183601, Training loss: 3.074603796005249, Elapsed Time: 45.83 seconds\n",
      "Step 183601, Validation loss: 3.078125\n",
      "Step 183801, Training loss: 3.084984064102173, Elapsed Time: 45.77 seconds\n",
      "Step 183801, Validation loss: 3.0\n",
      "Step 184001, Training loss: 3.080728054046631, Elapsed Time: 46.28 seconds\n",
      "Step 184001, Validation loss: 3.078125\n",
      "Step 184201, Training loss: 3.0715911388397217, Elapsed Time: 44.86 seconds\n",
      "Step 184201, Validation loss: 3.0\n",
      "Step 184401, Training loss: 3.0800764560699463, Elapsed Time: 45.58 seconds\n",
      "Step 184401, Validation loss: 3.140625\n",
      "Step 184601, Training loss: 3.081054210662842, Elapsed Time: 44.84 seconds\n",
      "Step 184601, Validation loss: 2.984375\n",
      "Step 184801, Training loss: 3.0738327503204346, Elapsed Time: 45.56 seconds\n",
      "Step 184801, Validation loss: 3.140625\n",
      "Step 185001, Training loss: 3.0770387649536133, Elapsed Time: 45.54 seconds\n",
      "Step 185001, Validation loss: 3.03125\n",
      "Step 185201, Training loss: 3.078357219696045, Elapsed Time: 45.57 seconds\n",
      "Step 185201, Validation loss: 3.046875\n",
      "Step 185401, Training loss: 3.0730221271514893, Elapsed Time: 44.76 seconds\n",
      "Step 185401, Validation loss: 3.109375\n",
      "Step 185601, Training loss: 3.072727918624878, Elapsed Time: 44.92 seconds\n",
      "Step 185601, Validation loss: 3.1875\n",
      "Step 185801, Training loss: 3.0758814811706543, Elapsed Time: 46.46 seconds\n",
      "Step 185801, Validation loss: 3.09375\n",
      "Step 186001, Training loss: 3.0724589824676514, Elapsed Time: 44.66 seconds\n",
      "Step 186001, Validation loss: 3.046875\n",
      "Step 186201, Training loss: 3.075822353363037, Elapsed Time: 46.05 seconds\n",
      "Step 186201, Validation loss: 3.078125\n",
      "Step 186401, Training loss: 3.0713725090026855, Elapsed Time: 45.32 seconds\n",
      "Step 186401, Validation loss: 3.015625\n",
      "Step 186601, Training loss: 3.069643259048462, Elapsed Time: 45.08 seconds\n",
      "Step 186601, Validation loss: 3.078125\n",
      "Step 186801, Training loss: 3.0658023357391357, Elapsed Time: 44.91 seconds\n",
      "Step 186801, Validation loss: 3.0625\n",
      "Step 187001, Training loss: 3.073496103286743, Elapsed Time: 45.84 seconds\n",
      "Step 187001, Validation loss: 3.0625\n",
      "Step 187201, Training loss: 3.0823380947113037, Elapsed Time: 44.63 seconds\n",
      "Step 187201, Validation loss: 3.09375\n",
      "Step 187401, Training loss: 3.0715763568878174, Elapsed Time: 45.79 seconds\n",
      "Step 187401, Validation loss: 3.015625\n",
      "Step 187601, Training loss: 3.0637972354888916, Elapsed Time: 45.50 seconds\n",
      "Step 187601, Validation loss: 3.078125\n",
      "Step 187801, Training loss: 3.0733609199523926, Elapsed Time: 45.60 seconds\n",
      "Step 187801, Validation loss: 3.09375\n",
      "Step 188001, Training loss: 3.0687410831451416, Elapsed Time: 45.05 seconds\n",
      "Step 188001, Validation loss: 3.125\n",
      "Step 188201, Training loss: 3.075256586074829, Elapsed Time: 46.36 seconds\n",
      "Step 188201, Validation loss: 3.046875\n",
      "Step 188401, Training loss: 3.063126802444458, Elapsed Time: 45.19 seconds\n",
      "Step 188401, Validation loss: 3.109375\n",
      "Step 188601, Training loss: 3.0789124965667725, Elapsed Time: 45.80 seconds\n",
      "Step 188601, Validation loss: 3.125\n",
      "Step 188801, Training loss: 3.066230297088623, Elapsed Time: 46.26 seconds\n",
      "Step 188801, Validation loss: 3.265625\n",
      "Step 189001, Training loss: 3.0648674964904785, Elapsed Time: 46.64 seconds\n",
      "Step 189001, Validation loss: 3.09375\n",
      "Step 189201, Training loss: 3.064863443374634, Elapsed Time: 44.88 seconds\n",
      "Step 189201, Validation loss: 3.109375\n",
      "Step 189401, Training loss: 3.0741357803344727, Elapsed Time: 45.80 seconds\n",
      "Step 189401, Validation loss: 3.109375\n",
      "Step 189601, Training loss: 3.0691137313842773, Elapsed Time: 45.02 seconds\n",
      "Step 189601, Validation loss: 3.09375\n",
      "Step 189801, Training loss: 3.0644009113311768, Elapsed Time: 45.69 seconds\n",
      "Step 189801, Validation loss: 3.015625\n",
      "Step 190001, Training loss: 3.061318874359131, Elapsed Time: 45.29 seconds\n",
      "Step 190001, Validation loss: 3.03125\n",
      "Step 190201, Training loss: 3.0697834491729736, Elapsed Time: 45.61 seconds\n",
      "Step 190201, Validation loss: 3.09375\n",
      "Step 190401, Training loss: 3.0555531978607178, Elapsed Time: 45.87 seconds\n",
      "Step 190401, Validation loss: 3.03125\n",
      "Step 190601, Training loss: 3.0718109607696533, Elapsed Time: 45.65 seconds\n",
      "Step 190601, Validation loss: 3.0625\n",
      "Step 190801, Training loss: 3.062030553817749, Elapsed Time: 46.21 seconds\n",
      "Step 190801, Validation loss: 3.046875\n",
      "Step 191001, Training loss: 3.063336133956909, Elapsed Time: 45.20 seconds\n",
      "Step 191001, Validation loss: 3.0625\n",
      "Step 191201, Training loss: 3.0703964233398438, Elapsed Time: 45.69 seconds\n",
      "Step 191201, Validation loss: 2.953125\n",
      "Step 191401, Training loss: 3.056828022003174, Elapsed Time: 44.59 seconds\n",
      "Step 191401, Validation loss: 3.09375\n",
      "Step 191601, Training loss: 3.0634918212890625, Elapsed Time: 45.46 seconds\n",
      "Step 191601, Validation loss: 2.984375\n",
      "Step 191801, Training loss: 3.0645742416381836, Elapsed Time: 45.47 seconds\n",
      "Step 191801, Validation loss: 3.09375\n",
      "Step 192001, Training loss: 3.063485622406006, Elapsed Time: 45.94 seconds\n",
      "Step 192001, Validation loss: 3.203125\n",
      "Step 192201, Training loss: 3.0572001934051514, Elapsed Time: 45.76 seconds\n",
      "Step 192201, Validation loss: 3.125\n",
      "Step 192401, Training loss: 3.0614852905273438, Elapsed Time: 45.94 seconds\n",
      "Step 192401, Validation loss: 2.984375\n",
      "Step 192601, Training loss: 3.0664613246917725, Elapsed Time: 44.82 seconds\n",
      "Step 192601, Validation loss: 2.984375\n",
      "Step 192801, Training loss: 3.054262399673462, Elapsed Time: 45.58 seconds\n",
      "Step 192801, Validation loss: 3.0\n",
      "Step 193001, Training loss: 3.063884735107422, Elapsed Time: 44.58 seconds\n",
      "Step 193001, Validation loss: 2.984375\n",
      "Step 193201, Training loss: 3.063631534576416, Elapsed Time: 45.79 seconds\n",
      "Step 193201, Validation loss: 3.109375\n",
      "Step 193401, Training loss: 3.058786630630493, Elapsed Time: 44.86 seconds\n",
      "Step 193401, Validation loss: 3.015625\n",
      "Step 193601, Training loss: 3.0584142208099365, Elapsed Time: 45.60 seconds\n",
      "Step 193601, Validation loss: 3.109375\n",
      "Step 193801, Training loss: 3.0674667358398438, Elapsed Time: 45.00 seconds\n",
      "Step 193801, Validation loss: 3.171875\n",
      "Step 194001, Training loss: 3.0581774711608887, Elapsed Time: 44.77 seconds\n",
      "Step 194001, Validation loss: 3.078125\n",
      "Step 194201, Training loss: 3.06061053276062, Elapsed Time: 45.55 seconds\n",
      "Step 194201, Validation loss: 3.0\n",
      "Step 194401, Training loss: 3.060655117034912, Elapsed Time: 44.95 seconds\n",
      "Step 194401, Validation loss: 3.015625\n",
      "Step 194601, Training loss: 3.0510878562927246, Elapsed Time: 45.90 seconds\n",
      "Step 194601, Validation loss: 3.125\n",
      "Step 194801, Training loss: 3.0557141304016113, Elapsed Time: 44.74 seconds\n",
      "Step 194801, Validation loss: 3.015625\n",
      "Step 195001, Training loss: 3.0573976039886475, Elapsed Time: 45.79 seconds\n",
      "Step 195001, Validation loss: 3.171875\n",
      "Step 195201, Training loss: 3.058323383331299, Elapsed Time: 45.20 seconds\n",
      "Step 195201, Validation loss: 2.9375\n",
      "Step 195401, Training loss: 3.055142402648926, Elapsed Time: 46.22 seconds\n",
      "Step 195401, Validation loss: 3.078125\n",
      "Step 195601, Training loss: 3.06284499168396, Elapsed Time: 45.56 seconds\n",
      "Step 195601, Validation loss: 2.953125\n",
      "Step 195801, Training loss: 3.0552868843078613, Elapsed Time: 46.06 seconds\n",
      "Step 195801, Validation loss: 3.0\n",
      "Step 196001, Training loss: 3.0547659397125244, Elapsed Time: 45.10 seconds\n",
      "Step 196001, Validation loss: 3.109375\n",
      "Step 196201, Training loss: 3.05291485786438, Elapsed Time: 45.80 seconds\n",
      "Step 196201, Validation loss: 3.140625\n",
      "Step 196401, Training loss: 3.0561015605926514, Elapsed Time: 45.52 seconds\n",
      "Step 196401, Validation loss: 3.109375\n",
      "Step 196601, Training loss: 3.0536181926727295, Elapsed Time: 45.41 seconds\n",
      "Step 196601, Validation loss: 2.921875\n",
      "Step 196801, Training loss: 3.0541319847106934, Elapsed Time: 44.99 seconds\n",
      "Step 196801, Validation loss: 3.140625\n",
      "Step 197001, Training loss: 3.056055784225464, Elapsed Time: 45.77 seconds\n",
      "Step 197001, Validation loss: 3.015625\n",
      "Step 197201, Training loss: 3.046100616455078, Elapsed Time: 44.99 seconds\n",
      "Step 197201, Validation loss: 2.984375\n",
      "Step 197401, Training loss: 3.054561138153076, Elapsed Time: 45.58 seconds\n",
      "Step 197401, Validation loss: 3.125\n",
      "Step 197601, Training loss: 3.051724672317505, Elapsed Time: 44.86 seconds\n",
      "Step 197601, Validation loss: 3.0625\n",
      "Step 197801, Training loss: 3.0516297817230225, Elapsed Time: 44.79 seconds\n",
      "Step 197801, Validation loss: 3.109375\n",
      "Step 198001, Training loss: 3.0502431392669678, Elapsed Time: 45.26 seconds\n",
      "Step 198001, Validation loss: 3.0625\n",
      "Step 198201, Training loss: 3.0590596199035645, Elapsed Time: 45.53 seconds\n",
      "Step 198201, Validation loss: 3.078125\n",
      "Step 198401, Training loss: 3.0517218112945557, Elapsed Time: 45.83 seconds\n",
      "Step 198401, Validation loss: 3.03125\n",
      "Step 198601, Training loss: 3.0499539375305176, Elapsed Time: 44.75 seconds\n",
      "Step 198601, Validation loss: 3.09375\n",
      "Step 198801, Training loss: 3.0651118755340576, Elapsed Time: 46.04 seconds\n",
      "Step 198801, Validation loss: 3.046875\n",
      "Step 199001, Training loss: 3.0548155307769775, Elapsed Time: 45.07 seconds\n",
      "Step 199001, Validation loss: 3.078125\n",
      "Step 199201, Training loss: 3.0486695766448975, Elapsed Time: 45.63 seconds\n",
      "Step 199201, Validation loss: 2.953125\n",
      "Step 199401, Training loss: 3.0536327362060547, Elapsed Time: 45.13 seconds\n",
      "Step 199401, Validation loss: 3.09375\n",
      "Step 199601, Training loss: 3.0553665161132812, Elapsed Time: 46.33 seconds\n",
      "Step 199601, Validation loss: 3.109375\n",
      "Step 199801, Training loss: 3.049907684326172, Elapsed Time: 45.20 seconds\n",
      "Step 199801, Validation loss: 3.0625\n",
      "Step 200001, Training loss: 3.0569751262664795, Elapsed Time: 45.58 seconds\n",
      "Step 200001, Validation loss: 3.140625\n",
      "Step 200201, Training loss: 3.0445330142974854, Elapsed Time: 44.68 seconds\n",
      "Step 200201, Validation loss: 3.0625\n",
      "Step 200401, Training loss: 3.053306818008423, Elapsed Time: 45.74 seconds\n",
      "Step 200401, Validation loss: 3.03125\n",
      "Step 200601, Training loss: 3.0536210536956787, Elapsed Time: 45.63 seconds\n",
      "Step 200601, Validation loss: 3.078125\n",
      "Step 200801, Training loss: 3.050952434539795, Elapsed Time: 45.29 seconds\n",
      "Step 200801, Validation loss: 3.09375\n",
      "Step 201001, Training loss: 3.0533783435821533, Elapsed Time: 45.25 seconds\n",
      "Step 201001, Validation loss: 3.1875\n",
      "Step 201201, Training loss: 3.042992115020752, Elapsed Time: 45.87 seconds\n",
      "Step 201201, Validation loss: 3.0\n",
      "Step 201401, Training loss: 3.052373170852661, Elapsed Time: 44.96 seconds\n",
      "Step 201401, Validation loss: 3.03125\n",
      "Step 201601, Training loss: 3.051532506942749, Elapsed Time: 45.70 seconds\n",
      "Step 201601, Validation loss: 3.171875\n",
      "Step 201801, Training loss: 3.0537233352661133, Elapsed Time: 46.53 seconds\n",
      "Step 201801, Validation loss: 3.03125\n",
      "Step 202001, Training loss: 3.047379732131958, Elapsed Time: 45.04 seconds\n",
      "Step 202001, Validation loss: 3.078125\n",
      "Step 202201, Training loss: 3.050854444503784, Elapsed Time: 45.83 seconds\n",
      "Step 202201, Validation loss: 3.09375\n",
      "Step 202401, Training loss: 3.047806978225708, Elapsed Time: 45.09 seconds\n",
      "Step 202401, Validation loss: 3.046875\n",
      "Step 202601, Training loss: 3.0514113903045654, Elapsed Time: 45.77 seconds\n",
      "Step 202601, Validation loss: 3.015625\n",
      "Step 202801, Training loss: 3.040637254714966, Elapsed Time: 44.65 seconds\n",
      "Step 202801, Validation loss: 3.171875\n",
      "Step 203001, Training loss: 3.0507915019989014, Elapsed Time: 46.43 seconds\n",
      "Step 203001, Validation loss: 3.109375\n",
      "Step 203201, Training loss: 3.0455739498138428, Elapsed Time: 44.66 seconds\n",
      "Step 203201, Validation loss: 2.984375\n",
      "Step 203401, Training loss: 3.0504918098449707, Elapsed Time: 46.06 seconds\n",
      "Step 203401, Validation loss: 2.96875\n",
      "Step 203601, Training loss: 3.0321218967437744, Elapsed Time: 44.67 seconds\n",
      "Step 203601, Validation loss: 3.046875\n",
      "Step 203801, Training loss: 3.0382487773895264, Elapsed Time: 45.60 seconds\n",
      "Step 203801, Validation loss: 3.015625\n",
      "Step 204001, Training loss: 3.0468461513519287, Elapsed Time: 45.06 seconds\n",
      "Step 204001, Validation loss: 3.0625\n",
      "Step 204201, Training loss: 3.0448875427246094, Elapsed Time: 45.88 seconds\n",
      "Step 204201, Validation loss: 3.078125\n",
      "Step 204401, Training loss: 3.044177532196045, Elapsed Time: 45.04 seconds\n",
      "Step 204401, Validation loss: 3.109375\n",
      "Step 204601, Training loss: 3.0358901023864746, Elapsed Time: 45.48 seconds\n",
      "Step 204601, Validation loss: 2.953125\n",
      "Step 204801, Training loss: 3.0570569038391113, Elapsed Time: 44.70 seconds\n",
      "Step 204801, Validation loss: 3.125\n",
      "Step 205001, Training loss: 3.0387179851531982, Elapsed Time: 45.40 seconds\n",
      "Step 205001, Validation loss: 3.0625\n",
      "Step 205201, Training loss: 3.0456326007843018, Elapsed Time: 45.24 seconds\n",
      "Step 205201, Validation loss: 3.015625\n",
      "Step 205401, Training loss: 3.047696352005005, Elapsed Time: 45.62 seconds\n",
      "Step 205401, Validation loss: 3.09375\n",
      "Step 205601, Training loss: 3.049006223678589, Elapsed Time: 45.67 seconds\n",
      "Step 205601, Validation loss: 3.109375\n",
      "Step 205801, Training loss: 3.0459771156311035, Elapsed Time: 44.69 seconds\n",
      "Step 205801, Validation loss: 2.90625\n",
      "Step 206001, Training loss: 3.051339864730835, Elapsed Time: 46.34 seconds\n",
      "Step 206001, Validation loss: 3.140625\n",
      "Step 206201, Training loss: 3.0478715896606445, Elapsed Time: 44.89 seconds\n",
      "Step 206201, Validation loss: 3.046875\n",
      "Step 206401, Training loss: 3.041468858718872, Elapsed Time: 45.51 seconds\n",
      "Step 206401, Validation loss: 3.171875\n",
      "Step 206601, Training loss: 3.046354055404663, Elapsed Time: 44.88 seconds\n",
      "Step 206601, Validation loss: 3.0625\n",
      "Step 206801, Training loss: 3.0449342727661133, Elapsed Time: 45.55 seconds\n",
      "Step 206801, Validation loss: 3.1875\n",
      "Step 207001, Training loss: 3.035886526107788, Elapsed Time: 44.91 seconds\n",
      "Step 207001, Validation loss: 3.0625\n",
      "Step 207201, Training loss: 3.0506269931793213, Elapsed Time: 45.84 seconds\n",
      "Step 207201, Validation loss: 3.015625\n",
      "Step 207401, Training loss: 3.035637140274048, Elapsed Time: 44.72 seconds\n",
      "Step 207401, Validation loss: 3.125\n",
      "Step 207601, Training loss: 3.0424563884735107, Elapsed Time: 45.68 seconds\n",
      "Step 207601, Validation loss: 3.046875\n",
      "Step 207801, Training loss: 3.0440971851348877, Elapsed Time: 44.75 seconds\n",
      "Step 207801, Validation loss: 3.140625\n",
      "Step 208001, Training loss: 3.0380325317382812, Elapsed Time: 46.15 seconds\n",
      "Step 208001, Validation loss: 3.015625\n",
      "Step 208201, Training loss: 3.0355825424194336, Elapsed Time: 44.73 seconds\n",
      "Step 208201, Validation loss: 3.078125\n",
      "Step 208401, Training loss: 3.0355889797210693, Elapsed Time: 44.73 seconds\n",
      "Step 208401, Validation loss: 3.0\n",
      "Step 208601, Training loss: 3.039518356323242, Elapsed Time: 45.76 seconds\n",
      "Step 208601, Validation loss: 3.09375\n",
      "Step 208801, Training loss: 3.035870313644409, Elapsed Time: 44.91 seconds\n",
      "Step 208801, Validation loss: 2.96875\n",
      "Step 209001, Training loss: 3.0343844890594482, Elapsed Time: 45.87 seconds\n",
      "Step 209001, Validation loss: 3.015625\n",
      "Step 209201, Training loss: 3.0357961654663086, Elapsed Time: 44.97 seconds\n",
      "Step 209201, Validation loss: 3.125\n",
      "Step 209401, Training loss: 3.038393497467041, Elapsed Time: 45.68 seconds\n",
      "Step 209401, Validation loss: 2.984375\n",
      "Step 209601, Training loss: 3.0407731533050537, Elapsed Time: 45.11 seconds\n",
      "Step 209601, Validation loss: 2.953125\n",
      "Step 209801, Training loss: 3.0533742904663086, Elapsed Time: 46.98 seconds\n",
      "Step 209801, Validation loss: 3.171875\n",
      "Step 210001, Training loss: 3.039524555206299, Elapsed Time: 45.85 seconds\n",
      "Step 210001, Validation loss: 2.984375\n",
      "Step 210201, Training loss: 3.0359063148498535, Elapsed Time: 46.06 seconds\n",
      "Step 210201, Validation loss: 3.109375\n",
      "Step 210401, Training loss: 3.039250135421753, Elapsed Time: 45.44 seconds\n",
      "Step 210401, Validation loss: 3.03125\n",
      "Step 210601, Training loss: 3.0401384830474854, Elapsed Time: 45.40 seconds\n",
      "Step 210601, Validation loss: 3.171875\n",
      "Step 210801, Training loss: 3.047339677810669, Elapsed Time: 45.91 seconds\n",
      "Step 210801, Validation loss: 3.015625\n",
      "Step 211001, Training loss: 3.0466341972351074, Elapsed Time: 46.02 seconds\n",
      "Step 211001, Validation loss: 3.0625\n",
      "Step 211201, Training loss: 3.045949697494507, Elapsed Time: 45.30 seconds\n",
      "Step 211201, Validation loss: 2.890625\n",
      "Step 211401, Training loss: 3.03511905670166, Elapsed Time: 46.68 seconds\n",
      "Step 211401, Validation loss: 3.046875\n",
      "Step 211601, Training loss: 3.0330870151519775, Elapsed Time: 45.13 seconds\n",
      "Step 211601, Validation loss: 3.03125\n",
      "Step 211801, Training loss: 3.0418801307678223, Elapsed Time: 46.22 seconds\n",
      "Step 211801, Validation loss: 2.984375\n",
      "Step 212001, Training loss: 3.0414352416992188, Elapsed Time: 45.16 seconds\n",
      "Step 212001, Validation loss: 2.921875\n",
      "Step 212201, Training loss: 3.0407605171203613, Elapsed Time: 45.60 seconds\n",
      "Step 212201, Validation loss: 3.0625\n",
      "Step 212401, Training loss: 3.041100025177002, Elapsed Time: 44.80 seconds\n",
      "Step 212401, Validation loss: 3.046875\n",
      "Step 212601, Training loss: 3.041963815689087, Elapsed Time: 49.72 seconds\n",
      "Step 212601, Validation loss: 2.953125\n",
      "Step 212801, Training loss: 3.033869504928589, Elapsed Time: 49.80 seconds\n",
      "Step 212801, Validation loss: 3.078125\n",
      "Step 213001, Training loss: 3.0406887531280518, Elapsed Time: 49.41 seconds\n",
      "Step 213001, Validation loss: 3.046875\n",
      "Step 213201, Training loss: 3.0341811180114746, Elapsed Time: 45.08 seconds\n",
      "Step 213201, Validation loss: 3.0625\n",
      "Step 213401, Training loss: 3.0468597412109375, Elapsed Time: 45.75 seconds\n",
      "Step 213401, Validation loss: 3.125\n",
      "Step 213601, Training loss: 3.039193630218506, Elapsed Time: 44.92 seconds\n",
      "Step 213601, Validation loss: 3.125\n",
      "Step 213801, Training loss: 3.0499000549316406, Elapsed Time: 45.87 seconds\n",
      "Step 213801, Validation loss: 3.046875\n",
      "Step 214001, Training loss: 3.047654151916504, Elapsed Time: 44.92 seconds\n",
      "Step 214001, Validation loss: 3.125\n",
      "Step 214201, Training loss: 3.038728952407837, Elapsed Time: 46.00 seconds\n",
      "Step 214201, Validation loss: 3.0625\n",
      "Step 214401, Training loss: 3.047372341156006, Elapsed Time: 44.85 seconds\n",
      "Step 214401, Validation loss: 3.03125\n",
      "Step 214601, Training loss: 3.041280746459961, Elapsed Time: 45.74 seconds\n",
      "Step 214601, Validation loss: 3.0625\n",
      "Step 214801, Training loss: 3.032764196395874, Elapsed Time: 44.84 seconds\n",
      "Step 214801, Validation loss: 3.078125\n",
      "Step 215001, Training loss: 3.0349998474121094, Elapsed Time: 46.10 seconds\n",
      "Step 215001, Validation loss: 3.09375\n",
      "Step 215201, Training loss: 3.031726121902466, Elapsed Time: 45.39 seconds\n",
      "Step 215201, Validation loss: 3.0625\n",
      "Step 215401, Training loss: 3.0445396900177, Elapsed Time: 46.61 seconds\n",
      "Step 215401, Validation loss: 2.921875\n",
      "Step 215601, Training loss: 3.0370354652404785, Elapsed Time: 45.60 seconds\n",
      "Step 215601, Validation loss: 3.0\n",
      "Step 215801, Training loss: 3.0376505851745605, Elapsed Time: 45.07 seconds\n",
      "Step 215801, Validation loss: 3.0625\n",
      "Step 216001, Training loss: 3.0364201068878174, Elapsed Time: 44.68 seconds\n",
      "Step 216001, Validation loss: 3.0625\n",
      "Step 216201, Training loss: 3.0394275188446045, Elapsed Time: 44.81 seconds\n",
      "Step 216201, Validation loss: 3.0625\n",
      "Step 216401, Training loss: 3.031717538833618, Elapsed Time: 45.64 seconds\n",
      "Step 216401, Validation loss: 3.0625\n",
      "Step 216601, Training loss: 3.0354297161102295, Elapsed Time: 45.48 seconds\n",
      "Step 216601, Validation loss: 3.046875\n",
      "Step 216801, Training loss: 3.0421791076660156, Elapsed Time: 45.57 seconds\n",
      "Step 216801, Validation loss: 3.0\n",
      "Step 217001, Training loss: 3.0434558391571045, Elapsed Time: 44.68 seconds\n",
      "Step 217001, Validation loss: 3.09375\n",
      "Step 217201, Training loss: 3.0269384384155273, Elapsed Time: 46.38 seconds\n",
      "Step 217201, Validation loss: 3.140625\n",
      "Step 217401, Training loss: 3.031559705734253, Elapsed Time: 45.62 seconds\n",
      "Step 217401, Validation loss: 3.125\n",
      "Step 217601, Training loss: 3.039557456970215, Elapsed Time: 45.59 seconds\n",
      "Step 217601, Validation loss: 3.03125\n",
      "Step 217801, Training loss: 3.0443708896636963, Elapsed Time: 45.42 seconds\n",
      "Step 217801, Validation loss: 3.015625\n",
      "Step 218001, Training loss: 3.0360217094421387, Elapsed Time: 45.57 seconds\n",
      "Step 218001, Validation loss: 3.0\n",
      "Step 218201, Training loss: 3.040459632873535, Elapsed Time: 45.42 seconds\n",
      "Step 218201, Validation loss: 2.953125\n",
      "Step 218401, Training loss: 3.0391499996185303, Elapsed Time: 45.70 seconds\n",
      "Step 218401, Validation loss: 3.046875\n",
      "Step 218601, Training loss: 3.0398640632629395, Elapsed Time: 45.15 seconds\n",
      "Step 218601, Validation loss: 3.0625\n",
      "Step 218801, Training loss: 3.0371665954589844, Elapsed Time: 45.62 seconds\n",
      "Step 218801, Validation loss: 3.140625\n",
      "Step 219001, Training loss: 3.041152000427246, Elapsed Time: 45.02 seconds\n",
      "Step 219001, Validation loss: 2.984375\n",
      "Step 219201, Training loss: 3.0245516300201416, Elapsed Time: 45.11 seconds\n",
      "Step 219201, Validation loss: 3.046875\n",
      "Step 219401, Training loss: 3.0351030826568604, Elapsed Time: 44.85 seconds\n",
      "Step 219401, Validation loss: 3.0\n",
      "Step 219601, Training loss: 3.0373950004577637, Elapsed Time: 44.63 seconds\n",
      "Step 219601, Validation loss: 3.078125\n",
      "Step 219801, Training loss: 3.037203550338745, Elapsed Time: 46.34 seconds\n",
      "Step 219801, Validation loss: 3.03125\n",
      "Step 220001, Training loss: 3.0360708236694336, Elapsed Time: 44.67 seconds\n",
      "Step 220001, Validation loss: 3.0625\n",
      "Step 220201, Training loss: 3.0374064445495605, Elapsed Time: 45.57 seconds\n",
      "Step 220201, Validation loss: 3.03125\n",
      "Step 220401, Training loss: 3.042660713195801, Elapsed Time: 45.08 seconds\n",
      "Step 220401, Validation loss: 3.078125\n",
      "Step 220601, Training loss: 3.0296363830566406, Elapsed Time: 45.67 seconds\n",
      "Step 220601, Validation loss: 2.90625\n",
      "Step 220801, Training loss: 3.037543773651123, Elapsed Time: 45.53 seconds\n",
      "Step 220801, Validation loss: 3.09375\n",
      "Step 221001, Training loss: 3.034356117248535, Elapsed Time: 45.78 seconds\n",
      "Step 221001, Validation loss: 2.984375\n",
      "Step 221201, Training loss: 3.035663366317749, Elapsed Time: 44.90 seconds\n",
      "Step 221201, Validation loss: 3.03125\n",
      "Step 221401, Training loss: 3.030640125274658, Elapsed Time: 46.35 seconds\n",
      "Step 221401, Validation loss: 2.96875\n",
      "Step 221601, Training loss: 3.0364303588867188, Elapsed Time: 45.50 seconds\n",
      "Step 221601, Validation loss: 3.046875\n",
      "Step 221801, Training loss: 3.036547899246216, Elapsed Time: 45.90 seconds\n",
      "Step 221801, Validation loss: 3.171875\n",
      "Step 222001, Training loss: 3.03139328956604, Elapsed Time: 45.18 seconds\n",
      "Step 222001, Validation loss: 3.078125\n",
      "Step 222201, Training loss: 3.037193775177002, Elapsed Time: 46.08 seconds\n",
      "Step 222201, Validation loss: 3.109375\n",
      "Step 222401, Training loss: 3.031299591064453, Elapsed Time: 45.66 seconds\n",
      "Step 222401, Validation loss: 3.03125\n",
      "Step 222601, Training loss: 3.0338265895843506, Elapsed Time: 46.57 seconds\n",
      "Step 222601, Validation loss: 3.15625\n",
      "Step 222801, Training loss: 3.038017749786377, Elapsed Time: 44.64 seconds\n",
      "Step 222801, Validation loss: 3.03125\n",
      "Step 223001, Training loss: 3.034381628036499, Elapsed Time: 46.29 seconds\n",
      "Step 223001, Validation loss: 3.109375\n",
      "Step 223201, Training loss: 3.0393311977386475, Elapsed Time: 44.60 seconds\n",
      "Step 223201, Validation loss: 3.078125\n",
      "Step 223401, Training loss: 3.0242950916290283, Elapsed Time: 45.54 seconds\n",
      "Step 223401, Validation loss: 3.03125\n",
      "Step 223601, Training loss: 3.0369679927825928, Elapsed Time: 45.97 seconds\n",
      "Step 223601, Validation loss: 3.046875\n",
      "Step 223801, Training loss: 3.0337753295898438, Elapsed Time: 45.23 seconds\n",
      "Step 223801, Validation loss: 3.046875\n",
      "Step 224001, Training loss: 3.030122995376587, Elapsed Time: 45.90 seconds\n",
      "Step 224001, Validation loss: 3.015625\n",
      "Step 224201, Training loss: 3.029297113418579, Elapsed Time: 45.39 seconds\n",
      "Step 224201, Validation loss: 3.109375\n",
      "Step 224401, Training loss: 3.0317819118499756, Elapsed Time: 45.64 seconds\n",
      "Step 224401, Validation loss: 2.96875\n",
      "Step 224601, Training loss: 3.03855037689209, Elapsed Time: 44.92 seconds\n",
      "Step 224601, Validation loss: 3.125\n",
      "Step 224801, Training loss: 3.037008285522461, Elapsed Time: 45.89 seconds\n",
      "Step 224801, Validation loss: 3.03125\n",
      "Step 225001, Training loss: 3.035738706588745, Elapsed Time: 48.95 seconds\n",
      "Step 225001, Validation loss: 3.0\n",
      "Final generated text:\n",
      "Once upon a time, the only way to be sure that the person who has the most money is the one who has the most money is the one who has the most money. The person who has the most money is the one who has the most money.\n",
      "\n",
      "If you have a lot of money, you will have a lot of people who have the most money. If you have a lot of money, you will have a lot of people who have the most money.\n",
      "\n",
      "If you have a lot of money,"
     ]
    }
   ],
   "source": [
    "schedule = optax.cosine_decay_schedule(\n",
    "  init_value=init_learning_rate,\n",
    "  decay_steps=max_steps\n",
    ")\n",
    "optax_chain = optax.chain(\n",
    "  optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
    ")\n",
    "optimizer = nnx.ModelAndOptimizer(model, optax_chain)\n",
    "\n",
    "train_metrics = nnx.metrics.Average('loss')\n",
    "val_metrics = nnx.metrics.Average('val_loss')\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "start_prompt = \"Once upon a time\"\n",
    "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
    "print(f\"Initial generated text:\")\n",
    "generated_text = model.generate_text(\n",
    "    seqlen//10, start_tokens\n",
    ")\n",
    "\n",
    "\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "  'val_loss': []\n",
    "}\n",
    "\n",
    "step = 0\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    input_batch, target_batch = get_batch(\"train\")\n",
    "    if len(input_batch) % len(jax.devices()) != 0: continue  # skip the remaining elements\n",
    "    train_step(model, optimizer, train_metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
    "\n",
    "    if step % 200 == 0:\n",
    "      train_loss = float(train_metrics.compute())\n",
    "      metrics_history['train_loss'].append(train_loss)\n",
    "\n",
    "      elapsed_time = time.time() - start_time\n",
    "      print(f\"Step {step + 1}, Training loss: {train_loss}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "      # eval step\n",
    "      input_val_batch, target_val_batch = get_batch('val')\n",
    "      loss, logits = loss_fn(model, jax.device_put((input_val_batch, target_val_batch), None))\n",
    "      val_metrics.update(val_loss=loss, logits=logits)\n",
    "      val_loss = float(val_metrics.compute())\n",
    "      metrics_history['val_loss'].append(val_loss)\n",
    "      wandb.log(data={'val_loss': val_loss, 'train_loss': train_loss}, step=step)\n",
    "      print(f\"Step {step + 1}, Validation loss: {val_loss}\")\n",
    "      train_metrics.reset()\n",
    "      val_metrics.reset()\n",
    "\n",
    "      start_time = time.time()\n",
    "    step += 1\n",
    "\n",
    "    if step > max_steps:\n",
    "      break\n",
    "\n",
    "# Final text generation\n",
    "print(f\"Final generated text:\")\n",
    "generated_text = model.generate_text(\n",
    "    seqlen//10, start_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3688cf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(metrics_history['train_loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ad35b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:[process=0][thread=MainThread][operation_id=1] _SignalingThread.join() waiting for signals ([]) blocking the main thread will slow down blocking save times. This is likely due to main thread calling result() on a CommitFuture.\n"
     ]
    }
   ],
   "source": [
    "import orbax.checkpoint as orbax\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "home = Path.home()\n",
    "checkpoint_path = os.path.join(str(home), \"checkpoints\")\n",
    "\n",
    "# make sure the folder is empty and usable\n",
    "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
    "\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "train_state = nnx.state(model)\n",
    "checkpointer.save(checkpoint_path, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1830c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/prac/.venv/lib/python3.12/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1269: UserWarning: Sharding info not provided when restoring. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the only way to be sure that the person who has the most money is the one who has the most money is the one who has the most money. The person who has the most money is the one who has the most money.\n",
      "\n",
      "If you have a lot of money, you will have a lot of people who have the most money. If you have a lot of money, you will have a lot of people who have the most money.\n",
      "\n",
      "If you have a lot of money,Restored model generated text:\n",
      "Once upon a time, the only way to be sure that the person who has the most money is the one who has the most money is the one who has the most money. The person who has the most money is the one who has the most money.\n",
      "\n",
      "If you have a lot of money, you will have a lot of people who have the most money. If you have a lot of money, you will have a lot of people who have the most money.\n",
      "\n",
      "If you have a lot of money,\n"
     ]
    }
   ],
   "source": [
    "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
    "state = nnx.state(model)\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "state = checkpointer.restore(checkpoint_path, item=state)\n",
    "nnx.update(model, state)\n",
    "\n",
    "generated_text = model.generate_text(\n",
    "    seqlen//10, start_tokens\n",
    ")\n",
    "print(f\"Restored model generated text:\\n{generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
